{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Phone Training - English\n",
    "Version 2:  This version has a core structure using HM-RNN. Unlike traditional approaches, our model can automatically detect boundaries. It is trainable and updates the upper layer only upon detecting boundaries. This makes our model suitable for detecting boundaries and capturing the representations of sub-segments based on these detected boundaries. In essence, our model performs boundary detection and representation learning simultaneously.\n",
    "\n",
    "Version 3: this version completed the coding of the core model structure as well as the dataloading, preprocessing, padding and loss calculation processes. At present we only try mel->model -> mel structure, since wav <> wav would introduce extra complexion. In addition, our model will process padded multi-batch tensors as normal but count for the paddings (ignore paddings) during calculation. \n",
    "\n",
    "Version 4: this version is testing whether our hmrnn is not working. It imports a modified version of model: model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('./multiscale_rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import pickle\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it, masked_loss\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import PhonLearn_Net\n",
    "from model_test import PhonLearn_Net, DirectPassModel, TwoRNNModel, TwoRNNAttn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规范用语；规定两种方式：全加载；按rec加载（舍弃了按chunk加载，处理起来更简单）\n",
    "# RandomPhoneDataset; AnnoPhoneDataset; AnnoSeqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset that loads cutted wave files from disk and returns input-output pairs for\n",
    "    training autoencoder. \n",
    "    \n",
    "    Version 3: wav -> mel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load_dir, load_control_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the class by reading a CSV file and merging the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        The function reads the CSV file from the provided control path, extracts the \"rec\" and \"idx\" columns,\n",
    "        and concatenates the values from these columns using an underscore. It then appends the \".wav\" extension\n",
    "        to each of the merged strings and converts the merged pandas Series to a list, which is assigned to\n",
    "        the 'dataset' attribute of the class.\n",
    "\n",
    "        Args:\n",
    "        load_dir (str): The directory containing the files to load.\n",
    "        load_control_path (str): The path to the CSV file containing the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        Attributes:\n",
    "        dataset (list): A list of merged strings from the \"rec\" and \"idx\" columns, with the \".wav\" extension.\n",
    "        \"\"\"\n",
    "        control_file = pd.read_csv(load_control_path)\n",
    "        control_file = control_file[control_file['n_frames'] > 0]\n",
    "        control_file = control_file[control_file['duration'] <= 2.0]\n",
    "        \n",
    "        # Extract the \"rec\" and \"idx\" columns\n",
    "        rec_col = control_file['rec'].astype(str)\n",
    "        idx_col = control_file['idx'].astype(str).str.zfill(8)\n",
    "        \n",
    "        # Merge the two columns by concatenating the strings with '_' and append extension name\n",
    "        merged_col = rec_col + '_' + idx_col + \".wav\"\n",
    "        \n",
    "        self.dataset = merged_col.tolist()\n",
    "        self.load_dir = load_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of input-output pairs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (input_data, output_data) for the given index.\n",
    "\n",
    "        The function first checks if the provided index is a tensor, and if so, converts it to a list.\n",
    "        It then constructs the file path for the .wav file using the dataset attribute and the provided index.\n",
    "        The .wav file is loaded using torchaudio, and its data is normalized. If a transform is provided,\n",
    "        the data is transformed using the specified transform. Finally, the input_data and output_data are\n",
    "        set to the same data (creating a tuple), and the tuple is returned.\n",
    "\n",
    "        Args:\n",
    "        idx (int or torch.Tensor): The index of the desired data.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing input_data and output_data, both of which are the audio data\n",
    "               from the .wav file at the specified index.\n",
    "\n",
    "        Note: \n",
    "        This function assumes that the class has the following attributes:\n",
    "        - self.load_dir (str): The directory containing the .wav files.\n",
    "        - self.dataset (list): A list of .wav file names.\n",
    "        - self.transform (callable, optional): An optional transform to apply to the audio data.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        wav_name = os.path.join(self.load_dir,\n",
    "                                self.dataset[idx])\n",
    "        \n",
    "        data, sample_rate = torchaudio.load(wav_name, normalize=True)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        # # Prepare for possible in-out discrepencies in the future\n",
    "        # input_data = data\n",
    "        # output_data = data\n",
    "        \n",
    "        return data\n",
    "\n",
    "def collate_fn(xx):\n",
    "    # only working for one data at the moment\n",
    "    batch_first = True\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=batch_first, padding_value=0)\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "\n",
    "class MyTransform(nn.Module): \n",
    "    def __init__(self, sample_rate, n_fft): \n",
    "        super().__init__()\n",
    "        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft)\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "    \n",
    "    def forward(self, waveform): \n",
    "        mel_spec = self.transform(waveform)\n",
    "        mel_spec = self.to_db(mel_spec)\n",
    "        mel_spec = mel_spec.squeeze()\n",
    "        mel_spec = mel_spec.permute(1, 0) # (F, L) -> (L, F)\n",
    "        return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: THIS IS HOW WE CAN CREATE THE DATASET AND DATALOADER\n",
    "# sample_rate = 16000\n",
    "# n_fft = 400\n",
    "\n",
    "# transform = MyTransform(sample_rate, n_fft)\n",
    "\n",
    "# ds = PhoneDataset(phone_seg_random_path, phone_seg_random_log_path + \"s0101a.csv\", transform=transform)\n",
    "\n",
    "# test_dl = DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# indata, in_lens = next(iter(test_dl))\n",
    "\n",
    "# print(indata.shape, in_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# SEGMENTS_IN_CHUNK = 100  # set_size\n",
    "\n",
    "INPUT_DIM = 128\n",
    "OUTPUT_DIM = 128\n",
    "\n",
    "INTER_DIM_0 = 64\n",
    "INTER_DIM_1 = 32\n",
    "INTER_DIM_2 = 8\n",
    "INTER_DIM_3 = 16\n",
    "\n",
    "SIZE_LIST = [INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "\n",
    "LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "model = DirectPassModel(1.0, SIZE_LIST, in_size=INPUT_DIM, \n",
    "                      in2_size=INTER_DIM_0, hid_size=INTER_DIM_3, out_size=OUTPUT_DIM)\n",
    "# model = PhonLearn_Net(1.0, SIZE_LIST, in_size=INPUT_DIM, \n",
    "#                       in2_size=INTER_DIM_0, hid_size=INTER_DIM_3, out_size=OUTPUT_DIM)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DirectPassModel(\n",
       "  (lin_1): Linear(in_features=128, out_features=16, bias=True)\n",
       "  (lin_2): Linear(in_features=16, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4240"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NNHDmuigs8OB"
   },
   "outputs": [],
   "source": [
    "# Define recorders of training hists, for ease of extension\n",
    "class Recorder: \n",
    "    def __init__(self, IOPath): \n",
    "        self.record = []\n",
    "        self.IOPath = IOPath\n",
    "\n",
    "    def save(self): \n",
    "        pass\n",
    "    \n",
    "    def append(self, content): \n",
    "        self.record.append(content)\n",
    "    \n",
    "    def get(self): \n",
    "        return self.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kGMfle47t3Hj"
   },
   "outputs": [],
   "source": [
    "class LossRecorder(Recorder): \n",
    "    def read(self): \n",
    "        # only used by loss hists \n",
    "        with open(self.IOPath, 'rb') as f:\n",
    "            self.record = pickle.load(f)\n",
    "    \n",
    "    def save(self): \n",
    "        with open(self.IOPath, 'wb') as file:\n",
    "            pickle.dump(self.record, file)\n",
    "\n",
    "\n",
    "class HistRecorder(Recorder):     \n",
    "    def save(self): \n",
    "        with open(self.IOPath, \"a\") as txt:\n",
    "            txt.write(\"\\n\".join(self.record))\n",
    "    \n",
    "    def print(self, content): \n",
    "        self.append(content)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "ts = str(get_timestamp())\n",
    "# ts = \"0130021416\"\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "READ = False\n",
    "# READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "# if READ: \n",
    "#     valid_losses.read()\n",
    "#     train_losses.read()\n",
    "\n",
    "#     # model_name = last_model_name\n",
    "#     model_name = \"PT_0130021416_9.pt\"\n",
    "#     model_path = os.path.join(model_save_dir, model_name)\n",
    "#     state = torch.load(model_path)\n",
    "#     model = PhonLearn_Net()\n",
    "#     model.load_state_dict(state)\n",
    "#     # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\torchaudio\\functional\\functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mytrans = MyTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT)\n",
    "ds = PhoneDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "small_len = int(0.1 * len(ds))\n",
    "other_len = len(ds) - small_len\n",
    "\n",
    "# Randomly split the dataset into train and validation sets\n",
    "ds, other_ds = random_split(ds, [small_len, other_len])\n",
    "\n",
    "train_len = int(0.8 * len(ds))\n",
    "valid_len = len(ds) - train_len\n",
    "\n",
    "# Randomly split the dataset into train and validation sets\n",
    "train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "valid_num = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BASE = 0\n",
    "clip_value = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_loader), this counts the number of batches\n",
    "# len(valid_loader.dataset), this counts the number of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_data(n, max_length, dim):\n",
    "    data = []\n",
    "    x_lengths = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        # Randomly choose the length of the sequence\n",
    "        length = torch.randint(1, max_length + 1, (1,)).item()\n",
    "\n",
    "        # Create a sequence with a strong pattern (e.g., repeated values)\n",
    "        pattern = torch.randint(1, 10, (1, dim))\n",
    "        sequence = pattern.repeat(length, 1).float()\n",
    "\n",
    "        # Add random noise to the sequence to mimic variations in real data\n",
    "        noise = torch.randn(length, dim) * 0.1\n",
    "        sequence += noise\n",
    "\n",
    "        data.append(sequence)\n",
    "        x_lengths.append(length)\n",
    "\n",
    "    # Find the length of the longest sequence in the batch\n",
    "    longest_sequence = max(x_lengths)\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence\n",
    "    padded_data = []\n",
    "    for seq, length in zip(data, x_lengths):\n",
    "        padded_sequence = torch.zeros(longest_sequence, dim)\n",
    "        padded_sequence[:length] = seq\n",
    "        padded_data.append(padded_sequence)\n",
    "\n",
    "    data = torch.stack(padded_data)\n",
    "\n",
    "    return data, x_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tld = []\n",
    "for i in range(350): \n",
    "    dt, xl = generate_fake_data(BATCH_SIZE, 50, INPUT_DIM)\n",
    "    tld.append((dt, xl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50): \n",
    "#     m = generate_mask_from_lengths_mat(tld[i][1], device=device)\n",
    "#     x = tld[i][0]\n",
    "#     print(x.size(1) == m.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch = x.size(0)\n",
    "            batch, length, dim = x.shape\n",
    "            # print(x)\n",
    "            mellog = np.log(x + 1e-9)\n",
    "            melnormalized = librosa.util.normalize(mellog)\n",
    "            print(melnormalized)\n",
    "\n",
    "            # # Loop through each dimension and plot the histogram\n",
    "            # for i in range(dim):\n",
    "            #     plt.figure()\n",
    "            #     plt.hist(x[:, :, i].flatten(), bins=20)\n",
    "            #     plt.title(f\"Histogram for dimension {i+1}\")\n",
    "            #     plt.xlabel(\"Value\")\n",
    "            #     plt.ylabel(\"Frequency\")\n",
    "            #     plt.show()\n",
    "            raise Exception\n",
    "            # print(x.size(1))\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "\n",
    "            recon_x, _ = model(x, x_mask) # _ = hidden, z_1, z_2\n",
    "\n",
    "            \n",
    "            loss = masked_loss(recon_loss, recon_x, x, x_mask)\n",
    "            # loss = recon_loss(recon_x, x)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            # loss = loss / batch\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 20 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "                gc.collect()\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            # batch = x.size(0)\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            recon_x, _ = model(x, x_mask) # _ = hidden, z_1, z_2\n",
    "\n",
    "            loss = masked_loss(recon_loss, recon_x, x, x_mask)\n",
    "            # loss = recon_loss(recon_x, x)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            if idx % 20 == 0:\n",
    "                # \\t Recon {recon / batch: .3f} \\t KL {kl / batch: .3f}\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "                gc.collect()\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(tld)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(tld):\n",
    "            optimizer.zero_grad()\n",
    "            batch = x.size(0)\n",
    "            # print(x.size(1))\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "\n",
    "            recon_x, _ = model(x, x_mask) # _ = hidden, z_1, z_2\n",
    "\n",
    "            \n",
    "            loss = masked_loss(recon_loss, recon_x, x, x_mask)\n",
    "            # loss = recon_loss(recon_x, x)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            # loss = loss / batch\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 20 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "                gc.collect()\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\": \n",
    "#     fake_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\tempfile.py:256\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     fd \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39;49mopen(file, flags, \u001b[39m0o600\u001b[39;49m)\n\u001b[0;32m    257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'a:\\\\ProgramData\\\\anaconda3\\\\envs\\\\wavln\\\\lib\\\\site-packages\\\\librosa\\\\util\\\\__pycache__\\\\tmpv7rsw37p'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m----> 2\u001b[0m     train()\n",
      "Cell \u001b[1;32mIn[28], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m mellog \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(x \u001b[39m+\u001b[39m \u001b[39m1e-9\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m melnormalized \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49mnormalize(mellog)\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(melnormalized)\n\u001b[0;32m     17\u001b[0m \u001b[39m# # Loop through each dimension and plot the histogram\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# for i in range(dim):\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m#     plt.figure()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m#     plt.ylabel(\"Frequency\")\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#     plt.show()\u001b[39;00m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\lazy_loader\\__init__.py:76\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m attr_to_modules:\n\u001b[0;32m     75\u001b[0m     submod_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpackage_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mattr_to_modules[name]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 76\u001b[0m     submod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(submod_path)\n\u001b[0;32m     77\u001b[0m     attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(submod, name)\n\u001b[0;32m     79\u001b[0m     \u001b[39m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[39m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[39m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\librosa\\util\\utils.py:1075\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Numba stencil for local minima computation\"\"\"\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m     \u001b[39mreturn\u001b[39;00m (x[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m x[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m&\u001b[39m (x[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m])\n\u001b[0;32m   1063\u001b[0m \u001b[39m@numba\u001b[39;49m\u001b[39m.\u001b[39;49mguvectorize(\n\u001b[0;32m   1064\u001b[0m     [\n\u001b[0;32m   1065\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mvoid(int16[:], bool_[:])\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1066\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mvoid(int32[:], bool_[:])\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1067\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mvoid(int64[:], bool_[:])\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1068\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mvoid(float32[:], bool_[:])\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1069\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mvoid(float64[:], bool_[:])\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1070\u001b[0m     ],\n\u001b[0;32m   1071\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m(n)->(n)\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1072\u001b[0m     cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1073\u001b[0m     nopython\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1074\u001b[0m )\n\u001b[1;32m-> 1075\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39m_localmax\u001b[39;49m(x, y):  \u001b[39m# pragma: no cover\u001b[39;49;00m\n\u001b[0;32m   1076\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"Vectorized wrapper for the localmax stencil\"\"\"\u001b[39;49;00m\n\u001b[0;32m   1077\u001b[0m     y[:] \u001b[39m=\u001b[39;49m _localmax_sten(x)\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\np\\ufunc\\decorators.py:201\u001b[0m, in \u001b[0;36mguvectorize.<locals>.wrap\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(func):\n\u001b[1;32m--> 201\u001b[0m     guvec \u001b[39m=\u001b[39m GUVectorize(func, signature, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    202\u001b[0m     \u001b[39mfor\u001b[39;00m fty \u001b[39min\u001b[39;00m ftylist:\n\u001b[0;32m    203\u001b[0m         guvec\u001b[39m.\u001b[39madd(fty)\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\np\\ufunc\\decorators.py:56\u001b[0m, in \u001b[0;36mGUVectorize.__new__\u001b[1;34m(cls, func, signature, **kws)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m imp \u001b[39mis\u001b[39;00m gufunc\u001b[39m.\u001b[39mGUFunc:\n\u001b[0;32m     55\u001b[0m     is_dyn \u001b[39m=\u001b[39m kws\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mis_dynamic\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m imp(func, signature, identity\u001b[39m=\u001b[39;49midentity, cache\u001b[39m=\u001b[39;49mcache,\n\u001b[0;32m     57\u001b[0m                is_dynamic\u001b[39m=\u001b[39;49mis_dyn, targetoptions\u001b[39m=\u001b[39;49mkws,\n\u001b[0;32m     58\u001b[0m                writable_args\u001b[39m=\u001b[39;49mwritable_args)\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m imp(func, signature, identity\u001b[39m=\u001b[39midentity, cache\u001b[39m=\u001b[39mcache,\n\u001b[0;32m     61\u001b[0m                targetoptions\u001b[39m=\u001b[39mkws, writable_args\u001b[39m=\u001b[39mwritable_args)\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\np\\ufunc\\gufunc.py:28\u001b[0m, in \u001b[0;36mGUFunc.__init__\u001b[1;34m(self, py_func, signature, identity, cache, is_dynamic, targetoptions, writable_args)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_identity \u001b[39m=\u001b[39m identity\n\u001b[0;32m     25\u001b[0m \u001b[39m# GUFunc cannot inherit from GUFuncBuilder because \"identity\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m# is a property of GUFunc. Thus, we hold a reference to a GUFuncBuilder\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m# object here\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgufunc_builder \u001b[39m=\u001b[39m GUFuncBuilder(\n\u001b[0;32m     29\u001b[0m     py_func, signature, identity, cache, targetoptions, writable_args)\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgufunc_builder\u001b[39m.\u001b[39mpy_func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m     31\u001b[0m functools\u001b[39m.\u001b[39mupdate_wrapper(\u001b[39mself\u001b[39m, py_func)\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\np\\ufunc\\ufuncbuilder.py:346\u001b[0m, in \u001b[0;36mGUFuncBuilder.__init__\u001b[1;34m(self, py_func, signature, identity, cache, targetoptions, writable_args)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midentity \u001b[39m=\u001b[39m parse_identity(identity)\n\u001b[0;32m    345\u001b[0m \u001b[39mwith\u001b[39;00m _suppress_deprecation_warning_nopython_not_supplied():\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_func \u001b[39m=\u001b[39m jit(_target\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mnpyufunc\u001b[39;49m\u001b[39m'\u001b[39;49m, cache\u001b[39m=\u001b[39;49mcache)(py_func)\n\u001b[0;32m    347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature \u001b[39m=\u001b[39m signature\n\u001b[0;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msout \u001b[39m=\u001b[39m parse_signature(signature)\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\core\\decorators.py:234\u001b[0m, in \u001b[0;36m_jit.<locals>.wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    230\u001b[0m disp \u001b[39m=\u001b[39m dispatcher(py_func\u001b[39m=\u001b[39mfunc, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlocals\u001b[39m,\n\u001b[0;32m    231\u001b[0m                   targetoptions\u001b[39m=\u001b[39mtargetoptions,\n\u001b[0;32m    232\u001b[0m                   \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdispatcher_args)\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m cache:\n\u001b[1;32m--> 234\u001b[0m     disp\u001b[39m.\u001b[39;49menable_caching()\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m sigs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     \u001b[39m# Register the Dispatcher to the type inference mechanism,\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[39m# even though the decorator hasn't returned yet.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m typeinfer\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\np\\ufunc\\ufuncbuilder.py:105\u001b[0m, in \u001b[0;36mUFuncDispatcher.enable_caching\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menable_caching\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m FunctionCache(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpy_func)\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\core\\caching.py:601\u001b[0m, in \u001b[0;36mCache.__init__\u001b[1;34m(self, py_func)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(py_func)\n\u001b[0;32m    600\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_py_func \u001b[39m=\u001b[39m py_func\n\u001b[1;32m--> 601\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_impl_class(py_func)\n\u001b[0;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl\u001b[39m.\u001b[39mlocator\u001b[39m.\u001b[39mget_cache_path()\n\u001b[0;32m    603\u001b[0m \u001b[39m# This may be a bit strict but avoids us maintaining a magic number\u001b[39;00m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\core\\caching.py:333\u001b[0m, in \u001b[0;36mCacheImpl.__init__\u001b[1;34m(self, py_func)\u001b[0m\n\u001b[0;32m    331\u001b[0m source_path \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetfile(py_func)\n\u001b[0;32m    332\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_locator_classes:\n\u001b[1;32m--> 333\u001b[0m     locator \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_function(py_func, source_path)\n\u001b[0;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m locator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\core\\caching.py:180\u001b[0m, in \u001b[0;36m_SourceFileBackedLocatorMixin.from_function\u001b[1;34m(cls, py_func, py_file)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(py_func, py_file)\n\u001b[0;32m    179\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_cache_path()\n\u001b[0;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[39m# Cannot ensure the cache directory exists or is writable\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\numba\\core\\caching.py:107\u001b[0m, in \u001b[0;36m_CacheLocator.ensure_cache_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m os\u001b[39m.\u001b[39mmakedirs(path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    106\u001b[0m \u001b[39m# Ensure the directory is writable by trying to write a temporary file\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m tempfile\u001b[39m.\u001b[39;49mTemporaryFile(\u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49mpath)\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\tempfile.py:559\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[1;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[39mreturn\u001b[39;00m fd\n\u001b[0;32m    558\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m     file \u001b[39m=\u001b[39m _io\u001b[39m.\u001b[39;49mopen(\u001b[39mdir\u001b[39;49m, mode, buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[0;32m    560\u001b[0m                     newline\u001b[39m=\u001b[39;49mnewline, encoding\u001b[39m=\u001b[39;49mencoding, errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    561\u001b[0m                     opener\u001b[39m=\u001b[39;49mopener)\n\u001b[0;32m    562\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    563\u001b[0m         raw \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(file, \u001b[39m'\u001b[39m\u001b[39mbuffer\u001b[39m\u001b[39m'\u001b[39m, file)\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\tempfile.py:556\u001b[0m, in \u001b[0;36mNamedTemporaryFile.<locals>.opener\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopener\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m    555\u001b[0m     \u001b[39mnonlocal\u001b[39;00m name\n\u001b[1;32m--> 556\u001b[0m     fd, name \u001b[39m=\u001b[39m _mkstemp_inner(\u001b[39mdir\u001b[39;49m, prefix, suffix, flags, output_type)\n\u001b[0;32m    557\u001b[0m     \u001b[39mreturn\u001b[39;00m fd\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\tempfile.py:256\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    254\u001b[0m _sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mtempfile.mkstemp\u001b[39m\u001b[39m\"\u001b[39m, file)\n\u001b[0;32m    255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     fd \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39;49mopen(file, flags, \u001b[39m0o600\u001b[39;49m)\n\u001b[0;32m    257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[39mcontinue\u001b[39;00m    \u001b[39m# try again\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "valid_losses.save()\n",
    "train_losses.save()\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22f8ea41b40>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAAGxCAYAAAC9csYjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5I0lEQVR4nO3deXxU9b3/8feZyWSSCRAQCEkkkOAKiILggqjgUrgiXpC60VJBXEC0llJaxba21l+Fel3oolKsRCwVqZelWrSIlSAWvAWl1VpEKoFQSEqxSjAhM5mZ8/sjc04yyWSZMFvI6/l4zCMzZ845881hfOTtd/kcwzRNUwAAAEnmSHYDAAAAJEIJAABIEYQSAACQEgglAAAgJRBKAABASiCUAACAlEAoAQAAKYFQAgAAUgKhBAAApARCCZAinnvuORmGoe3btye7KVEbM2aMxowZk7TPDwaD+vWvf60rr7xSvXr1ksvlUk5OjiZMmKBXXnlFwWAwaW0D0HZpyW4AgI7vqaeeStpn19TUaNKkSXr99dd100036emnn1Zubq7+/e9/6w9/+IOuv/56rVy5UhMnTkxaGwG0DaEEQBjTNFVTU6PMzMw2HzNo0KA4tqhlc+fO1fr167Vs2TLdfPPNYe9NnjxZ3/72t3Xs2LGYfFZ1dbU8Hk9MzgWgKYZvgA5m9+7d+spXvqKcnBy53W4NHDhQTz75ZNg+NTU1+ta3vqWhQ4cqOztbJ510kkaOHKnf/e53Tc5nGIbuvvtuLV68WAMHDpTb7dayZcvs4aSNGzfqzjvvVK9evdSzZ09NnjxZBw8eDDtH4+GbvXv3yjAMPfroo3r88cdVVFSkLl26aOTIkXrnnXeatOGZZ57R6aefLrfbrUGDBumFF17Q9OnTVVhY2OK1qKio0K9+9SuNGzeuSSCxnHbaaTr77LMl1Q+R7d27N2yfkpISGYahkpKSsN/prLPO0ltvvaWLLrpIHo9HM2bM0KRJk9S/f/+IQ0IXXHCBzj33XPu1aZp66qmnNHToUGVmZqpHjx667rrrtGfPnhZ/L6CzIpQAHcjf//53nXfeefrb3/6mxx57TL///e919dVX65577tGDDz5o7+f1evWf//xH8+bN09q1a7VixQpdfPHFmjx5sp5//vkm5127dq2efvppPfDAA1q/fr0uueQS+73bbrtNLpdLL7zwgh555BGVlJRo6tSpbWrvk08+qQ0bNmjRokX6zW9+o6qqKo0fP15Hjhyx91myZInuuOMOnX322Vq9erW+973v6cEHHwwLCM3ZuHGjamtrNWnSpDa1J1rl5eWaOnWqvvKVr+jVV1/V7NmzNWPGDJWVlenNN98M2/ejjz7Sn//8Z91yyy32tpkzZ2rOnDm68sortXbtWj311FP68MMPddFFF+lf//pXXNoMdGgmgJRQXFxsSjK3bdvW7D7jxo0z+/btax45ciRs+913321mZGSY//nPfyIe5/f7zdraWvPWW281hw0bFvaeJDM7O7vJsVZ7Zs+eHbb9kUceMSWZ5eXl9rbRo0ebo0ePtl+XlpaakswhQ4aYfr/f3v7nP//ZlGSuWLHCNE3TDAQCZm5urnnBBReEfca+fftMl8tl9u/fv9lrYZqmuXDhQlOS+Yc//KHF/Rr/TqWlpWHbN27caEoyN27cGPY7STL/+Mc/hu1bW1tr9unTx/zKV74Stv073/mOmZ6ebh4+fNg0TdPcunWrKcl87LHHwvbbv3+/mZmZaX7nO99pU5uBzoSeEqCDqKmp0R//+Edde+218ng88vv99mP8+PGqqakJGxp56aWXNGrUKHXp0kVpaWlyuVx69tlntXPnzibnvvzyy9WjR4+In/vf//3fYa+toZB9+/a12uarr75aTqez2WN37dqliooK3XDDDWHH9evXT6NGjWr1/PHWo0cPXX755WHb0tLSNHXqVK1evdru8QkEAvr1r3+tiRMnqmfPnpKk3//+9zIMQ1OnTg37t8rNzdU555zTpp4goLMhlAAdxKeffiq/36+f//zncrlcYY/x48dLkg4fPixJWr16tW644QadfPLJWr58ubZu3apt27ZpxowZqqmpaXLuvLy8Zj/X+iNrcbvdktSmyaOtHfvpp59Kkvr06dPk2EjbGuvXr58kqbS0tNV926O562JdxxdffFGStH79epWXl4cN3fzrX/+SaZrq06dPk3+vd955x/63AlCP1TdAB9GjRw85nU597Wtf01133RVxn6KiIknS8uXLVVRUpJUrV8owDPt9r9cb8biG+ySSFVoiza+oqKho9fjLLrtMLpdLa9eu1axZs1rdPyMjQ1LT69BcQGjuugwaNEjnn3++iouLNXPmTBUXFys/P19jx4619+nVq5cMw9DmzZvtMNZQpG1AZ0dPCdBBeDweXXbZZdqxY4fOPvtsjRgxosnD+iNvGIbS09PD/qhWVFREXH2TTGeccYZyc3P129/+Nmx7WVmZtmzZ0urxubm5uu2227R+/fqIE3gl6ZNPPtH7778vSfZqHuu15eWXX4667bfccov+7//+T2+//bZeeeUVTZs2LWyoasKECTJNUwcOHIj4bzVkyJCoPxM40dFTAqSYN998s8mSVUkaP368fvrTn+riiy/WJZdcojvvvFOFhYU6evSo/vGPf+iVV16xV4RMmDBBq1ev1uzZs3Xddddp//79euihh5SXl6fdu3cn+DdqnsPh0IMPPqiZM2fquuuu04wZM/T555/rwQcfVF5enhyO1v+/6fHHH9eePXs0ffp0rV+/Xtdee6369Omjw4cPa8OGDSouLtaLL76os88+W+edd57OOOMMzZs3T36/Xz169NCaNWv09ttvR932KVOmaO7cuZoyZYq8Xq+mT58e9v6oUaN0xx136JZbbtH27dt16aWXKisrS+Xl5Xr77bc1ZMgQ3XnnnVF/LnAiI5QAKebee++NuL20tFSDBg3Se++9p4ceekjf+973dOjQIXXv3l2nnXaaPa9Eqvu/+EOHDmnx4sVaunSpBgwYoPvuu0///Oc/w5YOp4I77rhDhmHokUce0bXXXqvCwkLdd999+t3vfqeysrJWj8/IyNC6dev0m9/8RsuWLdPMmTNVWVmpHj16aMSIEVq6dKmuueYaSZLT6dQrr7yiu+++W7NmzZLb7dZNN92kX/ziF7r66qujand2drauvfZavfDCCxo1apROP/30Jvv88pe/1IUXXqhf/vKXeuqppxQMBpWfn69Ro0bp/PPPj+rzgM7AME3TTHYjAKChzz//XKeffromTZqkJUuWJLs5ABKEnhIASVVRUaEf//jHuuyyy9SzZ0/t27dPTzzxhI4ePapvfOMbyW4egAQilABIKrfbrb1792r27Nn6z3/+I4/HowsvvFCLFy/W4MGDk908AAnE8A0AAEgJLAkGAAApgVACAABSAqEEAACkhA4x0TUYDOrgwYPq2rVr0sphAwCA6JimqaNHjyo/P79NxRA7RCg5ePCgCgoKkt0MAADQDvv371ffvn1b3a9DhJKuXbtKqvulunXrluTWAACAtqisrFRBQYH9d7w1HSKUWEM23bp1I5QAANDBtHXqBRNdAQBASiCUAACAlEAoAQAAKaFDzCkBACDWTNOU3+9XIBBIdlM6LKfTqbS0tJiV6yCUAAA6HZ/Pp/LyclVXVye7KR2ex+NRXl6e0tPTj/tchBIAQKcSDAZVWloqp9Op/Px8paenU5izHUzTlM/n07///W+VlpbqtNNOa1OBtJYQSgAAnYrP51MwGFRBQYE8Hk+ym9OhZWZmyuVyad++ffL5fMrIyDiu8zHRFQDQKR3v/9WjTiyvI/8iAAAgJRBKAABASiCUAADQiY0ZM0Zz5sxJdjMkMdEVAIAOobUVQtOmTdNzzz0X9XlXr14tl8vVzlbFVqcOJave/afe/+fnGj8kTxcM6Jns5gAA0Kzy8nL7+cqVK/XAAw9o165d9rbMzMyw/Wtra9sUNk466aTYNfI4derhm5KP/61lW/fpw4OVyW4KACCJTNNUtc+flIdpmm1qY25urv3Izs6WYRj265qaGnXv3l2//e1vNWbMGGVkZGj58uX69NNPNWXKFPXt21cej0dDhgzRihUrws7bePimsLBQDz/8sGbMmKGuXbuqX79+WrJkSSwvd7M6dU9JVrpTklTl9Se5JQCAZDpWG9CgB9Yn5bP//qNx8qTH5s/xvffeq8cee0zFxcVyu92qqanR8OHDde+996pbt25at26dvva1r2nAgAG64IILmj3PY489poceekj333+//vd//1d33nmnLr30Up155pkxaWdzOncocdf9+lU+7nsAAOj45syZo8mTJ4dtmzdvnv3861//uv7whz/opZdeajGUjB8/XrNnz5ZUF3SeeOIJlZSUEEriyeopqfbRUwIAnVmmy6m//2hc0j47VkaMGBH2OhAIaOHChVq5cqUOHDggr9crr9errKysFs9z9tln28+tYaJDhw7FrJ3N6dShxBPqKfmC4RsA6NQMw4jZEEoyNQ4bjz32mJ544gktWrRIQ4YMUVZWlubMmSOfz9fieRpPkDUMQ8FgMObtbazj/wscB2v4ptrL8A0A4MSzefNmTZw4UVOnTpVUdzPC3bt3a+DAgUluWWSdevWNPdGV4RsAwAno1FNP1YYNG7Rlyxbt3LlTM2fOVEVFRbKb1axOHUqsrjpW3wAATkTf//73de6552rcuHEaM2aMcnNzNWnSpGQ3q1mdevimizV8w+obAEAHMn36dE2fPt1+XVhYGLHeyUknnaS1a9e2eK6SkpKw13v37m2yz1/+8pfoG9kOnbunxF03fMNEVwAAkq9Th5KsdHpKAABIFZ07lLip6AoAQKro3KEk1FPi9QflD8R//TUAAGhepw4l1pwSiVLzAAAkW6cOJe40p1xOQxKl5gEASLZOHUokapUAAJAqOn0osWqVVFFqHgCApOr0ocRDqXkAAFICoYSeEgBAJzFmzBjNmTPHfl1YWKhFixa1eIxhGK1WhY2VTh9KuoRW4DDRFQCQyq655hpdeeWVEd/bunWrDMPQe++9F9U5t23bpjvuuCMWzYuJTh9KrImulJoHAKSyW2+9VW+++ab27dvX5L2lS5dq6NChOvfcc6M6Z+/eveXxeGLVxOPW6UNJVmhOSTXDNwDQeZmm5KtKziPCjfQimTBhgnJycvTcc8+Fba+urtbKlSs1adIkTZkyRX379pXH49GQIUO0YsWKFs/ZePhm9+7duvTSS5WRkaFBgwZpw4YN0V7J49Kp7xIsSVnWnBKGbwCg86qtlh7OT85n339QSs9qdbe0tDTdfPPNeu655/TAAw/IMOrqbL300kvy+Xy67bbbtGLFCt17773q1q2b1q1bp6997WsaMGCALrjgglbPHwwGNXnyZPXq1UvvvPOOKisrw+afJEJUPSV+v1/f+973VFRUpMzMTA0YMEA/+tGPFAy2XKJ906ZNGj58uDIyMjRgwAAtXrz4uBodS3YoYfgGAJDiZsyYob1796qkpMTetnTpUk2ePFknn3yy5s2bp6FDh2rAgAH6+te/rnHjxumll15q07nfeOMN7dy5U7/+9a81dOhQXXrppXr44Yfj9JtEFlVPyU9+8hMtXrxYy5Yt0+DBg7V9+3bdcsstys7O1je+8Y2Ix5SWlmr8+PG6/fbbtXz5cv3pT3/S7Nmz1bt3b335y1+OyS9xPOqXBDN8AwCdlstT12ORrM9uozPPPFMXXXSRli5dqssuu0yffPKJNm/erNdff12BQEALFy7UypUrdeDAAXm9Xnm9XmVltd4LI0k7d+5Uv3791LdvX3vbyJEjo/51jkdUoWTr1q2aOHGirr76akl1Y1ErVqzQ9u3bmz1m8eLF6tevnz1mNXDgQG3fvl2PPvpos6HEupCWysrKaJoZFat4WjU9JQDQeRlGm4ZQUsGtt96qu+++W08++aSKi4vVv39/XXHFFfqf//kfPfHEE1q0aJGGDBmirKwszZkzRz6fr03nNSPMbbGGiBIlquGbiy++WH/84x/18ccfS5L++te/6u2339b48eObPWbr1q0aO3Zs2LZx48Zp+/btqq2tjXjMggULlJ2dbT8KCgqiaWZU6lff0FMCAEh9N9xwg5xOp1544QUtW7ZMt9xyiwzD0ObNmzVx4kRNnTpV55xzjgYMGKDdu3e3+byDBg1SWVmZDh6s7zHaunVrPH6FZkUVSu69915NmTJFZ555plwul4YNG6Y5c+ZoypQpzR5TUVGhPn36hG3r06eP/H6/Dh8+HPGY+fPn68iRI/Zj//790TQzKlnUKQEAdCBdunTRjTfeqPvvv18HDx7U9OnTJUmnnnqqNmzYoC1btmjnzp2aOXOmKioq2nzeK6+8UmeccYZuvvlm/fWvf9XmzZv13e9+N06/RWRRhZKVK1dq+fLleuGFF/Tee+9p2bJlevTRR7Vs2bIWj2vc/WN1ETXXLeR2u9WtW7ewR7xkWTfkY04JAKCDuPXWW/XZZ5/pyiuvVL9+/SRJ3//+93Xuuedq3LhxGjNmjHJzczVp0qQ2n9PhcGjNmjXyer06//zzddttt+nHP/5xnH6DyKKaU/Ltb39b9913n2666SZJ0pAhQ7Rv3z4tWLBA06ZNi3hMbm5uk6R26NAhpaWlqWfPnu1sdux4Qj0lrL4BAHQUI0eObDIH5KSTTmq1HHzDVTuStHfv3rDXp59+ujZv3hy2LdJck3iJqqekurpaDkf4IU6ns8UlwSNHjmxSfOX111/XiBEj5HK5ovn4uLB6SpjoCgBAckUVSq655hr9+Mc/1rp167R3716tWbNGjz/+uK699lp7n/nz5+vmm2+2X8+aNUv79u3T3LlztXPnTi1dulTPPvus5s2bF7vf4jjUF09j+AYAgGSKavjm5z//ub7//e9r9uzZOnTokPLz8zVz5kw98MAD9j7l5eUqKyuzXxcVFenVV1/VN7/5TT355JPKz8/Xz372s5SoUSLVT3St8vplmmbClz8BAIA6hpnIwaJ2qqysVHZ2to4cORLzSa+VNbU6+4evS5J2/b//kjvNGdPzAwBSS01NjUpLS1VUVKSMjIxkN6fDa+l6Rvv3u9PfkM/jqg8h3JQPADqPDvD/5B1CLK9jpw8laU6H3Gl1l+ELJrsCwAnPWmRRXV2d5JacGKzrGIvFK53+LsFSXal5r9+naia7AsAJz+l0qnv37jp06JAkyePxMJ+wHUzTVHV1tQ4dOqTu3bvL6Tz+6Q+EEtXVKvm0ip4SAOgscnNzJckOJmi/7t2729fzeBFK1KBWCaXmAaBTMAxDeXl5ysnJafY+bGidy+WKSQ+JhVCiBrVKmOgKAJ2K0+mM6R9VHJ9OP9FVkjzplJoHACDZCCVi+AYAgFRAKBGl5gEASAWEEoWXmgcAAMlBKJHkSWeiKwAAyUYokdQl1FPCnBIAAJKHUKL6nhKKpwEAkDyEEtWVmZdEmXkAAJKIUKK6MvMSE10BAEgmQonq65RUMacEAICkIZSovk5JNatvAABIGkKJ6svMM9EVAIDkIZSoQU8JE10BAEgaQokaVHT1+WWaZpJbAwBA50QoUf1EV9OUjtXSWwIAQDIQSiRlupz2c0rNAwCQHIQSSQ6Hoax0Ss0DAJBMhJIQj5tS8wAAJBOhJKS+p4ThGwAAkoFQEmItC6bUPAAAyUEoCbFLzTPRFQCApCCUhDSsVQIAABKPUBLise9/QygBACAZCCUh1kTXKia6AgCQFISSECa6AgCQXISSEGuiK0uCAQBIDkJJiCc00ZXiaQAAJAehJKSLNdGV1TcAACQFoSTEk26VmWf4BgCAZCCUhNhl5hm+AQAgKQglIfbqGya6AgCQFISSELuiKz0lAAAkBaEkxJPORFcAAJKJUBLSxc0N+QAASCZCSYgnNNH1WG1AgaCZ5NYAAND5EEpCrImuEkM4AAAkA6EkxJ3mkNNhSKLUPAAAyUAoCTEMwx7CodQ8AACJRyhpwC41z2RXAAASjlDSgNVTUsWcEgAAEo5Q0oBd1ZXhGwAAEo5Q0kBWOqXmAQBIFkJJA1apeW7KBwBA4hFKGrBKzbP6BgCAxCOUNGDNKaFOCQAAiUcoaSArnTsFAwCQLISSBjzW6huWBAMAkHCEkga62BNdGb4BACDRCCUNMNEVAIDkIZQ0YC8JZqIrAAAJRyhpoL54Gj0lAAAkGqGkAcrMAwCQPFGFksLCQhmG0eRx1113Rdy/pKQk4v4fffRRTBofa/WhhOEbAAASLS2anbdt26ZAoP4P9t/+9jd96Utf0vXXX9/icbt27VK3bt3s1717946ymYlh1SmpZvgGAICEiyqUNA4TCxcu1CmnnKLRo0e3eFxOTo66d+8edeMSzUNPCQAASdPuOSU+n0/Lly/XjBkzZBhGi/sOGzZMeXl5uuKKK7Rx48ZWz+31elVZWRn2SIQuoYmuvkBQPn8wIZ8JAADqtDuUrF27Vp9//rmmT5/e7D55eXlasmSJVq1apdWrV+uMM87QFVdcobfeeqvFcy9YsEDZ2dn2o6CgoL3NjEpmaPhGko6xLBgAgIQyTNM023PguHHjlJ6erldeeSWq46655hoZhqGXX3652X28Xq+8Xq/9urKyUgUFBTpy5EjY3JR4OP27r8kXCOpP912uk7tnxvWzAAA4kVVWVio7O7vNf7+jmlNi2bdvn9544w2tXr066mMvvPBCLV++vMV93G633G53e5p23LLcTvmqg6pmWTAAAAnVruGb4uJi5eTk6Oqrr4762B07digvL689H5sQlJoHACA5ou4pCQaDKi4u1rRp05SWFn74/PnzdeDAAT3//POSpEWLFqmwsFCDBw+2J8auWrVKq1atik3r44BS8wAAJEfUoeSNN95QWVmZZsyY0eS98vJylZWV2a99Pp/mzZunAwcOKDMzU4MHD9a6des0fvz442t1HFHVFQCA5Gj3RNdEinaizPGY+qv/09v/OKwnbjxH1w7rG9fPAgDgRBbt32/ufdOIJ7QsmAJqAAAkFqGkkS6h4RtKzQMAkFiEkkY8oYmuX9BTAgBAQhFKGrEmulKnBACAxCKUNJIVqlNSxZJgAAASilDSSP1EV3pKAABIJEJJI0x0BQAgOQgljXjs4mkM3wAAkEiEkkayrOEbekoAAEgoQkkjlJkHACA5CCWNWKtvuCEfAACJRShppL54Gj0lAAAkEqGkkfrVNwF1gHsVAgBwwiCUNGLVKQkETXn9wSS3BgCAzoNQ0ognNKdEYrIrAACJRChpxOkwlOmq6y1hsisAAIlDKIkgi8muAAAkHKEkAk86peYBAEg0QkkEWZSaBwAg4QglEWRxp2AAABKOUBKB3VPCRFcAABKGUBKBNdGVOSUAACQOoSQCa6Irq28AAEgcQkkEdql5JroCAJAwhJIIrFLzVQzfAACQMISSCOqXBBNKAABIFEJJBPaSYFbfAACQMISSCDz0lAAAkHCEkgiy0pnoCgBAohFKIrDqlDDRFQCAxCGURMBEVwAAEo9QEoGHia4AACQcoSSC+uJp9JQAAJAohJIIrDLzVb6AgkEzya0BAKBzIJREYPWUSNKxWoZwAABIBEJJBBkuhwyj7jkrcAAASAxCSQSGYdi1SqqoVQIAQEIQSpph1yphsisAAAlBKGmGXdWVZcEAACQEoaQZHnpKAABIKEJJM+w5JUx0BQAgIQglzchyc1M+AAASiVDSDKvU/BcM3wAAkBCEkmbYpeYZvgEAICEIJc2wSs1/wfANAAAJQShphlWnhJ4SAAASg1DSDGuiKxVdAQBIDEJJM7LSqVMCAEAiEUqaYfeUMHwDAEBCEEqa4aHMPAAACUUoaQY35AMAILEIJc1g+AYAgMQilDTDvkswq28AAEgIQkkzKDMPAEBiEUqaYZWZ9/qD8geCSW4NAAAnPkJJMzyhia6SVF3LEA4AAPFGKGlGutOhNIchiRU4AAAkAqGkGYZhUGoeAIAEIpS0wCo1z035AACIv6hCSWFhoQzDaPK46667mj1m06ZNGj58uDIyMjRgwAAtXrz4uBudKJ5QTwkrcAAAiL+oQsm2bdtUXl5uPzZs2CBJuv766yPuX1paqvHjx+uSSy7Rjh07dP/99+uee+7RqlWrjr/lCWAN31CrBACA+EuLZufevXuHvV64cKFOOeUUjR49OuL+ixcvVr9+/bRo0SJJ0sCBA7V9+3Y9+uij+vKXv9y+FieQfadghm8AAIi7ds8p8fl8Wr58uWbMmCHDMCLus3XrVo0dOzZs27hx47R9+3bV1tY2e26v16vKysqwRzJYN+VjoisAAPHX7lCydu1aff7555o+fXqz+1RUVKhPnz5h2/r06SO/36/Dhw83e9yCBQuUnZ1tPwoKCtrbzOPSxc1EVwAAEqXdoeTZZ5/VVVddpfz8/Bb3a9yLYppmxO0NzZ8/X0eOHLEf+/fvb28zjwsTXQEASJyo5pRY9u3bpzfeeEOrV69ucb/c3FxVVFSEbTt06JDS0tLUs2fPZo9zu91yu93taVpMWaXmq30M3wAAEG/t6ikpLi5WTk6Orr766hb3GzlypL1Cx/L6669rxIgRcrlc7fnohLJuykdFVwAA4i/qUBIMBlVcXKxp06YpLS28o2X+/Pm6+eab7dezZs3Svn37NHfuXO3cuVNLly7Vs88+q3nz5h1/yxMgy57oSigBACDeog4lb7zxhsrKyjRjxowm75WXl6usrMx+XVRUpFdffVUlJSUaOnSoHnroIf3sZz/rEMuBpfo6JVUM3wAAEHdRzykZO3asPVm1seeee67JttGjR+u9996LumGpIIvVNwAAJAz3vmmBVafkC+qUAAAQd4SSFtg9JcwpAQAg7gglLbAmurIkGACA+COUtMDqKaF4GgAA8UcoaYF9l2AmugIAEHeEkhZYE11rA6a8foZwAACIJ0JJC7JCFV0lqZoVOAAAxBWhpAVpTofcaXWXqIohHAAA4opQ0gq7qis9JQAAxBWhpBXWChx6SgAAiC9CSSvsWiX0lAAAEFeEklZ40qlVAgBAIhBKWkGtEgAAEoNQ0gpr+KaKUvMAAMQVoaQVHmuiK8M3AADEFaGkFV2s4RtCCQAAcUUoaYWH4RsAABKCUNIKq9Q8wzcAAMQXoaQVdkVXekoAAIgrQkkrrIquzCkBACC+CCWtsOaUUDwNAID4IpS0wl59w/ANAABxRShphYeJrgAAJAShpBX1E10JJQAAxBOhpBX2vW+4SzAAAHFFKGmFXafE55dpmkluDQAAJy5CSSusnpKgKdXUBpPcGgAATlyEklZkupz2c+aVAAAQP4SSVjgcBitwAABIAEJJG9grcJjsCgBA3BBK2sCa7FrN8A0AAHFDKGkDSs0DABB/hJI2oNQ8AADxRyhpA4+bia4AAMQboaQNstKtia6EEgAA4oVQ0gZZVk8JwzcAAMQNoaQNPPSUAAAQd4SSNmCiKwAA8UcoaQMmugIAEH+EkjawJ7pSPA0AgLghlLQBZeYBAIg/QkkbUGYeAID4I5S0gcdtlZmnpwQAgHghlLRBFzc9JQAAxBuhpA3q65TQUwIAQLwQStqAMvMAAMQfoaQNrDLzx2oDCgTNJLcGAIATE6GkDawlwVJdMAEAALFHKGkDd5pDDqPuOUM4AADEB6GkDQzDaFBAjVACAEA8EErayJrsyk35AACID0JJG1k35fuCnhIAAOKCUNJGXdxWTwmhBACAeCCUtJEn3eopYfgGAIB4IJS0kd1TwvANAABxQShpI7vUPBNdAQCIC0JJG1lVXVkSDABAfBBK2si+/w0TXQEAiAtCSRt57DklDN8AABAPUYeSAwcOaOrUqerZs6c8Ho+GDh2qd999t9n9S0pKZBhGk8dHH310XA1PtKx0hm8AAIintNZ3qffZZ59p1KhRuuyyy/Taa68pJydHn3zyibp3797qsbt27VK3bt3s17179466sclkl5ln+AYAgLiIKpT85Cc/UUFBgYqLi+1thYWFbTo2JyenTeFFkrxer7xer/26srIymmbGhTXRlTLzAADER1TDNy+//LJGjBih66+/Xjk5ORo2bJieeeaZNh07bNgw5eXl6YorrtDGjRtb3HfBggXKzs62HwUFBdE0My6sJcGUmQcAID6iCiV79uzR008/rdNOO03r16/XrFmzdM899+j5559v9pi8vDwtWbJEq1at0urVq3XGGWfoiiuu0FtvvdXsMfPnz9eRI0fsx/79+6NpZlx0YaIrAABxZZimabZ15/T0dI0YMUJbtmyxt91zzz3atm2btm7d2uYPveaaa2QYhl5++eU27V9ZWans7GwdOXIkbF5KIu0o+0zXPrVFfXtk6u17L09KGwAA6Eii/fsdVU9JXl6eBg0aFLZt4MCBKisri6qRF154oXbv3h3VMclmT3Rl+AYAgLiIKpSMGjVKu3btCtv28ccfq3///lF96I4dO5SXlxfVMclWv/qG4RsAAOIhqtU33/zmN3XRRRfp4Ycf1g033KA///nPWrJkiZYsWWLvM3/+fB04cMCeZ7Jo0SIVFhZq8ODB8vl8Wr58uVatWqVVq1bF9jeJM6tOic8fVG0gKJeTunMAAMRSVKHkvPPO05o1azR//nz96Ec/UlFRkRYtWqSvfvWr9j7l5eVhwzk+n0/z5s3TgQMHlJmZqcGDB2vdunUaP3587H6LBLBW30h1k12zPYQSAABiKaqJrsmSChNdJen0774mXyCoLfddrvzumUlrBwAAHUFcJ7p2dh7uFAwAQNwQSqJQf6dgJrsCABBrhJIo2KXm6SkBACDmCCVRoNQ8AADxQyiJgl1qnuEbAABijlASBU+oVkmVj54SAABijVASBUrNAwAQP4SSKGTZS4IZvgEAINYIJVGwlgRXM3wDAEDMEUqiUL/6hp4SAABijVASBbtOCT0lAADEHKEkCvUTXekpAQAg1gglUWD1DQAA8UMoiUJWOsM3AADEC6EkCpSZBwAgfgglUaDMPAAA8UMoiYLHLp5GTwkAALFGKImCVTytyheQaZpJbg0AACcWQkkUrDolgaAprz+Y5NYAAHBiIZREwZroKjGvBACAWCOURMHpMJThqrtkzCsBACC2CCVRslbgVFGrBACAmCKURMkawqHUPAAAsUUoiZInnWXBAADEA6EkSvUF1AglAADEEqEkSh7uFAwAQFwQSqJk3ZSPia4AAMQWoSRKWfSUAAAQF4SSKFk9JcwpAQAgtgglUbJ6Sr5g9Q0AADFFKImSFUqqGb4BACCmCCVRsuqUfMHwDQAAMUUoiVJ9TwmhBACAWCKURCnLKjPPXYIBAIgpQkmUPG7KzAMAEA+EkijVl5mnpwQAgFgilESJG/IBABAfhJIo2XNKCCUAAMQUoSRK9uqb2oCCQTPJrQEA4MRBKIlSVmiiq2lKNX7mlQAAECuEkihlupwyjLrnlJoHACB2CCVRMgzDnldCqXkAAGKHUNIO9gocSs0DABAzhJJ2sGqVVNFTAgBAzBBK2sGu6kpPCQAAMUMoaQcPtUoAAIg5Qkk72KXmGb4BACBmCCXtwERXAABij1DSDpSaBwAg9ggl7WCVmq/iTsEAAMQMoaQdrFLz1fSUAAAQM4SSdrBW33zBRFcAAGKGUNIOXayeEia6AgAQM4SSdrDrlDCnBACAmCGUtIM1p4TVNwAAxA6hpB3s1TeEEgAAYoZQ0g7W8E01wzcAAMQMoaQdutBTAgBAzEUdSg4cOKCpU6eqZ8+e8ng8Gjp0qN59990Wj9m0aZOGDx+ujIwMDRgwQIsXL253g1MBZeYBAIi9tGh2/uyzzzRq1Chddtlleu2115STk6NPPvlE3bt3b/aY0tJSjR8/XrfffruWL1+uP/3pT5o9e7Z69+6tL3/5y8fb/qSw5pTU1AYVCJpyOowktwgAgI4vqlDyk5/8RAUFBSouLra3FRYWtnjM4sWL1a9fPy1atEiSNHDgQG3fvl2PPvpoBw4lTvt5lc+vbhmuJLYGAIATQ1TDNy+//LJGjBih66+/Xjk5ORo2bJieeeaZFo/ZunWrxo4dG7Zt3Lhx2r59u2prayMe4/V6VVlZGfZIJelOh9JCvSPVVHUFACAmogole/bs0dNPP63TTjtN69ev16xZs3TPPffo+eefb/aYiooK9enTJ2xbnz595Pf7dfjw4YjHLFiwQNnZ2fajoKAgmmbGnWEY9rySL5jsCgBATEQVSoLBoM4991w9/PDDGjZsmGbOnKnbb79dTz/9dIvHGUb4nAvTNCNut8yfP19HjhyxH/v374+mmQlhrcCh1DwAALERVSjJy8vToEGDwrYNHDhQZWVlzR6Tm5urioqKsG2HDh1SWlqaevbsGfEYt9utbt26hT1SjcdeFszwDQAAsRBVKBk1apR27doVtu3jjz9W//79mz1m5MiR2rBhQ9i2119/XSNGjJDL1XEniGalU2oeAIBYiiqUfPOb39Q777yjhx9+WP/4xz/0wgsvaMmSJbrrrrvsfebPn6+bb77Zfj1r1izt27dPc+fO1c6dO7V06VI9++yzmjdvXux+iySwS80zfAMAQExEFUrOO+88rVmzRitWrNBZZ52lhx56SIsWLdJXv/pVe5/y8vKw4ZyioiK9+uqrKikp0dChQ/XQQw/pZz/7WYddDmyh1DwAALEVVZ0SSZowYYImTJjQ7PvPPfdck22jR4/We++9F+1HpTTuFAwAQGxx75t2ymKiKwAAMUUoaSdroitLggEAiA1CSTtZc0oongYAQGwQStqpvngawzcAAMQCoaSdPEx0BQAgpggl7dSFOiUAAMQUoaSdrDklrL4BACA2CCXtxOobAABii1DSTtQpAQAgtggl7WRVdK08Vqu9h6vkDwST3CIAADq2qMvMo07XjLo7HB/1+jXm0RKlOQz1O8mjwl5ZKuyZpaJeHhX16qLCXh7lZ2fK4TCS3GIAAFIboaSd+nTL0KzRp2jjR4e099Mqef1B7TlcpT2Hq5rsm57mUP+TPCrqlaWiXll2cCns5VF2pkuZLqcMg9ACAOjcDNM0zWQ3ojWVlZXKzs7WkSNH1K1bt2Q3p4lg0FR5ZY32Hq5S6eEq7T1cpb2f1gWU/f+pVm2g5UvsMOrmqHRxp9k/65471cXtUhe3U1mh97pmpCkrPa3B/nXvedKd6uJOkyc9TelpjMoBAJIv2r/f9JTEgMNh6OTumTq5e6ZGndor7D1/IKiDn9eo9NMqO7SUhkLLPz87pkDQVNCUjtb4dbQmNit50p0OedzOUHhxyhP6aYUZK8Bkptdt87id8qSH9ksPbQ/tb+2T4XLQmwMAiCtCSZylOR3q19Ojfj09Gn1677D3gkFTx2oDqvL6ddTrV5XXry+8fn1R41eVz68vvIG656Ht9vuhR7U3oCpf3fYqX0A+f91kW18gKF91UJ9X18bs9zAMyeNyyhMKNZ5066dTma7QzwbbrPcz7ddOZboaHNNgH3cagQcAQChJKofDsIdlcmJwvtpA0A4q1aFQUx0KLHXBJRRgvHWvq2vr3q/2BUKPuudVPr+O+QKq8gZ0rLZuybNpqu48cbjXj8OQMl3hoSbD1SDMpKfJ46oPOJmh5+Gv0+xwlGHtG/pJ6AGAjoFQcgJxOR3K9jiU7XHF7Jx2b46vrmfGCi9VvoCO+eoDzTEr2NT66583eT/0Xm3dNqtnJxjHwCPV9fKEBZZQWLGfN3jdMPTU7+tQpqv+tRV6Gj7PSHMozclcHgA4HoQStKhhb466xvbc/kBQx2obBBpfoMFrv47VNtgeCjjW+9Z7NbX1ocd+HtrHF6odY5qyzxNPLqcRHlxcTmWkO5XpcjTZlpEWHnYyXOEhKCPNOrZhGHIow0XPD4ATF6EESZPmdKir02HXfIk1O/TUBlTjC6q61m8HmZragI75Gr4fsJ9bAadh8DnW4H1vbTAsHFlqA6ZqA7GbsNwSd5pD7jSHHWbqnzvkTgv9bLg9zSm3y2H/rDu+7v3w7fXb7PNY2+gNAhBnhBKcsOIdeiTJNE15/UE7xFhBpaY22CTMNAxDNf768GMfGzrGOq5un6C8tXVDXoFg/dJyrz8orz+oygQEoIacDsMOKO5mAk7DEGPt43LWPdKdhtKc1mtDLqdDaaGf9muHQ+lphtIc9fulOR1KcxhyOowGPx1yOo0m211Oh5wOQ07DoGgh0MEQSoDjYBiG3VvRPc6fVRsI2gGoJhR8vP7Qz9pA/Xv+ut6cuufBuuehbV5//X5WsPFGfG7tH7SHwSQpEDQbDIXFbnVXvDgMKc1RF3yswJLmqA9D4c9DAchRH5Qav2+FpYZByj4utH96mqPBPvVBq2kIqz9/k/dCx6c5DbkcDsIVOg1CCdBBWH/IurgT+59tMGjWBxV/UD5/gzDUYHvD0NMk7PgD8gdM+QNB+UI/awNB1QZN1fqD8gdN1Qbqzm09rxsOC8ofqNsWCJr1PwNB+7W1LWLbzdAS+Q5+30yHUdfz53JYPU2th6OGYSrNYYQFrbQmx0Q+vi7IGXKG3rN6qKzzWW2wzlm/PdST1eQ89a/THAZzo9AEoQRAixwOw16CnapMs1FosX8GQ2HIVG3oeW0o5PhDwcffaHtdEKrbXhsKUHXbw8NU4+N9EY6rbfR5vgbnqwtgkY9tLGhKPn9QPklSB09YDVjhpGmocYQNyTlD252OSMN4DbY3Gc4L37/JMKAz8n6ORp/tNBo8b9gmh0NOh+R0OOx90pyGHEaDdjjrz9FwyNHah16wcIQSAB2eYYT+mKVubmoz0zTDeov8zYal+qDlbyYoRQxgQbOVYxruV9cjVRswQz9Dr4OmAg0+o+E+VvvCQmEzPVmBUHj01r1K5GVOGYah+gAUmgcV9jwUdhwONd1mv2fIGRqqdDgUCk0OOQ3Z+1lhqbmg5TAMXTe8r846OTup14NQAgApxDDq56KcKBr2ZPmDpgKhQBUWdBoFH6u3y3r4g8EIvWF1oafh9tpAUMFQsLMCUSAUsgKBCMc3fB2o/5yAKQWs5w0fpml/ZsBs3EZTwUY/rbY3k8tkmtbKveTfhm54/x6EEgDAie1E6slqr+aGGBuHrrr7oZkKBNXgeV0ACgYbPleEbaFAZIaHpMbhKfx8ofBlmjqtT5dkXyZCCQAA8UYwa5sTp38QAAB0aIQSAACQEgglAAAgJRBKAABASiCUAACAlEAoAQAAKYFQAgAAUgKhBAAApARCCQAASAmEEgAAkBIIJQAAICUQSgAAQEoglAAAgJTQIe4SbJqmJKmysjLJLQEAAG1l/d22/o63pkOEkqNHj0qSCgoKktwSAAAQraNHjyo7O7vV/QyzrfEliYLBoA4ePKiuXbvKMIyYnbeyslIFBQXav3+/unXrFrPznui4bu3DdWsfrlv0uGbtw3Vrn5aum2maOnr0qPLz8+VwtD5jpEP0lDgcDvXt2zdu5+/WrRtfwHbgurUP1619uG7R45q1D9etfZq7bm3pIbEw0RUAAKQEQgkAAEgJnTqUuN1u/eAHP5Db7U52UzoUrlv7cN3ah+sWPa5Z+3Dd2ieW161DTHQFAAAnvk7dUwIAAFIHoQQAAKQEQgkAAEgJhBIAAJASCCUAACAldOpQ8tRTT6moqEgZGRkaPny4Nm/enOwmpbQf/vCHMgwj7JGbm5vsZqWct956S9dcc43y8/NlGIbWrl0b9r5pmvrhD3+o/Px8ZWZmasyYMfrwww+T09gU0do1mz59epPv3oUXXpicxqaIBQsW6LzzzlPXrl2Vk5OjSZMmadeuXWH78F1rqi3Xje9bU08//bTOPvtsu2rryJEj9dprr9nvx+q71mlDycqVKzVnzhx997vf1Y4dO3TJJZfoqquuUllZWbKbltIGDx6s8vJy+/HBBx8ku0kpp6qqSuecc45+8YtfRHz/kUce0eOPP65f/OIX2rZtm3Jzc/WlL33JvvFkZ9TaNZOk//qv/wr77r366qsJbGHq2bRpk+666y6988472rBhg/x+v8aOHauqqip7H75rTbXlukl83xrr27evFi5cqO3bt2v79u26/PLLNXHiRDt4xOy7ZnZS559/vjlr1qywbWeeeaZ53333JalFqe8HP/iBec455yS7GR2KJHPNmjX262AwaObm5poLFy60t9XU1JjZ2dnm4sWLk9DC1NP4mpmmaU6bNs2cOHFiUtrTURw6dMiUZG7atMk0Tb5rbdX4upkm37e26tGjh/mrX/0qpt+1TtlT4vP59O6772rs2LFh28eOHastW7YkqVUdw+7du5Wfn6+ioiLddNNN2rNnT7Kb1KGUlpaqoqIi7Lvndrs1evRovnutKCkpUU5Ojk4//XTdfvvtOnToULKblFKOHDkiSTrppJMk8V1rq8bXzcL3rXmBQEAvvviiqqqqNHLkyJh+1zplKDl8+LACgYD69OkTtr1Pnz6qqKhIUqtS3wUXXKDnn39e69ev1zPPPKOKigpddNFF+vTTT5PdtA7D+n7x3YvOVVddpd/85jd688039dhjj2nbtm26/PLL5fV6k920lGCapubOnauLL75YZ511liS+a20R6bpJfN+a88EHH6hLly5yu92aNWuW1qxZo0GDBsX0u5YWs9Z2QIZhhL02TbPJNtS76qqr7OdDhgzRyJEjdcopp2jZsmWaO3duElvW8fDdi86NN95oPz/rrLM0YsQI9e/fX+vWrdPkyZOT2LLUcPfdd+v999/X22+/3eQ9vmvNa+668X2L7IwzztBf/vIXff7551q1apWmTZumTZs22e/H4rvWKXtKevXqJafT2STBHTp0qEnSQ/OysrI0ZMgQ7d69O9lN6TCs1Up8945PXl6e+vfvz3dP0te//nW9/PLL2rhxo/r27Wtv57vWsuauWyR83+qkp6fr1FNP1YgRI7RgwQKdc845+ulPfxrT71qnDCXp6ekaPny4NmzYELZ9w4YNuuiii5LUqo7H6/Vq586dysvLS3ZTOoyioiLl5uaGffd8Pp82bdrEdy8Kn376qfbv39+pv3umaeruu+/W6tWr9eabb6qoqCjsfb5rkbV23SLh+xaZaZryer2x/a7FaBJuh/Piiy+aLpfLfPbZZ82///3v5pw5c8ysrCxz7969yW5ayvrWt75llpSUmHv27DHfeecdc8KECWbXrl25Zo0cPXrU3LFjh7ljxw5Tkvn444+bO3bsMPft22eapmkuXLjQzM7ONlevXm1+8MEH5pQpU8y8vDyzsrIyyS1Pnpau2dGjR81vfetb5pYtW8zS0lJz48aN5siRI82TTz65U1+zO++808zOzjZLSkrM8vJy+1FdXW3vw3etqdauG9+3yObPn2++9dZbZmlpqfn++++b999/v+lwOMzXX3/dNM3Yfdc6bSgxTdN88sknzf79+5vp6enmueeeG7YkDE3deOONZl5enulyucz8/Hxz8uTJ5ocffpjsZqWcjRs3mpKaPKZNm2aaZt1SzR/84Admbm6u6Xa7zUsvvdT84IMPktvoJGvpmlVXV5tjx441e/fubbpcLrNfv37mtGnTzLKysmQ3O6kiXS9JZnFxsb0P37WmWrtufN8imzFjhv33snfv3uYVV1xhBxLTjN13zTBN02xnzw0AAEDMdMo5JQAAIPUQSgAAQEoglAAAgJRAKAEAACmBUAIAAFICoQQAAKQEQgkAAEgJhBIAAJASCCUAACAlEEoAAEBKIJQAAICU8P8BT5YHlh2M3GEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_UcLyHcQMGu0",
    "outputId": "4b56200c-1dc4-4286-b51a-d19e9c75ddd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0613160106'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "E6-6g-sW3zeg"
   },
   "source": [
    "We can see that with Sigmoid the model seems to perform slightly worse, but this should not be a big problem. Then the decision might depend more on whether the use of it is common and justifiable. I don't think it's common, and the use of it does not seem to be analytically necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNJ0DAlaWrEE"
   },
   "outputs": [],
   "source": [
    "class ResAECluster(ResAE): \n",
    "    def __init__(self, input_dim=INPUT_DIM, inter_dim1=INTER_DIM_1, inter_dim2=INTER_DIM_2, inter_dim3=INTER_DIM_3, latent_dim=LATENT_DIM, output_dim=OUTPUT_DIM): \n",
    "        super().__init__(input_dim, inter_dim1, inter_dim2, inter_dim3, latent_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        org_size = x.size()\n",
    "        batch = org_size[0]\n",
    "        x = x.view(batch, -1)\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        # mu, logvar = h.chunk(2, dim=1)\n",
    "        # z = self.reparameterise(mu, logvar)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFrvzaGC45Gi"
   },
   "outputs": [],
   "source": [
    "seq = \"_01_05\"\n",
    "tags = pd.read_csv(tags_name + seq + \".csv\")\n",
    "gsds = GroundedSoundDataset(tags, test_name + seq + \".npy\")\n",
    "eval_loader = DataLoader(gsds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "QARr-C_qpBLb",
    "outputId": "acd5e8ab-1ef6-4968-c205-b437ecfd7b8e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'model_english_0130021416_29_full'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXLqyT3PYFkR"
   },
   "outputs": [],
   "source": [
    "# model_name = last_model_name\n",
    "model_name = \"model_english_0130021416_13_full\"\n",
    "model_path = save_dir + model_name + \".pt\"\n",
    "state = torch.load(model_path)\n",
    "model = ResAECluster()\n",
    "model.load_state_dict(state)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "hiddens = None\n",
    "tags = None\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (s, e, t) in enumerate(eval_loader):\n",
    "        s = s.to(device)\n",
    "        hidden = model(s)\n",
    "        hidden = hidden.cpu().data.numpy()\n",
    "\n",
    "        if hiddens is not None: \n",
    "            hiddens = np.concatenate((hiddens, hidden), axis=0)\n",
    "            tags = np.concatenate((tags, t), axis=0)\n",
    "        else: \n",
    "            hiddens = hidden\n",
    "            tags = t\n",
    "num_phones = np.unique(tags).shape[0]\n",
    "kmeansmodel = KMeans(n_clusters=num_phones) # , random_state=0\n",
    "clusters = kmeansmodel.fit_predict(hiddens)\n",
    "np.save(save_dir + model_name + seq + \"_hiddenclusters.npy\", clusters)\n",
    "np.save(save_dir + model_name + seq + \"_hiddenrepresentation.npy\", hiddens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzTuc2Mz6niT"
   },
   "outputs": [],
   "source": [
    "h, c, v = homogeneity_completeness_v_measure(tags, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ogsEovzEbpc",
    "outputId": "3cd43d32-f30c-4fb3-f5eb-945d6dd0ecc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_01_05 0.30813685860010276 0.2726217590636009 0.2892933823757265\n"
     ]
    }
   ],
   "source": [
    "print(seq, h, c, v) # trained on sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMFbNimpx-iJ"
   },
   "outputs": [],
   "source": [
    "# _17_24 0.3429902101084872 0.329164358854651 0.33593508938856537   # 256+8\n",
    "# _17_24 0.3071758873778334 0.2958512436337788 0.3014072290332542   # 128+4\n",
    "# _17_24 0.3048181747064378 0.303971996633573 0.3043944976042278    # 128+2\n",
    "# _17_24 0.3109960687106377 0.3020004935745723 0.3064322772063619   # 256+2, 2res\n",
    "# _17_24 0.27632046463064963 0.29796767719078493 0.28673608598337974    # 256+64+2, 2res, new model\n",
    "# _17_24 0.29619001674434664 0.30940705212658304 0.30265430485339223    # 256+64+4, 0res\n",
    "# _17_24 0.3394207670351701 0.3356821861468344 0.33754112484613674      # 256+64+4, 2res\n",
    "# _17_24 0.3246121630042821 0.3173438869288583 0.32093687897447765  # 256+3, 2res, not very bad. So we may try this. This is error, decoder only having 1 res\n",
    "# _17_24 0.3227539602867097 0.32256957773330264 0.3226617426690128  # 256+3, 2res\n",
    "# _17_24 0.3403517130774138 0.33762198107176034 0.33898135170147237 # 256+3, 1res\n",
    "# _17_24 0.3202704367215642 0.31097127191607643 0.3155523587925454  # 256+3, 0res\n",
    "\n",
    "\n",
    "# _01_05 0.30784101366300043 0.2717512535534188 0.28867252408265254"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_gYDGP0Cdf1o"
   },
   "source": [
    "总的来说分成四个，神经网层来进行降维处理，得到的损失比较大，但是 hcv值倒是接近不过，如果能尽量的接近原作的模型结构，我们就不去动它了，所以可能目前来看最好的是保留两个降为层加上两个残差层，最后从256降到2，也许是最好的结果当然降到4也是可以的，都是比较低的维度，不过如果我们想要直接能够，在，可视的空间中画出这些点来，2或者3可能会比四更好一些。 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nTTdgp_HokAn"
   },
   "source": [
    "从使用不同数量残插块儿的实验结果来看，是由一个残渣块，应该是最好的解决方式，使用零个或两个第三个都可能是都会使hcv值相对降低。由此来看在选择，隐性层纬度为三的情况下，我们应该选择适用一个参差款。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BxdQ9f85WY1K"
   },
   "source": [
    "### Conclusion\n",
    "Adding new data slightly improves the performance of the model in HCV score, in addition, shuffling the training data largely lowers the HGV score perhaps we should discuss this phenomenon and justify use no shuffling during training. Perhaps this is because of some sort of phonotactics or naturalness of sound streams. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kjoJ2fFKpmVC"
   },
   "source": [
    "Good news is that for the English model it performs similar well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjPWgjpid7PR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
