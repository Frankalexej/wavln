{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Phone Training - English\n",
    "Version 2:  This version has a core structure using HM-RNN. Unlike traditional approaches, our model can automatically detect boundaries. It is trainable and updates the upper layer only upon detecting boundaries. This makes our model suitable for detecting boundaries and capturing the representations of sub-segments based on these detected boundaries. In essence, our model performs boundary detection and representation learning simultaneously.\n",
    "\n",
    "Version 3: this version completed the coding of the core model structure as well as the dataloading, preprocessing, padding and loss calculation processes. At present we only try mel->model -> mel structure, since wav <> wav would introduce extra complexion. In addition, our model will process padded multi-batch tensors as normal but count for the paddings (ignore paddings) during calculation. \n",
    "\n",
    "Version 4: this version is testing whether our hmrnn is not working. It imports a modified version of model: model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('./multiscale_rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import pickle\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it, masked_loss\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PhonLearn_Net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规范用语；规定两种方式：全加载；按rec加载（舍弃了按chunk加载，处理起来更简单）\n",
    "# RandomPhoneDataset; AnnoPhoneDataset; AnnoSeqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset that loads cutted wave files from disk and returns input-output pairs for\n",
    "    training autoencoder. \n",
    "    \n",
    "    Version 3: wav -> mel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load_dir, load_control_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the class by reading a CSV file and merging the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        The function reads the CSV file from the provided control path, extracts the \"rec\" and \"idx\" columns,\n",
    "        and concatenates the values from these columns using an underscore. It then appends the \".wav\" extension\n",
    "        to each of the merged strings and converts the merged pandas Series to a list, which is assigned to\n",
    "        the 'dataset' attribute of the class.\n",
    "\n",
    "        Args:\n",
    "        load_dir (str): The directory containing the files to load.\n",
    "        load_control_path (str): The path to the CSV file containing the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        Attributes:\n",
    "        dataset (list): A list of merged strings from the \"rec\" and \"idx\" columns, with the \".wav\" extension.\n",
    "        \"\"\"\n",
    "        control_file = pd.read_csv(load_control_path)\n",
    "        control_file = control_file[control_file['n_frames'] > 0]\n",
    "        control_file = control_file[control_file['duration'] <= 2.0]\n",
    "        \n",
    "        # Extract the \"rec\" and \"idx\" columns\n",
    "        rec_col = control_file['rec'].astype(str)\n",
    "        idx_col = control_file['idx'].astype(str).str.zfill(8)\n",
    "        \n",
    "        # Merge the two columns by concatenating the strings with '_' and append extension name\n",
    "        merged_col = rec_col + '_' + idx_col + \".wav\"\n",
    "        \n",
    "        self.dataset = merged_col.tolist()\n",
    "        self.load_dir = load_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of input-output pairs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (input_data, output_data) for the given index.\n",
    "\n",
    "        The function first checks if the provided index is a tensor, and if so, converts it to a list.\n",
    "        It then constructs the file path for the .wav file using the dataset attribute and the provided index.\n",
    "        The .wav file is loaded using torchaudio, and its data is normalized. If a transform is provided,\n",
    "        the data is transformed using the specified transform. Finally, the input_data and output_data are\n",
    "        set to the same data (creating a tuple), and the tuple is returned.\n",
    "\n",
    "        Args:\n",
    "        idx (int or torch.Tensor): The index of the desired data.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing input_data and output_data, both of which are the audio data\n",
    "               from the .wav file at the specified index.\n",
    "\n",
    "        Note: \n",
    "        This function assumes that the class has the following attributes:\n",
    "        - self.load_dir (str): The directory containing the .wav files.\n",
    "        - self.dataset (list): A list of .wav file names.\n",
    "        - self.transform (callable, optional): An optional transform to apply to the audio data.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        wav_name = os.path.join(self.load_dir,\n",
    "                                self.dataset[idx])\n",
    "        \n",
    "        data, sample_rate = torchaudio.load(wav_name, normalize=True)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        # # Prepare for possible in-out discrepencies in the future\n",
    "        # input_data = data\n",
    "        # output_data = data\n",
    "        \n",
    "        return data\n",
    "\n",
    "def collate_fn(xx):\n",
    "    # only working for one data at the moment\n",
    "    batch_first = True\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=batch_first, padding_value=0)\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "\n",
    "class MyTransform(nn.Module): \n",
    "    def __init__(self, sample_rate, n_fft): \n",
    "        super().__init__()\n",
    "        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft)\n",
    "    \n",
    "    def forward(self, waveform): \n",
    "        mel_spec = self.transform(waveform)\n",
    "        mel_spec = mel_spec.squeeze()\n",
    "        mel_spec = mel_spec.permute(1, 0) # (F, L) -> (L, F)\n",
    "        return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: THIS IS HOW WE CAN CREATE THE DATASET AND DATALOADER\n",
    "# sample_rate = 16000\n",
    "# n_fft = 400\n",
    "\n",
    "# transform = MyTransform(sample_rate, n_fft)\n",
    "\n",
    "# ds = PhoneDataset(phone_seg_random_path, phone_seg_random_log_path + \"s0101a.csv\", transform=transform)\n",
    "\n",
    "# test_dl = DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# indata, in_lens = next(iter(test_dl))\n",
    "\n",
    "# print(indata.shape, in_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# SEGMENTS_IN_CHUNK = 100  # set_size\n",
    "\n",
    "INPUT_DIM = 128\n",
    "OUTPUT_DIM = 128\n",
    "\n",
    "INTER_DIM_0 = 64\n",
    "INTER_DIM_1 = 32\n",
    "INTER_DIM_2 = 8\n",
    "INTER_DIM_3 = 3\n",
    "\n",
    "SIZE_LIST = [INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "\n",
    "LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "recon_loss = nn.MSELoss(reduction='mean')\n",
    "model = PhonLearn_Net(1.0, SIZE_LIST, in_size=INPUT_DIM, \n",
    "                      in2_size=INTER_DIM_0, hid_size=INTER_DIM_3, out_size=OUTPUT_DIM)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhonLearn_Net(\n",
       "  (encoder): Encoder(\n",
       "    (lin_1): LinearPack(\n",
       "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (rnn): HM_LSTM(\n",
       "      (cell_1): HM_LSTMCell()\n",
       "      (cell_2): HM_LSTMCell()\n",
       "    )\n",
       "    (lin_2): LinearPack(\n",
       "      (linear): Linear(in_features=8, out_features=3, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lin_1): LinearPack(\n",
       "      (linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (rnn): LSTM(8, 64, batch_first=True)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (w_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (w_k): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (w_v): Linear(in_features=3, out_features=64, bias=True)\n",
       "    )\n",
       "    (lin_2): LinearPack(\n",
       "      (linear): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56149"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NNHDmuigs8OB"
   },
   "outputs": [],
   "source": [
    "# Define recorders of training hists, for ease of extension\n",
    "class Recorder: \n",
    "    def __init__(self, IOPath): \n",
    "        self.record = []\n",
    "        self.IOPath = IOPath\n",
    "\n",
    "    def save(self): \n",
    "        pass\n",
    "    \n",
    "    def append(self, content): \n",
    "        self.record.append(content)\n",
    "    \n",
    "    def get(self): \n",
    "        return self.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kGMfle47t3Hj"
   },
   "outputs": [],
   "source": [
    "class LossRecorder(Recorder): \n",
    "    def read(self): \n",
    "        # only used by loss hists \n",
    "        with open(self.IOPath, 'rb') as f:\n",
    "            self.record = pickle.load(f)\n",
    "    \n",
    "    def save(self): \n",
    "        with open(self.IOPath, 'wb') as file:\n",
    "            pickle.dump(self.record, file)\n",
    "\n",
    "\n",
    "class HistRecorder(Recorder):     \n",
    "    def save(self): \n",
    "        with open(self.IOPath, \"a\") as txt:\n",
    "            txt.write(\"\\n\".join(self.record))\n",
    "    \n",
    "    def print(self, content): \n",
    "        self.append(content)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "ts = str(get_timestamp())\n",
    "# ts = \"0130021416\"\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "READ = False\n",
    "# READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    valid_losses.read()\n",
    "    train_losses.read()\n",
    "\n",
    "    # model_name = last_model_name\n",
    "    model_name = \"PT_0130021416_9.pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "    model = PhonLearn_Net()\n",
    "    model.load_state_dict(state)\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\torchaudio\\functional\\functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mytrans = MyTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT)\n",
    "ds = PhoneDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "train_len = int(0.8 * len(ds))\n",
    "valid_len = len(ds) - train_len\n",
    "\n",
    "# Randomly split the dataset into train and validation sets\n",
    "train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "valid_num = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BASE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            batch = x.size(0)\n",
    "            # print(x.size(1))\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            recon_x, _ = model(x, x_mask) # _ = hidden, z_1, z_2\n",
    "            \n",
    "            loss = masked_loss(recon_loss, recon_x, x, x_mask)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            # loss = loss / batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 20 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "                gc.collect()\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            batch = x.size(0)\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            recon_x, _ = model(x, x_mask) # _ = hidden, z_1, z_2\n",
    "\n",
    "            loss = masked_loss(recon_loss, recon_x, x, x_mask)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            if idx % 20 == 0:\n",
    "                # \\t Recon {recon / batch: .3f} \\t KL {kl / batch: .3f}\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "                gc.collect()\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss  47.236 in Step 0\n",
      "Training loss  95.204 in Step 20\n",
      "Training loss  27.758 in Step 40\n",
      "Training loss  84.375 in Step 60\n",
      "Training loss  87.464 in Step 80\n",
      "Training loss  19.101 in Step 100\n",
      "Training loss  23.252 in Step 120\n",
      "Training loss  94.971 in Step 140\n",
      "Training loss  76.845 in Step 160\n",
      "Training loss  144.011 in Step 180\n",
      "Training loss  690.503 in Step 200\n",
      "Training loss  31.237 in Step 220\n",
      "Training loss  103.132 in Step 240\n",
      "Training loss  50.907 in Step 260\n",
      "Training loss  70.931 in Step 280\n",
      "Training loss  136.197 in Step 300\n",
      "Training loss  62.136 in Step 320\n",
      "Training loss  371.980 in Step 340\n",
      "Training loss  206.231 in Step 360\n",
      "Training loss  14.090 in Step 380\n",
      "Training loss  60.188 in Step 400\n",
      "Training loss  178.188 in Step 420\n",
      "Training loss  50.812 in Step 440\n",
      "Training loss  238.896 in Step 460\n",
      "Training loss  31.765 in Step 480\n",
      "Training loss  36.669 in Step 500\n",
      "Training loss  356.313 in Step 520\n",
      "Training loss  90.099 in Step 540\n",
      "Training loss  191.524 in Step 560\n",
      "Training loss  74.530 in Step 580\n",
      "Training loss  83.751 in Step 600\n",
      "Training loss  314.480 in Step 620\n",
      "Training loss  11.667 in Step 640\n",
      "Training loss  46.939 in Step 660\n",
      "Training loss  41.369 in Step 680\n",
      "Training loss  60.293 in Step 700\n",
      "Training loss  63.247 in Step 720\n",
      "Training loss  42.656 in Step 740\n",
      "Training loss  460.633 in Step 760\n",
      "Training loss  16.916 in Step 780\n",
      "Training loss  54.050 in Step 800\n",
      "Training loss  201.967 in Step 820\n",
      "Training loss  664.519 in Step 840\n",
      "Training loss  134.700 in Step 860\n",
      "Training loss  288.863 in Step 880\n",
      "Training loss  336.659 in Step 900\n",
      "Training loss  116.861 in Step 920\n",
      "Training loss  188.230 in Step 940\n",
      "Training loss  23.158 in Step 960\n",
      "Training loss  88.218 in Step 980\n",
      "Training loss  157.341 in Step 1000\n",
      "Training loss  312.414 in Step 1020\n",
      "Training loss  119.129 in Step 1040\n",
      "Training loss  15.316 in Step 1060\n",
      "Training loss  136.848 in Step 1080\n",
      "Training loss  98.402 in Step 1100\n",
      "Training loss  75.745 in Step 1120\n",
      "Training loss  54.135 in Step 1140\n",
      "Training loss  266.925 in Step 1160\n",
      "Training loss  52.058 in Step 1180\n",
      "Training loss  215.896 in Step 1200\n",
      "Training loss  31.846 in Step 1220\n",
      "Training loss  76.527 in Step 1240\n",
      "Training loss  68.060 in Step 1260\n",
      "Training loss  219.614 in Step 1280\n",
      "Training loss  34.094 in Step 1300\n",
      "Training loss  112.335 in Step 1320\n",
      "Training loss  166.549 in Step 1340\n",
      "Training loss  72.734 in Step 1360\n",
      "Training loss  180.193 in Step 1380\n",
      "Training loss  125.147 in Step 1400\n",
      "Training loss  88.925 in Step 1420\n",
      "Training loss  56.901 in Step 1440\n",
      "Training loss  306.788 in Step 1460\n",
      "Training loss  114.178 in Step 1480\n",
      "Training loss  124.085 in Step 1500\n",
      "Training loss  30.878 in Step 1520\n",
      "Training loss  198.657 in Step 1540\n",
      "Training loss  326.098 in Step 1560\n",
      "Training loss  175.884 in Step 1580\n",
      "Training loss  58.932 in Step 1600\n",
      "Training loss  194.225 in Step 1620\n",
      "Training loss  228.087 in Step 1640\n",
      "Training loss  44.981 in Step 1660\n",
      "Training loss  27.857 in Step 1680\n",
      "Training loss  123.901 in Step 1700\n",
      "Training loss  30.860 in Step 1720\n",
      "Training loss  42.682 in Step 1740\n",
      "Training loss  200.798 in Step 1760\n",
      "※※※Training loss  136.656※※※\n",
      "Training timepoint saved\n",
      "Valid loss  16.162 in Step 0\n",
      "Valid loss  14.890 in Step 20\n",
      "Valid loss  85.626 in Step 40\n",
      "Valid loss  29.717 in Step 60\n",
      "Valid loss  443.691 in Step 80\n",
      "Valid loss  170.497 in Step 100\n",
      "Valid loss  180.606 in Step 120\n",
      "Valid loss  671.214 in Step 140\n",
      "Valid loss  232.565 in Step 160\n",
      "Valid loss  84.061 in Step 180\n",
      "Valid loss  43.664 in Step 200\n",
      "Valid loss  62.988 in Step 220\n",
      "Valid loss  48.376 in Step 240\n",
      "Valid loss  75.734 in Step 260\n",
      "Valid loss  60.405 in Step 280\n",
      "Valid loss  35.888 in Step 300\n",
      "Valid loss  266.949 in Step 320\n",
      "Valid loss  68.891 in Step 340\n",
      "Valid loss  204.937 in Step 360\n",
      "Valid loss  243.646 in Step 380\n",
      "Valid loss  154.849 in Step 400\n",
      "Valid loss  31.354 in Step 420\n",
      "Valid loss  246.505 in Step 440\n",
      "※※※Valid loss  138.144※※※\n",
      "Epoch 1\n",
      "Training loss  29.715 in Step 0\n",
      "Training loss  22.770 in Step 20\n",
      "Training loss  83.565 in Step 40\n",
      "Training loss  31.642 in Step 60\n",
      "Training loss  411.083 in Step 80\n",
      "Training loss  88.772 in Step 100\n",
      "Training loss  13.542 in Step 120\n",
      "Training loss  278.910 in Step 140\n",
      "Training loss  33.994 in Step 160\n",
      "Training loss  42.409 in Step 180\n",
      "Training loss  25.747 in Step 200\n",
      "Training loss  106.350 in Step 220\n",
      "Training loss  17.785 in Step 240\n",
      "Training loss  55.474 in Step 260\n",
      "Training loss  31.226 in Step 280\n",
      "Training loss  7.431 in Step 300\n",
      "Training loss  28.900 in Step 320\n",
      "Training loss  103.276 in Step 340\n",
      "Training loss  159.243 in Step 360\n",
      "Training loss  18.893 in Step 380\n",
      "Training loss  78.224 in Step 400\n",
      "Training loss  128.011 in Step 420\n",
      "Training loss  156.632 in Step 440\n",
      "Training loss  198.582 in Step 460\n",
      "Training loss  30.243 in Step 480\n",
      "Training loss  61.475 in Step 500\n",
      "Training loss  208.236 in Step 520\n",
      "Training loss  43.637 in Step 540\n",
      "Training loss  39.580 in Step 560\n",
      "Training loss  7.723 in Step 580\n",
      "Training loss  183.048 in Step 600\n",
      "Training loss  600.990 in Step 620\n",
      "Training loss  363.129 in Step 640\n",
      "Training loss  110.358 in Step 660\n",
      "Training loss  35.052 in Step 680\n",
      "Training loss  100.849 in Step 700\n",
      "Training loss  238.196 in Step 720\n",
      "Training loss  373.364 in Step 740\n",
      "Training loss  190.873 in Step 760\n",
      "Training loss  88.152 in Step 780\n",
      "Training loss  189.530 in Step 800\n",
      "Training loss  278.527 in Step 820\n",
      "Training loss  53.857 in Step 840\n",
      "Training loss  107.517 in Step 860\n",
      "Training loss  46.400 in Step 880\n",
      "Training loss  58.435 in Step 900\n",
      "Training loss  52.766 in Step 920\n",
      "Training loss  142.395 in Step 940\n",
      "Training loss  248.867 in Step 960\n",
      "Training loss  33.806 in Step 980\n",
      "Training loss  672.298 in Step 1000\n",
      "Training loss  35.851 in Step 1020\n",
      "Training loss  33.041 in Step 1040\n",
      "Training loss  103.905 in Step 1060\n",
      "Training loss  26.632 in Step 1080\n",
      "Training loss  64.319 in Step 1100\n",
      "Training loss  496.536 in Step 1120\n",
      "Training loss  12.012 in Step 1140\n",
      "Training loss  19.532 in Step 1160\n",
      "Training loss  91.752 in Step 1180\n",
      "Training loss  92.635 in Step 1200\n",
      "Training loss  55.427 in Step 1220\n",
      "Training loss  24.323 in Step 1240\n",
      "Training loss  236.424 in Step 1260\n",
      "Training loss  27.561 in Step 1280\n",
      "Training loss  117.113 in Step 1300\n",
      "Training loss  281.744 in Step 1320\n",
      "Training loss  139.002 in Step 1340\n",
      "Training loss  153.611 in Step 1360\n",
      "Training loss  208.506 in Step 1380\n",
      "Training loss  178.076 in Step 1400\n",
      "Training loss  108.249 in Step 1420\n",
      "Training loss  187.531 in Step 1440\n",
      "Training loss  170.427 in Step 1460\n",
      "Training loss  35.023 in Step 1480\n",
      "Training loss  54.107 in Step 1500\n",
      "Training loss  132.932 in Step 1520\n",
      "Training loss  53.574 in Step 1540\n",
      "Training loss  27.347 in Step 1560\n",
      "Training loss  45.009 in Step 1580\n",
      "Training loss  173.207 in Step 1600\n",
      "Training loss  254.304 in Step 1620\n",
      "Training loss  23.774 in Step 1640\n",
      "Training loss  984.220 in Step 1660\n",
      "Training loss  18.487 in Step 1680\n",
      "Training loss  209.035 in Step 1700\n",
      "Training loss  260.767 in Step 1720\n",
      "Training loss  191.829 in Step 1740\n",
      "Training loss  133.886 in Step 1760\n",
      "※※※Training loss  135.706※※※\n",
      "Training timepoint saved\n",
      "Valid loss  18.602 in Step 0\n",
      "Valid loss  265.034 in Step 20\n",
      "Valid loss  43.510 in Step 40\n",
      "Valid loss  47.993 in Step 60\n",
      "Valid loss  182.934 in Step 80\n",
      "Valid loss  44.364 in Step 100\n",
      "Valid loss  70.914 in Step 120\n",
      "Valid loss  83.497 in Step 140\n",
      "Valid loss  59.568 in Step 160\n",
      "Valid loss  130.513 in Step 180\n",
      "Valid loss  48.651 in Step 200\n",
      "Valid loss  46.024 in Step 220\n",
      "Valid loss  76.769 in Step 240\n",
      "Valid loss  50.486 in Step 260\n",
      "Valid loss  68.204 in Step 280\n",
      "Valid loss  32.815 in Step 300\n",
      "Valid loss  34.720 in Step 320\n",
      "Valid loss  92.546 in Step 340\n",
      "Valid loss  14.505 in Step 360\n",
      "Valid loss  83.973 in Step 380\n",
      "Valid loss  63.356 in Step 400\n",
      "Valid loss  1062.916 in Step 420\n",
      "Valid loss  49.942 in Step 440\n",
      "※※※Valid loss  136.979※※※\n",
      "Epoch 2\n",
      "Training loss  500.648 in Step 0\n",
      "Training loss  136.190 in Step 20\n",
      "Training loss  162.531 in Step 40\n",
      "Training loss  53.296 in Step 60\n",
      "Training loss  164.885 in Step 80\n",
      "Training loss  123.695 in Step 100\n",
      "Training loss  36.126 in Step 120\n",
      "Training loss  51.773 in Step 140\n",
      "Training loss  673.734 in Step 160\n",
      "Training loss  39.799 in Step 180\n",
      "Training loss  10.991 in Step 200\n",
      "Training loss  22.690 in Step 220\n",
      "Training loss  39.664 in Step 240\n",
      "Training loss  175.285 in Step 260\n",
      "Training loss  39.761 in Step 280\n",
      "Training loss  25.319 in Step 300\n",
      "Training loss  116.503 in Step 320\n",
      "Training loss  82.406 in Step 340\n",
      "Training loss  32.147 in Step 360\n",
      "Training loss  431.708 in Step 380\n",
      "Training loss  144.747 in Step 400\n",
      "Training loss  102.707 in Step 420\n",
      "Training loss  90.368 in Step 440\n",
      "Training loss  531.601 in Step 460\n",
      "Training loss  506.960 in Step 480\n",
      "Training loss  45.177 in Step 500\n",
      "Training loss  237.398 in Step 520\n",
      "Training loss  83.160 in Step 540\n",
      "Training loss  23.580 in Step 560\n",
      "Training loss  90.651 in Step 580\n",
      "Training loss  29.231 in Step 600\n",
      "Training loss  28.193 in Step 620\n",
      "Training loss  300.428 in Step 640\n",
      "Training loss  31.641 in Step 660\n",
      "Training loss  68.628 in Step 680\n",
      "Training loss  35.259 in Step 700\n",
      "Training loss  91.231 in Step 720\n",
      "Training loss  67.157 in Step 740\n",
      "Training loss  53.170 in Step 760\n",
      "Training loss  321.742 in Step 780\n",
      "Training loss  159.575 in Step 800\n",
      "Training loss  544.577 in Step 820\n",
      "Training loss  117.031 in Step 840\n",
      "Training loss  105.015 in Step 860\n",
      "Training loss  74.747 in Step 880\n",
      "Training loss  31.923 in Step 900\n",
      "Training loss  126.330 in Step 920\n",
      "Training loss  76.217 in Step 940\n",
      "Training loss  25.303 in Step 960\n",
      "Training loss  91.646 in Step 980\n",
      "Training loss  52.903 in Step 1000\n",
      "Training loss  31.751 in Step 1020\n",
      "Training loss  13.609 in Step 1040\n",
      "Training loss  89.664 in Step 1060\n",
      "Training loss  119.785 in Step 1080\n",
      "Training loss  13.501 in Step 1100\n",
      "Training loss  269.320 in Step 1120\n",
      "Training loss  163.954 in Step 1140\n",
      "Training loss  179.949 in Step 1160\n",
      "Training loss  51.070 in Step 1180\n",
      "Training loss  69.195 in Step 1200\n",
      "Training loss  117.578 in Step 1220\n",
      "Training loss  63.740 in Step 1240\n",
      "Training loss  628.006 in Step 1260\n",
      "Training loss  385.526 in Step 1280\n",
      "Training loss  83.412 in Step 1300\n",
      "Training loss  262.718 in Step 1320\n",
      "Training loss  357.965 in Step 1340\n",
      "Training loss  127.757 in Step 1360\n",
      "Training loss  27.099 in Step 1380\n",
      "Training loss  38.349 in Step 1400\n",
      "Training loss  23.918 in Step 1420\n",
      "Training loss  118.860 in Step 1440\n",
      "Training loss  70.825 in Step 1460\n",
      "Training loss  208.277 in Step 1480\n",
      "Training loss  93.800 in Step 1500\n",
      "Training loss  134.881 in Step 1520\n",
      "Training loss  183.114 in Step 1540\n",
      "Training loss  907.856 in Step 1560\n",
      "Training loss  163.158 in Step 1580\n",
      "Training loss  76.930 in Step 1600\n",
      "Training loss  112.292 in Step 1620\n",
      "Training loss  36.706 in Step 1640\n",
      "Training loss  190.928 in Step 1660\n",
      "Training loss  856.879 in Step 1680\n",
      "Training loss  78.038 in Step 1700\n",
      "Training loss  58.674 in Step 1720\n",
      "Training loss  136.347 in Step 1740\n",
      "Training loss  38.950 in Step 1760\n",
      "※※※Training loss  135.699※※※\n",
      "Training timepoint saved\n",
      "Valid loss  57.149 in Step 0\n",
      "Valid loss  434.238 in Step 20\n",
      "Valid loss  1011.890 in Step 40\n",
      "Valid loss  27.740 in Step 60\n",
      "Valid loss  33.302 in Step 80\n",
      "Valid loss  81.006 in Step 100\n",
      "Valid loss  20.157 in Step 120\n",
      "Valid loss  52.192 in Step 140\n",
      "Valid loss  17.881 in Step 160\n",
      "Valid loss  34.513 in Step 180\n",
      "Valid loss  407.473 in Step 200\n",
      "Valid loss  346.458 in Step 220\n",
      "Valid loss  87.851 in Step 240\n",
      "Valid loss  23.985 in Step 260\n",
      "Valid loss  728.116 in Step 280\n",
      "Valid loss  152.663 in Step 300\n",
      "Valid loss  541.528 in Step 320\n",
      "Valid loss  58.717 in Step 340\n",
      "Valid loss  205.245 in Step 360\n",
      "Valid loss  17.223 in Step 380\n",
      "Valid loss  38.515 in Step 400\n",
      "Valid loss  53.986 in Step 420\n",
      "Valid loss  189.745 in Step 440\n",
      "※※※Valid loss  136.768※※※\n",
      "Epoch 3\n",
      "Training loss  95.982 in Step 0\n",
      "Training loss  26.819 in Step 20\n",
      "Training loss  56.298 in Step 40\n",
      "Training loss  102.630 in Step 60\n",
      "Training loss  43.861 in Step 80\n",
      "Training loss  52.075 in Step 100\n",
      "Training loss  19.040 in Step 120\n",
      "Training loss  324.114 in Step 140\n",
      "Training loss  201.926 in Step 160\n",
      "Training loss  14.785 in Step 180\n",
      "Training loss  220.351 in Step 200\n",
      "Training loss  267.899 in Step 220\n",
      "Training loss  101.556 in Step 240\n",
      "Training loss  59.982 in Step 260\n",
      "Training loss  103.153 in Step 280\n",
      "Training loss  168.120 in Step 300\n",
      "Training loss  9.774 in Step 320\n",
      "Training loss  60.207 in Step 340\n",
      "Training loss  60.420 in Step 360\n",
      "Training loss  38.263 in Step 380\n",
      "Training loss  22.978 in Step 400\n",
      "Training loss  202.796 in Step 420\n",
      "Training loss  254.765 in Step 440\n",
      "Training loss  25.076 in Step 460\n",
      "Training loss  58.843 in Step 480\n",
      "Training loss  80.426 in Step 500\n",
      "Training loss  304.837 in Step 520\n",
      "Training loss  210.906 in Step 540\n",
      "Training loss  126.150 in Step 560\n",
      "Training loss  28.589 in Step 580\n",
      "Training loss  272.500 in Step 600\n",
      "Training loss  108.287 in Step 620\n",
      "Training loss  18.272 in Step 640\n",
      "Training loss  23.526 in Step 660\n",
      "Training loss  207.317 in Step 680\n",
      "Training loss  81.102 in Step 700\n",
      "Training loss  43.193 in Step 720\n",
      "Training loss  35.741 in Step 740\n",
      "Training loss  111.859 in Step 760\n",
      "Training loss  100.381 in Step 780\n",
      "Training loss  369.796 in Step 800\n",
      "Training loss  545.168 in Step 820\n",
      "Training loss  117.035 in Step 840\n",
      "Training loss  201.026 in Step 860\n",
      "Training loss  102.960 in Step 880\n",
      "Training loss  20.729 in Step 900\n",
      "Training loss  57.341 in Step 920\n",
      "Training loss  52.530 in Step 940\n",
      "Training loss  495.612 in Step 960\n",
      "Training loss  16.810 in Step 980\n",
      "Training loss  33.974 in Step 1000\n",
      "Training loss  291.638 in Step 1020\n",
      "Training loss  509.049 in Step 1040\n",
      "Training loss  22.938 in Step 1060\n",
      "Training loss  216.062 in Step 1080\n",
      "Training loss  50.481 in Step 1100\n",
      "Training loss  37.792 in Step 1120\n",
      "Training loss  129.255 in Step 1140\n",
      "Training loss  122.430 in Step 1160\n",
      "Training loss  290.048 in Step 1180\n",
      "Training loss  105.757 in Step 1200\n",
      "Training loss  125.042 in Step 1220\n",
      "Training loss  173.592 in Step 1240\n",
      "Training loss  43.383 in Step 1260\n",
      "Training loss  42.002 in Step 1280\n",
      "Training loss  16.172 in Step 1300\n",
      "Training loss  17.417 in Step 1320\n",
      "Training loss  56.368 in Step 1340\n",
      "Training loss  92.899 in Step 1360\n",
      "Training loss  110.272 in Step 1380\n",
      "Training loss  152.062 in Step 1400\n",
      "Training loss  166.422 in Step 1420\n",
      "Training loss  49.224 in Step 1440\n",
      "Training loss  22.954 in Step 1460\n",
      "Training loss  43.307 in Step 1480\n",
      "Training loss  26.741 in Step 1500\n",
      "Training loss  57.370 in Step 1520\n",
      "Training loss  23.851 in Step 1540\n",
      "Training loss  70.886 in Step 1560\n",
      "Training loss  252.187 in Step 1580\n",
      "Training loss  16.552 in Step 1600\n",
      "Training loss  116.427 in Step 1620\n",
      "Training loss  115.516 in Step 1640\n",
      "Training loss  51.926 in Step 1660\n",
      "Training loss  16.523 in Step 1680\n",
      "Training loss  799.557 in Step 1700\n",
      "Training loss  76.258 in Step 1720\n",
      "Training loss  213.446 in Step 1740\n",
      "Training loss  62.250 in Step 1760\n",
      "※※※Training loss  134.983※※※\n",
      "Training timepoint saved\n",
      "Valid loss  22.533 in Step 0\n",
      "Valid loss  21.866 in Step 20\n",
      "Valid loss  80.070 in Step 40\n",
      "Valid loss  105.218 in Step 60\n",
      "Valid loss  52.679 in Step 80\n",
      "Valid loss  196.537 in Step 100\n",
      "Valid loss  693.826 in Step 120\n",
      "Valid loss  52.591 in Step 140\n",
      "Valid loss  90.104 in Step 160\n",
      "Valid loss  118.091 in Step 180\n",
      "Valid loss  49.794 in Step 200\n",
      "Valid loss  88.648 in Step 220\n",
      "Valid loss  127.179 in Step 240\n",
      "Valid loss  34.887 in Step 260\n",
      "Valid loss  109.765 in Step 280\n",
      "Valid loss  33.271 in Step 300\n",
      "Valid loss  131.610 in Step 320\n",
      "Valid loss  240.937 in Step 340\n",
      "Valid loss  24.212 in Step 360\n",
      "Valid loss  42.170 in Step 380\n",
      "Valid loss  80.089 in Step 400\n",
      "Valid loss  26.978 in Step 420\n",
      "Valid loss  184.625 in Step 440\n",
      "※※※Valid loss  136.899※※※\n",
      "Epoch 4\n",
      "Training loss  105.983 in Step 0\n",
      "Training loss  36.620 in Step 20\n",
      "Training loss  108.646 in Step 40\n",
      "Training loss  322.630 in Step 60\n",
      "Training loss  129.331 in Step 80\n",
      "Training loss  143.366 in Step 100\n",
      "Training loss  82.068 in Step 120\n",
      "Training loss  37.083 in Step 140\n",
      "Training loss  63.586 in Step 160\n",
      "Training loss  30.751 in Step 180\n",
      "Training loss  1253.619 in Step 200\n",
      "Training loss  163.270 in Step 220\n",
      "Training loss  135.883 in Step 240\n",
      "Training loss  18.657 in Step 260\n",
      "Training loss  49.177 in Step 280\n",
      "Training loss  39.909 in Step 300\n",
      "Training loss  17.109 in Step 320\n",
      "Training loss  35.802 in Step 340\n",
      "Training loss  715.402 in Step 360\n",
      "Training loss  59.791 in Step 380\n",
      "Training loss  465.868 in Step 400\n",
      "Training loss  246.809 in Step 420\n",
      "Training loss  93.454 in Step 440\n",
      "Training loss  84.764 in Step 460\n",
      "Training loss  178.977 in Step 480\n",
      "Training loss  14.990 in Step 500\n",
      "Training loss  62.378 in Step 520\n",
      "Training loss  67.312 in Step 540\n",
      "Training loss  132.326 in Step 560\n",
      "Training loss  213.793 in Step 580\n",
      "Training loss  42.943 in Step 600\n",
      "Training loss  258.617 in Step 620\n",
      "Training loss  202.375 in Step 640\n",
      "Training loss  44.901 in Step 660\n",
      "Training loss  54.647 in Step 680\n",
      "Training loss  16.686 in Step 700\n",
      "Training loss  88.693 in Step 720\n",
      "Training loss  20.513 in Step 740\n",
      "Training loss  16.340 in Step 760\n",
      "Training loss  18.766 in Step 780\n",
      "Training loss  61.366 in Step 800\n",
      "Training loss  68.141 in Step 820\n",
      "Training loss  29.763 in Step 840\n",
      "Training loss  84.582 in Step 860\n",
      "Training loss  507.936 in Step 880\n",
      "Training loss  12.306 in Step 900\n",
      "Training loss  137.868 in Step 920\n",
      "Training loss  296.574 in Step 940\n",
      "Training loss  157.380 in Step 960\n",
      "Training loss  280.301 in Step 980\n",
      "Training loss  116.225 in Step 1000\n",
      "Training loss  76.082 in Step 1020\n",
      "Training loss  96.597 in Step 1040\n",
      "Training loss  175.137 in Step 1060\n",
      "Training loss  395.447 in Step 1080\n",
      "Training loss  88.637 in Step 1100\n",
      "Training loss  12.688 in Step 1120\n",
      "Training loss  25.325 in Step 1140\n",
      "Training loss  91.226 in Step 1160\n",
      "Training loss  84.881 in Step 1180\n",
      "Training loss  78.099 in Step 1200\n",
      "Training loss  89.047 in Step 1220\n",
      "Training loss  37.825 in Step 1240\n",
      "Training loss  150.466 in Step 1260\n",
      "Training loss  192.402 in Step 1280\n",
      "Training loss  74.232 in Step 1300\n",
      "Training loss  43.304 in Step 1320\n",
      "Training loss  216.659 in Step 1340\n",
      "Training loss  86.872 in Step 1360\n",
      "Training loss  14.065 in Step 1380\n",
      "Training loss  131.813 in Step 1400\n",
      "Training loss  8.999 in Step 1420\n",
      "Training loss  141.809 in Step 1440\n",
      "Training loss  232.799 in Step 1460\n",
      "Training loss  80.670 in Step 1480\n",
      "Training loss  429.776 in Step 1500\n",
      "Training loss  130.059 in Step 1520\n",
      "Training loss  18.908 in Step 1540\n",
      "Training loss  252.208 in Step 1560\n",
      "Training loss  131.083 in Step 1580\n",
      "Training loss  123.593 in Step 1600\n",
      "Training loss  38.282 in Step 1620\n",
      "Training loss  151.333 in Step 1640\n",
      "Training loss  154.813 in Step 1660\n",
      "Training loss  9.042 in Step 1680\n",
      "Training loss  721.992 in Step 1700\n",
      "Training loss  203.672 in Step 1720\n",
      "Training loss  253.963 in Step 1740\n",
      "Training loss  66.785 in Step 1760\n",
      "※※※Training loss  135.464※※※\n",
      "Training timepoint saved\n",
      "Valid loss  15.675 in Step 0\n",
      "Valid loss  148.788 in Step 20\n",
      "Valid loss  1092.472 in Step 40\n",
      "Valid loss  313.403 in Step 60\n",
      "Valid loss  29.830 in Step 80\n",
      "Valid loss  97.067 in Step 100\n",
      "Valid loss  52.174 in Step 120\n",
      "Valid loss  39.919 in Step 140\n",
      "Valid loss  27.995 in Step 160\n",
      "Valid loss  17.496 in Step 180\n",
      "Valid loss  29.990 in Step 200\n",
      "Valid loss  26.912 in Step 220\n",
      "Valid loss  34.080 in Step 240\n",
      "Valid loss  223.331 in Step 260\n",
      "Valid loss  135.615 in Step 280\n",
      "Valid loss  47.858 in Step 300\n",
      "Valid loss  763.502 in Step 320\n",
      "Valid loss  40.307 in Step 340\n",
      "Valid loss  323.030 in Step 360\n",
      "Valid loss  167.223 in Step 380\n",
      "Valid loss  28.451 in Step 400\n",
      "Valid loss  36.055 in Step 420\n",
      "Valid loss  54.012 in Step 440\n",
      "※※※Valid loss  137.051※※※\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark: this problem: RuntimeError: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (200, 200) at dimension 2 of input [1, 1, 27]. is consistent across CPU and GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "valid_losses.save()\n",
    "train_losses.save()\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21836c1b280>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_UcLyHcQMGu0",
    "outputId": "4b56200c-1dc4-4286-b51a-d19e9c75ddd9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0130021416'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "E6-6g-sW3zeg"
   },
   "source": [
    "We can see that with Sigmoid the model seems to perform slightly worse, but this should not be a big problem. Then the decision might depend more on whether the use of it is common and justifiable. I don't think it's common, and the use of it does not seem to be analytically necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNJ0DAlaWrEE"
   },
   "outputs": [],
   "source": [
    "class ResAECluster(ResAE): \n",
    "    def __init__(self, input_dim=INPUT_DIM, inter_dim1=INTER_DIM_1, inter_dim2=INTER_DIM_2, inter_dim3=INTER_DIM_3, latent_dim=LATENT_DIM, output_dim=OUTPUT_DIM): \n",
    "        super().__init__(input_dim, inter_dim1, inter_dim2, inter_dim3, latent_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        org_size = x.size()\n",
    "        batch = org_size[0]\n",
    "        x = x.view(batch, -1)\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        # mu, logvar = h.chunk(2, dim=1)\n",
    "        # z = self.reparameterise(mu, logvar)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFrvzaGC45Gi"
   },
   "outputs": [],
   "source": [
    "seq = \"_01_05\"\n",
    "tags = pd.read_csv(tags_name + seq + \".csv\")\n",
    "gsds = GroundedSoundDataset(tags, test_name + seq + \".npy\")\n",
    "eval_loader = DataLoader(gsds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "QARr-C_qpBLb",
    "outputId": "acd5e8ab-1ef6-4968-c205-b437ecfd7b8e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'model_english_0130021416_29_full'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXLqyT3PYFkR"
   },
   "outputs": [],
   "source": [
    "# model_name = last_model_name\n",
    "model_name = \"model_english_0130021416_13_full\"\n",
    "model_path = save_dir + model_name + \".pt\"\n",
    "state = torch.load(model_path)\n",
    "model = ResAECluster()\n",
    "model.load_state_dict(state)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "hiddens = None\n",
    "tags = None\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (s, e, t) in enumerate(eval_loader):\n",
    "        s = s.to(device)\n",
    "        hidden = model(s)\n",
    "        hidden = hidden.cpu().data.numpy()\n",
    "\n",
    "        if hiddens is not None: \n",
    "            hiddens = np.concatenate((hiddens, hidden), axis=0)\n",
    "            tags = np.concatenate((tags, t), axis=0)\n",
    "        else: \n",
    "            hiddens = hidden\n",
    "            tags = t\n",
    "num_phones = np.unique(tags).shape[0]\n",
    "kmeansmodel = KMeans(n_clusters=num_phones) # , random_state=0\n",
    "clusters = kmeansmodel.fit_predict(hiddens)\n",
    "np.save(save_dir + model_name + seq + \"_hiddenclusters.npy\", clusters)\n",
    "np.save(save_dir + model_name + seq + \"_hiddenrepresentation.npy\", hiddens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzTuc2Mz6niT"
   },
   "outputs": [],
   "source": [
    "h, c, v = homogeneity_completeness_v_measure(tags, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ogsEovzEbpc",
    "outputId": "3cd43d32-f30c-4fb3-f5eb-945d6dd0ecc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_01_05 0.30813685860010276 0.2726217590636009 0.2892933823757265\n"
     ]
    }
   ],
   "source": [
    "print(seq, h, c, v) # trained on sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMFbNimpx-iJ"
   },
   "outputs": [],
   "source": [
    "# _17_24 0.3429902101084872 0.329164358854651 0.33593508938856537   # 256+8\n",
    "# _17_24 0.3071758873778334 0.2958512436337788 0.3014072290332542   # 128+4\n",
    "# _17_24 0.3048181747064378 0.303971996633573 0.3043944976042278    # 128+2\n",
    "# _17_24 0.3109960687106377 0.3020004935745723 0.3064322772063619   # 256+2, 2res\n",
    "# _17_24 0.27632046463064963 0.29796767719078493 0.28673608598337974    # 256+64+2, 2res, new model\n",
    "# _17_24 0.29619001674434664 0.30940705212658304 0.30265430485339223    # 256+64+4, 0res\n",
    "# _17_24 0.3394207670351701 0.3356821861468344 0.33754112484613674      # 256+64+4, 2res\n",
    "# _17_24 0.3246121630042821 0.3173438869288583 0.32093687897447765  # 256+3, 2res, not very bad. So we may try this. This is error, decoder only having 1 res\n",
    "# _17_24 0.3227539602867097 0.32256957773330264 0.3226617426690128  # 256+3, 2res\n",
    "# _17_24 0.3403517130774138 0.33762198107176034 0.33898135170147237 # 256+3, 1res\n",
    "# _17_24 0.3202704367215642 0.31097127191607643 0.3155523587925454  # 256+3, 0res\n",
    "\n",
    "\n",
    "# _01_05 0.30784101366300043 0.2717512535534188 0.28867252408265254"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_gYDGP0Cdf1o"
   },
   "source": [
    "总的来说分成四个，神经网层来进行降维处理，得到的损失比较大，但是 hcv值倒是接近不过，如果能尽量的接近原作的模型结构，我们就不去动它了，所以可能目前来看最好的是保留两个降为层加上两个残差层，最后从256降到2，也许是最好的结果当然降到4也是可以的，都是比较低的维度，不过如果我们想要直接能够，在，可视的空间中画出这些点来，2或者3可能会比四更好一些。 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nTTdgp_HokAn"
   },
   "source": [
    "从使用不同数量残插块儿的实验结果来看，是由一个残渣块，应该是最好的解决方式，使用零个或两个第三个都可能是都会使hcv值相对降低。由此来看在选择，隐性层纬度为三的情况下，我们应该选择适用一个参差款。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BxdQ9f85WY1K"
   },
   "source": [
    "### Conclusion\n",
    "Adding new data slightly improves the performance of the model in HCV score, in addition, shuffling the training data largely lowers the HGV score perhaps we should discuss this phenomenon and justify use no shuffling during training. Perhaps this is because of some sort of phonotactics or naturalness of sound streams. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kjoJ2fFKpmVC"
   },
   "source": [
    "Good news is that for the English model it performs similar well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjPWgjpid7PR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
