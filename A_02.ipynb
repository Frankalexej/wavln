{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Run\n",
    "A_02: LHY, but new normalization. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Direct - English\n",
    "Version 1: In this version we make the model \"simple\": make the encoder RNN into normal RNN first and try to see the result.  \n",
    "Version 2: Learning is not very much. Following Dr Coupe's advice we try simpler model structure.   \n",
    "Version 3: A simple trial training with Mel spectrogram instead of MFCC.   \n",
    "Version 4: try to enlarge the hidden dimensions so that we might still make sense of the hidden representation.   \n",
    "Version 5: restore to MFCC, keep 8-dimensional, direct feedin.   \n",
    "Version 6: This time using Wang et al. structure and hyperparams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PhxLearner, SimplerPhxLearner, LHYPhxLearner\n",
    "from my_dataset import DS_Tools\n",
    "from dataset import SeqDataset, MFCCTransform\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from recorder import *\n",
    "from loss import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIM = 39\n",
    "OUTPUT_DIM = 39\n",
    "\n",
    "INTER_DIM_0 = 32\n",
    "INTER_DIM_1 = 16\n",
    "INTER_DIM_2 = 8\n",
    "\n",
    "ENC_SIZE_LIST = [INPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "DEC_SIZE_LIST = [OUTPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.7\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 64\n",
    "\n",
    "LOADER_WORKER = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "masked_recon_loss = MaskedLoss(recon_loss)\n",
    "model_loss = masked_recon_loss\n",
    "\n",
    "model = LHYPhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=1)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LHYPhxLearner(\n",
       "  (encoder): LHYEncoder(\n",
       "    (lin): LinearPack(\n",
       "      (linear): Linear(in_features=39, out_features=32, bias=True)\n",
       "      (relu): Tanh()\n",
       "      (dropout): Dropout(p=0.7, inplace=False)\n",
       "    )\n",
       "    (rnn): LSTM(32, 8, batch_first=True)\n",
       "  )\n",
       "  (decoder): LHYDecoder(\n",
       "    (rnn): LSTM(8, 32, batch_first=True)\n",
       "    (lin): LinearPack(\n",
       "      (linear): Linear(in_features=32, out_features=39, bias=True)\n",
       "      (relu): Tanh()\n",
       "      (dropout): Dropout(p=0.7, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9287"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "# ts = \"0918192113\"\n",
    "stop_epoch = \"149\"\n",
    "ts = str(get_timestamp())\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "\n",
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "READ = False\n",
    "# READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    valid_losses.read()\n",
    "    train_losses.read()\n",
    "\n",
    "    model_raw_name = \"PT_{}_{}_full\".format(ts, stop_epoch)\n",
    "    model_name = model_raw_name + \".pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [],
   "source": [
    "mytrans = MFCCTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT)\n",
    "ds = SeqDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "\n",
    "test = False\n",
    "if test: \n",
    "    use_len = int(0.1 * len(ds))\n",
    "    remain_len = len(ds) - use_len\n",
    "\n",
    "    # Randomly split the dataset into train and validation sets\n",
    "    ds, remain_ds = random_split(ds, [use_len, remain_len])\n",
    "\n",
    "\n",
    "if READ: \n",
    "    valid_ds_indices = DS_Tools.read_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)))\n",
    "    all_indices = list(range(len(ds)))\n",
    "    train_ds_indices = list(set(all_indices).difference(set(valid_ds_indices)))\n",
    "\n",
    "    train_ds = torch.utils.data.Subset(ds, train_ds_indices)\n",
    "    valid_ds = torch.utils.data.Subset(ds, valid_ds_indices)\n",
    "else: \n",
    "    train_len = int(0.8 * len(ds))\n",
    "    valid_len = len(ds) - train_len\n",
    "\n",
    "    # Randomly split the dataset into train and validation sets\n",
    "    train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "    DS_Tools.save_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)), valid_ds.indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=SeqDataset.collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=LOADER_WORKER, collate_fn=SeqDataset.collate_fn)\n",
    "valid_num = len(valid_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BASE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y = x \n",
    "            \n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            # 这个函数计算的是全局梯度范数\n",
    "            # torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            # parameters: an iterable of Variables that will have gradients normalized\n",
    "            # max_norm: max norm of the gradients(阈值设定)\n",
    "            # norm_type: type of the used p-norm. Can be'inf'for infinity norm(定义范数类型)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            y = x    # extract MFCC-only data\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss  1.049 in Step 0\n",
      "Training loss  0.922 in Step 100\n",
      "Training loss  0.860 in Step 200\n",
      "Training loss  0.846 in Step 300\n",
      "Training loss  0.831 in Step 400\n",
      "Training loss  0.829 in Step 500\n",
      "Training loss  0.823 in Step 600\n",
      "Training loss  0.820 in Step 700\n",
      "Training loss  0.818 in Step 800\n",
      "Training loss  0.815 in Step 900\n",
      "Training loss  0.814 in Step 1000\n",
      "Training loss  0.817 in Step 1100\n",
      "Training loss  0.816 in Step 1200\n",
      "Training loss  0.813 in Step 1300\n",
      "Training loss  0.811 in Step 1400\n",
      "Training loss  0.816 in Step 1500\n",
      "Training loss  0.813 in Step 1600\n",
      "Training loss  0.819 in Step 1700\n",
      "※※※Training loss  0.833※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.642 in Step 0\n",
      "Valid loss  0.645 in Step 50\n",
      "Valid loss  0.637 in Step 100\n",
      "Valid loss  0.638 in Step 150\n",
      "Valid loss  0.637 in Step 200\n",
      "Valid loss  0.637 in Step 250\n",
      "Valid loss  0.635 in Step 300\n",
      "Valid loss  0.638 in Step 350\n",
      "Valid loss  0.637 in Step 400\n",
      "※※※Valid loss  0.639※※※\n",
      "Epoch 1\n",
      "Training loss  0.815 in Step 0\n",
      "Training loss  0.814 in Step 100\n",
      "Training loss  0.813 in Step 200\n",
      "Training loss  0.813 in Step 300\n",
      "Training loss  0.809 in Step 400\n",
      "Training loss  0.809 in Step 500\n",
      "Training loss  0.808 in Step 600\n",
      "Training loss  0.814 in Step 700\n",
      "Training loss  0.806 in Step 800\n",
      "Training loss  0.805 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.807 in Step 1100\n",
      "Training loss  0.809 in Step 1200\n",
      "Training loss  0.806 in Step 1300\n",
      "Training loss  0.807 in Step 1400\n",
      "Training loss  0.808 in Step 1500\n",
      "Training loss  0.809 in Step 1600\n",
      "Training loss  0.805 in Step 1700\n",
      "※※※Training loss  0.810※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.634 in Step 0\n",
      "Valid loss  0.635 in Step 50\n",
      "Valid loss  0.628 in Step 100\n",
      "Valid loss  0.629 in Step 150\n",
      "Valid loss  0.629 in Step 200\n",
      "Valid loss  0.628 in Step 250\n",
      "Valid loss  0.626 in Step 300\n",
      "Valid loss  0.629 in Step 350\n",
      "Valid loss  0.629 in Step 400\n",
      "※※※Valid loss  0.630※※※\n",
      "Epoch 2\n",
      "Training loss  0.804 in Step 0\n",
      "Training loss  0.809 in Step 100\n",
      "Training loss  0.806 in Step 200\n",
      "Training loss  0.807 in Step 300\n",
      "Training loss  0.809 in Step 400\n",
      "Training loss  0.808 in Step 500\n",
      "Training loss  0.807 in Step 600\n",
      "Training loss  0.803 in Step 700\n",
      "Training loss  0.808 in Step 800\n",
      "Training loss  0.808 in Step 900\n",
      "Training loss  0.806 in Step 1000\n",
      "Training loss  0.805 in Step 1100\n",
      "Training loss  0.800 in Step 1200\n",
      "Training loss  0.803 in Step 1300\n",
      "Training loss  0.807 in Step 1400\n",
      "Training loss  0.804 in Step 1500\n",
      "Training loss  0.807 in Step 1600\n",
      "Training loss  0.807 in Step 1700\n",
      "※※※Training loss  0.806※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.634 in Step 0\n",
      "Valid loss  0.636 in Step 50\n",
      "Valid loss  0.628 in Step 100\n",
      "Valid loss  0.630 in Step 150\n",
      "Valid loss  0.630 in Step 200\n",
      "Valid loss  0.627 in Step 250\n",
      "Valid loss  0.626 in Step 300\n",
      "Valid loss  0.630 in Step 350\n",
      "Valid loss  0.629 in Step 400\n",
      "※※※Valid loss  0.630※※※\n",
      "Epoch 3\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.803 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.804 in Step 400\n",
      "Training loss  0.804 in Step 500\n",
      "Training loss  0.805 in Step 600\n",
      "Training loss  0.804 in Step 700\n",
      "Training loss  0.806 in Step 800\n",
      "Training loss  0.806 in Step 900\n",
      "Training loss  0.808 in Step 1000\n",
      "Training loss  0.801 in Step 1100\n",
      "Training loss  0.803 in Step 1200\n",
      "Training loss  0.809 in Step 1300\n",
      "Training loss  0.806 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.800 in Step 1700\n",
      "※※※Training loss  0.805※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.638 in Step 0\n",
      "Valid loss  0.639 in Step 50\n",
      "Valid loss  0.631 in Step 100\n",
      "Valid loss  0.633 in Step 150\n",
      "Valid loss  0.634 in Step 200\n",
      "Valid loss  0.631 in Step 250\n",
      "Valid loss  0.630 in Step 300\n",
      "Valid loss  0.634 in Step 350\n",
      "Valid loss  0.633 in Step 400\n",
      "※※※Valid loss  0.634※※※\n",
      "Epoch 4\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.806 in Step 100\n",
      "Training loss  0.801 in Step 200\n",
      "Training loss  0.802 in Step 300\n",
      "Training loss  0.810 in Step 400\n",
      "Training loss  0.805 in Step 500\n",
      "Training loss  0.807 in Step 600\n",
      "Training loss  0.803 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.805 in Step 900\n",
      "Training loss  0.805 in Step 1000\n",
      "Training loss  0.802 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.808 in Step 1300\n",
      "Training loss  0.804 in Step 1400\n",
      "Training loss  0.806 in Step 1500\n",
      "Training loss  0.804 in Step 1600\n",
      "Training loss  0.804 in Step 1700\n",
      "※※※Training loss  0.804※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.639 in Step 0\n",
      "Valid loss  0.640 in Step 50\n",
      "Valid loss  0.632 in Step 100\n",
      "Valid loss  0.634 in Step 150\n",
      "Valid loss  0.635 in Step 200\n",
      "Valid loss  0.632 in Step 250\n",
      "Valid loss  0.631 in Step 300\n",
      "Valid loss  0.635 in Step 350\n",
      "Valid loss  0.634 in Step 400\n",
      "※※※Valid loss  0.635※※※\n",
      "Epoch 5\n",
      "Training loss  0.807 in Step 0\n",
      "Training loss  0.804 in Step 100\n",
      "Training loss  0.810 in Step 200\n",
      "Training loss  0.802 in Step 300\n",
      "Training loss  0.807 in Step 400\n",
      "Training loss  0.804 in Step 500\n",
      "Training loss  0.801 in Step 600\n",
      "Training loss  0.801 in Step 700\n",
      "Training loss  0.806 in Step 800\n",
      "Training loss  0.807 in Step 900\n",
      "Training loss  0.799 in Step 1000\n",
      "Training loss  0.804 in Step 1100\n",
      "Training loss  0.804 in Step 1200\n",
      "Training loss  0.805 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.806 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.803 in Step 1700\n",
      "※※※Training loss  0.804※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.644 in Step 0\n",
      "Valid loss  0.645 in Step 50\n",
      "Valid loss  0.637 in Step 100\n",
      "Valid loss  0.638 in Step 150\n",
      "Valid loss  0.640 in Step 200\n",
      "Valid loss  0.636 in Step 250\n",
      "Valid loss  0.636 in Step 300\n",
      "Valid loss  0.640 in Step 350\n",
      "Valid loss  0.640 in Step 400\n",
      "※※※Valid loss  0.640※※※\n",
      "Epoch 6\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.800 in Step 100\n",
      "Training loss  0.807 in Step 200\n",
      "Training loss  0.803 in Step 300\n",
      "Training loss  0.804 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.804 in Step 600\n",
      "Training loss  0.801 in Step 700\n",
      "Training loss  0.800 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.803 in Step 1000\n",
      "Training loss  0.802 in Step 1100\n",
      "Training loss  0.805 in Step 1200\n",
      "Training loss  0.799 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.806 in Step 1600\n",
      "Training loss  0.806 in Step 1700\n",
      "※※※Training loss  0.803※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.643 in Step 0\n",
      "Valid loss  0.644 in Step 50\n",
      "Valid loss  0.636 in Step 100\n",
      "Valid loss  0.637 in Step 150\n",
      "Valid loss  0.639 in Step 200\n",
      "Valid loss  0.635 in Step 250\n",
      "Valid loss  0.635 in Step 300\n",
      "Valid loss  0.638 in Step 350\n",
      "Valid loss  0.639 in Step 400\n",
      "※※※Valid loss  0.639※※※\n",
      "Epoch 7\n",
      "Training loss  0.801 in Step 0\n",
      "Training loss  0.801 in Step 100\n",
      "Training loss  0.802 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.808 in Step 600\n",
      "Training loss  0.802 in Step 700\n",
      "Training loss  0.804 in Step 800\n",
      "Training loss  0.800 in Step 900\n",
      "Training loss  0.800 in Step 1000\n",
      "Training loss  0.797 in Step 1100\n",
      "Training loss  0.802 in Step 1200\n",
      "Training loss  0.803 in Step 1300\n",
      "Training loss  0.808 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.806 in Step 1600\n",
      "Training loss  0.804 in Step 1700\n",
      "※※※Training loss  0.803※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.643 in Step 0\n",
      "Valid loss  0.645 in Step 50\n",
      "Valid loss  0.636 in Step 100\n",
      "Valid loss  0.637 in Step 150\n",
      "Valid loss  0.639 in Step 200\n",
      "Valid loss  0.635 in Step 250\n",
      "Valid loss  0.635 in Step 300\n",
      "Valid loss  0.639 in Step 350\n",
      "Valid loss  0.639 in Step 400\n",
      "※※※Valid loss  0.639※※※\n",
      "Epoch 8\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.800 in Step 100\n",
      "Training loss  0.799 in Step 200\n",
      "Training loss  0.805 in Step 300\n",
      "Training loss  0.799 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.803 in Step 600\n",
      "Training loss  0.801 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.801 in Step 900\n",
      "Training loss  0.802 in Step 1000\n",
      "Training loss  0.807 in Step 1100\n",
      "Training loss  0.805 in Step 1200\n",
      "Training loss  0.797 in Step 1300\n",
      "Training loss  0.801 in Step 1400\n",
      "Training loss  0.804 in Step 1500\n",
      "Training loss  0.807 in Step 1600\n",
      "Training loss  0.800 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.640 in Step 0\n",
      "Valid loss  0.642 in Step 50\n",
      "Valid loss  0.633 in Step 100\n",
      "Valid loss  0.634 in Step 150\n",
      "Valid loss  0.636 in Step 200\n",
      "Valid loss  0.633 in Step 250\n",
      "Valid loss  0.632 in Step 300\n",
      "Valid loss  0.636 in Step 350\n",
      "Valid loss  0.636 in Step 400\n",
      "※※※Valid loss  0.636※※※\n",
      "Epoch 9\n",
      "Training loss  0.801 in Step 0\n",
      "Training loss  0.801 in Step 100\n",
      "Training loss  0.802 in Step 200\n",
      "Training loss  0.802 in Step 300\n",
      "Training loss  0.800 in Step 400\n",
      "Training loss  0.804 in Step 500\n",
      "Training loss  0.804 in Step 600\n",
      "Training loss  0.800 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.801 in Step 900\n",
      "Training loss  0.801 in Step 1000\n",
      "Training loss  0.804 in Step 1100\n",
      "Training loss  0.802 in Step 1200\n",
      "Training loss  0.804 in Step 1300\n",
      "Training loss  0.801 in Step 1400\n",
      "Training loss  0.804 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.803 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.643 in Step 0\n",
      "Valid loss  0.645 in Step 50\n",
      "Valid loss  0.637 in Step 100\n",
      "Valid loss  0.637 in Step 150\n",
      "Valid loss  0.639 in Step 200\n",
      "Valid loss  0.636 in Step 250\n",
      "Valid loss  0.636 in Step 300\n",
      "Valid loss  0.639 in Step 350\n",
      "Valid loss  0.640 in Step 400\n",
      "※※※Valid loss  0.639※※※\n",
      "Epoch 10\n",
      "Training loss  0.803 in Step 0\n",
      "Training loss  0.802 in Step 100\n",
      "Training loss  0.803 in Step 200\n",
      "Training loss  0.802 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.803 in Step 600\n",
      "Training loss  0.801 in Step 700\n",
      "Training loss  0.801 in Step 800\n",
      "Training loss  0.806 in Step 900\n",
      "Training loss  0.802 in Step 1000\n",
      "Training loss  0.803 in Step 1100\n",
      "Training loss  0.800 in Step 1200\n",
      "Training loss  0.805 in Step 1300\n",
      "Training loss  0.805 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.800 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.642 in Step 0\n",
      "Valid loss  0.643 in Step 50\n",
      "Valid loss  0.635 in Step 100\n",
      "Valid loss  0.635 in Step 150\n",
      "Valid loss  0.638 in Step 200\n",
      "Valid loss  0.634 in Step 250\n",
      "Valid loss  0.633 in Step 300\n",
      "Valid loss  0.637 in Step 350\n",
      "Valid loss  0.638 in Step 400\n",
      "※※※Valid loss  0.637※※※\n",
      "Epoch 11\n",
      "Training loss  0.802 in Step 0\n",
      "Training loss  0.799 in Step 100\n",
      "Training loss  0.802 in Step 200\n",
      "Training loss  0.802 in Step 300\n",
      "Training loss  0.807 in Step 400\n",
      "Training loss  0.805 in Step 500\n",
      "Training loss  0.803 in Step 600\n",
      "Training loss  0.803 in Step 700\n",
      "Training loss  0.805 in Step 800\n",
      "Training loss  0.801 in Step 900\n",
      "Training loss  0.805 in Step 1000\n",
      "Training loss  0.799 in Step 1100\n",
      "Training loss  0.807 in Step 1200\n",
      "Training loss  0.800 in Step 1300\n",
      "Training loss  0.806 in Step 1400\n",
      "Training loss  0.800 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.643 in Step 0\n",
      "Valid loss  0.645 in Step 50\n",
      "Valid loss  0.637 in Step 100\n",
      "Valid loss  0.636 in Step 150\n",
      "Valid loss  0.639 in Step 200\n",
      "Valid loss  0.636 in Step 250\n",
      "Valid loss  0.635 in Step 300\n",
      "Valid loss  0.639 in Step 350\n",
      "Valid loss  0.639 in Step 400\n",
      "※※※Valid loss  0.639※※※\n",
      "Epoch 12\n",
      "Training loss  0.799 in Step 0\n",
      "Training loss  0.800 in Step 100\n",
      "Training loss  0.803 in Step 200\n",
      "Training loss  0.799 in Step 300\n",
      "Training loss  0.802 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.799 in Step 600\n",
      "Training loss  0.800 in Step 700\n",
      "Training loss  0.800 in Step 800\n",
      "Training loss  0.802 in Step 900\n",
      "Training loss  0.801 in Step 1000\n",
      "Training loss  0.803 in Step 1100\n",
      "Training loss  0.805 in Step 1200\n",
      "Training loss  0.800 in Step 1300\n",
      "Training loss  0.802 in Step 1400\n",
      "Training loss  0.804 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.804 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.644 in Step 0\n",
      "Valid loss  0.646 in Step 50\n",
      "Valid loss  0.638 in Step 100\n",
      "Valid loss  0.637 in Step 150\n",
      "Valid loss  0.640 in Step 200\n",
      "Valid loss  0.637 in Step 250\n",
      "Valid loss  0.636 in Step 300\n",
      "Valid loss  0.640 in Step 350\n",
      "Valid loss  0.641 in Step 400\n",
      "※※※Valid loss  0.640※※※\n",
      "Epoch 13\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.804 in Step 100\n",
      "Training loss  0.805 in Step 200\n",
      "Training loss  0.796 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.802 in Step 500\n",
      "Training loss  0.803 in Step 600\n",
      "Training loss  0.803 in Step 700\n",
      "Training loss  0.802 in Step 800\n",
      "Training loss  0.806 in Step 900\n",
      "Training loss  0.798 in Step 1000\n",
      "Training loss  0.803 in Step 1100\n",
      "Training loss  0.802 in Step 1200\n",
      "Training loss  0.798 in Step 1300\n",
      "Training loss  0.799 in Step 1400\n",
      "Training loss  0.801 in Step 1500\n",
      "Training loss  0.807 in Step 1600\n",
      "Training loss  0.804 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.646 in Step 0\n",
      "Valid loss  0.648 in Step 50\n",
      "Valid loss  0.640 in Step 100\n",
      "Valid loss  0.639 in Step 150\n",
      "Valid loss  0.642 in Step 200\n",
      "Valid loss  0.639 in Step 250\n",
      "Valid loss  0.639 in Step 300\n",
      "Valid loss  0.642 in Step 350\n",
      "Valid loss  0.643 in Step 400\n",
      "※※※Valid loss  0.642※※※\n",
      "Epoch 14\n",
      "Training loss  0.801 in Step 0\n",
      "Training loss  0.803 in Step 100\n",
      "Training loss  0.804 in Step 200\n",
      "Training loss  0.801 in Step 300\n",
      "Training loss  0.800 in Step 400\n",
      "Training loss  0.805 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.800 in Step 700\n",
      "Training loss  0.805 in Step 800\n",
      "Training loss  0.799 in Step 900\n",
      "Training loss  0.802 in Step 1000\n",
      "Training loss  0.801 in Step 1100\n",
      "Training loss  0.803 in Step 1200\n",
      "Training loss  0.801 in Step 1300\n",
      "Training loss  0.799 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.804 in Step 1600\n",
      "Training loss  0.802 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.646 in Step 0\n",
      "Valid loss  0.648 in Step 50\n",
      "Valid loss  0.640 in Step 100\n",
      "Valid loss  0.639 in Step 150\n",
      "Valid loss  0.642 in Step 200\n",
      "Valid loss  0.639 in Step 250\n",
      "Valid loss  0.638 in Step 300\n",
      "Valid loss  0.642 in Step 350\n",
      "Valid loss  0.643 in Step 400\n",
      "※※※Valid loss  0.642※※※\n",
      "Epoch 15\n",
      "Training loss  0.801 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.796 in Step 200\n",
      "Training loss  0.799 in Step 300\n",
      "Training loss  0.804 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.802 in Step 600\n",
      "Training loss  0.807 in Step 700\n",
      "Training loss  0.798 in Step 800\n",
      "Training loss  0.802 in Step 900\n",
      "Training loss  0.803 in Step 1000\n",
      "Training loss  0.799 in Step 1100\n",
      "Training loss  0.795 in Step 1200\n",
      "Training loss  0.804 in Step 1300\n",
      "Training loss  0.802 in Step 1400\n",
      "Training loss  0.801 in Step 1500\n",
      "Training loss  0.809 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.645 in Step 0\n",
      "Valid loss  0.647 in Step 50\n",
      "Valid loss  0.639 in Step 100\n",
      "Valid loss  0.638 in Step 150\n",
      "Valid loss  0.641 in Step 200\n",
      "Valid loss  0.638 in Step 250\n",
      "Valid loss  0.638 in Step 300\n",
      "Valid loss  0.641 in Step 350\n",
      "Valid loss  0.642 in Step 400\n",
      "※※※Valid loss  0.641※※※\n",
      "Epoch 16\n",
      "Training loss  0.799 in Step 0\n",
      "Training loss  0.798 in Step 100\n",
      "Training loss  0.800 in Step 200\n",
      "Training loss  0.801 in Step 300\n",
      "Training loss  0.801 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.796 in Step 700\n",
      "Training loss  0.799 in Step 800\n",
      "Training loss  0.800 in Step 900\n",
      "Training loss  0.805 in Step 1000\n",
      "Training loss  0.804 in Step 1100\n",
      "Training loss  0.803 in Step 1200\n",
      "Training loss  0.799 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.804 in Step 1500\n",
      "Training loss  0.799 in Step 1600\n",
      "Training loss  0.804 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.646 in Step 0\n",
      "Valid loss  0.648 in Step 50\n",
      "Valid loss  0.640 in Step 100\n",
      "Valid loss  0.639 in Step 150\n",
      "Valid loss  0.642 in Step 200\n",
      "Valid loss  0.638 in Step 250\n",
      "Valid loss  0.638 in Step 300\n",
      "Valid loss  0.641 in Step 350\n",
      "Valid loss  0.643 in Step 400\n",
      "※※※Valid loss  0.642※※※\n",
      "Epoch 17\n",
      "Training loss  0.798 in Step 0\n",
      "Training loss  0.802 in Step 100\n",
      "Training loss  0.801 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.804 in Step 400\n",
      "Training loss  0.801 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.804 in Step 700\n",
      "Training loss  0.802 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.799 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.800 in Step 1200\n",
      "Training loss  0.804 in Step 1300\n",
      "Training loss  0.804 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.799 in Step 1600\n",
      "Training loss  0.798 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.646 in Step 0\n",
      "Valid loss  0.648 in Step 50\n",
      "Valid loss  0.640 in Step 100\n",
      "Valid loss  0.639 in Step 150\n",
      "Valid loss  0.642 in Step 200\n",
      "Valid loss  0.639 in Step 250\n",
      "Valid loss  0.639 in Step 300\n",
      "Valid loss  0.642 in Step 350\n",
      "Valid loss  0.643 in Step 400\n",
      "※※※Valid loss  0.642※※※\n",
      "Epoch 18\n",
      "Training loss  0.799 in Step 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train()\n",
      "\u001b[1;32m/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb Cell 20\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_num \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)    \u001b[39m# train_loader\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, (x, x_lens) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ldlmdl/Documents/wavln/scripts/SL_D_E_6.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     y \u001b[39m=\u001b[39m x \n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[39m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(res)\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/multiprocessing/reductions.py:307\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrebuild_storage_fd\u001b[39m(\u001b[39mcls\u001b[39m, df, size):\n\u001b[0;32m--> 307\u001b[0m     fd \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mdetach()\n\u001b[1;32m    308\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         storage \u001b[39m=\u001b[39m storage_from_cache(\u001b[39mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/resource_sharer.py:58\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mwith\u001b[39;00m _resource_sharer\u001b[39m.\u001b[39mget_connection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id) \u001b[39mas\u001b[39;00m conn:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m reduction\u001b[39m.\u001b[39;49mrecv_handle(conn)\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/reduction.py:189\u001b[0m, in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Receive a handle over a local connection.'''\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39mwith\u001b[39;00m socket\u001b[39m.\u001b[39mfromfd(conn\u001b[39m.\u001b[39mfileno(), socket\u001b[39m.\u001b[39mAF_UNIX, socket\u001b[39m.\u001b[39mSOCK_STREAM) \u001b[39mas\u001b[39;00m s:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m recvfds(s, \u001b[39m1\u001b[39;49m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/reduction.py:157\u001b[0m, in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    155\u001b[0m a \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39marray(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m bytes_size \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mitemsize \u001b[39m*\u001b[39m size\n\u001b[0;32m--> 157\u001b[0m msg, ancdata, flags, addr \u001b[39m=\u001b[39m sock\u001b[39m.\u001b[39;49mrecvmsg(\u001b[39m1\u001b[39;49m, socket\u001b[39m.\u001b[39;49mCMSG_SPACE(bytes_size))\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m msg \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ancdata:\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "train_losses.save()\n",
    "\n",
    "valid_losses.save()\n",
    "\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f37b94bca50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This model should converge to loss around 0.49x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
