{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sibilant + stop Deaspiration Phenomenon Selection\n",
    "\n",
    "Here we want to work out how we can select only those instances (words) with only target seqs. But one problem is that we don't have teh exact recording files on that granularity level. We only have cut words and cut phones. But our target is something like two or three phones. This is a problem. \n",
    "\n",
    "However, considering that our target is not very long, I am thinking of finding all valid instances and integrate them into recordings. Then each time we train, read from the integrated recordings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE   # one type of clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import block_diag\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from model_padding import generate_mask_from_lengths_mat, mask_it\n",
    "from paths import *\n",
    "from misc_my_utils import *\n",
    "from model_loss import *\n",
    "from model_model import CTCPredNetV1 as TheLearner\n",
    "from model_dataset import WordDatasetPath as ThisDataset\n",
    "from model_dataset import Normalizer, DeNormalizer, TokenMap\n",
    "from model_dataset import MelSpecTransformDB as TheTransform\n",
    "from model_dataset import DS_Tools\n",
    "from reshandler import DictResHandler\n",
    "from misc_progress_bar import draw_progress_bar\n",
    "from test_bnd_detect_tools import *\n",
    "from misc_tools import PathUtils as PU\n",
    "from misc_tools import AudioCut, ARPABET\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_dir = train_cut_word_\n",
    "train_guide_path = os.path.join(src_, \"guide_train.csv\")\n",
    "valid_guide_path = os.path.join(src_, \"guide_validation.csv\")\n",
    "test_guide_path = os.path.join(src_, \"guide_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in guide file\n",
    "guide_file = pd.read_csv(valid_guide_path)\n",
    "# filtering out is not necessary, since we only include wuid for encoded words\n",
    "guide_file = guide_file[~guide_file[\"segment_nostress\"].isin([\"sil\", \"sp\", \"spn\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_guide = guide_file.groupby('wuid').apply(lambda x: ([row[\"segment\"] for index, row in x.iterrows()]).tolist()\n",
    "words_guide_str = guide_file.groupby('wuid').apply(lambda x: (\" \".join([row[\"segment\"] for index, row in x.iterrows()]), x[\"wuid\"].iloc[0])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_span_to_list_indices(phoneme_str, pattern):\n",
    "    # Split the string into a list of phonemes\n",
    "    phonemes = phoneme_str.split()\n",
    "    # Calculate the cumulative lengths including spaces (add 1 for each space)\n",
    "    cumulative_lengths = [0]  # Start with 0 for the first phoneme\n",
    "    for phoneme in phonemes:\n",
    "        # Add the length of the current phoneme and a space (except for the last one)\n",
    "        cumulative_lengths.append(cumulative_lengths[-1] + len(phoneme) + 1)\n",
    "    # Find all matches using re.finditer\n",
    "    matches = list(re.finditer(pattern, phoneme_str))\n",
    "    # Map regex span indices to phoneme list indices\n",
    "    match_indices = []\n",
    "    for match in matches:\n",
    "        start, end = match.span()\n",
    "        # Find the phoneme list index corresponding to the start of the match\n",
    "        list_start = next(i for i, length in enumerate(cumulative_lengths) if length > start) - 1\n",
    "        # Find the phoneme list index corresponding to the end of the match (subtract 1 because end is exclusive)\n",
    "        list_end = next(i for i, length in enumerate(cumulative_lengths) if length >= end) - 1\n",
    "        match_indices.append((list_start, list_end))\n",
    "    return match_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pattern = '^[PTK] (?!R)'\n",
    "sibstop_pattern = 'S [PTK] (?!R)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_indices = [name for i, (word, name) in enumerate(words_guide_str) if re.search(stop_pattern, word)]\n",
    "sibstop_indices = [name for i, (word, name) in enumerate(words_guide_str) if re.search(sibstop_pattern, word)]\n",
    "sibstop_subidx = [regex_span_to_list_indices(word, sibstop_pattern) for i, (word, name) in enumerate(words_guide_str) if re.search(sibstop_pattern, word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10729, 2606)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_indices), len(sibstop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(df, name_list, target_idx_list=None): \n",
    "    sibilant_list = []\n",
    "    sibilant_path = []\n",
    "    sibilant_startTime = []\n",
    "    sibilant_endTime = []\n",
    "    stop_list = []\n",
    "    stop_path = []\n",
    "    stop_startTime = []\n",
    "    stop_endTime = []\n",
    "    speaker_list = []\n",
    "    wuid_list = []\n",
    "    if target_idx_list is None:\n",
    "        for name in name_list: \n",
    "            # this is one word, there might be multiple matching cases\n",
    "            word_phonemes = df[df[\"wuid\"] == name]\n",
    "            stop = word_phonemes[word_phonemes[\"in_id\"] == 0]\n",
    "            # sibilant_list.append(sib[\"segment_nostress\"])\n",
    "            # sibilant_path.append(sib[\"phone_path\"])\n",
    "            # sibilant_startTime.append(sib[\"startTime\"])\n",
    "            # sibilant_endTime.append(sib[\"endTime\"])\n",
    "            stop_list.append(stop[\"segment_nostress\"])\n",
    "            stop_path.append(stop[\"phone_path\"])\n",
    "            stop_startTime.append(stop[\"startTime\"])\n",
    "            stop_endTime.append(stop[\"endTime\"])\n",
    "            speaker_list.append(stop[\"speaker\"])\n",
    "            wuid_list.append(name)\n",
    "    else:\n",
    "        for name, target_idx in zip(name_list, target_idx_list): \n",
    "            # this is one word, there might be multiple matching cases\n",
    "            word_phonemes = df[df[\"wuid\"] == name]\n",
    "            for target in target_idx: \n",
    "                target_phonemes = word_phonemes[word_phonemes[\"in_id\"].isin(target)]\n",
    "                sib = target_phonemes.iloc[0]\n",
    "                stop = target_phonemes.iloc[1]\n",
    "                sibilant_list.append(sib[\"segment_nostress\"])\n",
    "                sibilant_path.append(sib[\"phone_path\"])\n",
    "                sibilant_startTime.append(sib[\"startTime\"])\n",
    "                sibilant_endTime.append(sib[\"endTime\"])\n",
    "\n",
    "                stop_list.append(stop[\"segment_nostress\"])\n",
    "                stop_path.append(stop[\"phone_path\"])\n",
    "                stop_startTime.append(stop[\"startTime\"])\n",
    "                stop_endTime.append(stop[\"endTime\"])\n",
    "\n",
    "                speaker_list.append(stop[\"speaker\"])\n",
    "                wuid_list.append(name)\n",
    "    out_dict = {\n",
    "        \"sibilant\": sibilant_list, \n",
    "        \"stop\": stop_list, \n",
    "        \"sibilant_path\": sibilant_path, \n",
    "        \"stop_path\": stop_path, \n",
    "        \"sibilant_startTime\": sibilant_startTime, \n",
    "        \"sibilant_endTime\": sibilant_endTime, \n",
    "        \"stop_startTime\": stop_startTime,\n",
    "        \"stop_endTime\": stop_endTime,\n",
    "        \"speaker\": speaker_list,\n",
    "        \"wuid\": wuid_list\n",
    "    }\n",
    "    return pd.DataFrame(out_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
