{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sibilant + stop Deaspiration Phenomenon Selection\n",
    "\n",
    "Here we want to work out how we can select only those instances (words) with only target seqs. But one problem is that we don't have teh exact recording files on that granularity level. We only have cut words and cut phones. But our target is something like two or three phones. This is a problem. \n",
    "\n",
    "However, considering that our target is not very long, I am thinking of finding all valid instances and integrate them into recordings. Then each time we train, read from the integrated recordings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE   # one type of clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import block_diag\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from model_padding import generate_mask_from_lengths_mat, mask_it\n",
    "from paths import *\n",
    "from misc_my_utils import *\n",
    "from model_loss import *\n",
    "from model_model import CTCPredNetV1 as TheLearner\n",
    "from model_dataset import WordDatasetPath as ThisDataset\n",
    "from model_dataset import Normalizer, DeNormalizer, TokenMap\n",
    "from model_dataset import MelSpecTransformDB as TheTransform\n",
    "from model_dataset import DS_Tools\n",
    "from reshandler import DictResHandler\n",
    "from misc_progress_bar import draw_progress_bar\n",
    "from test_bnd_detect_tools import *\n",
    "from misc_tools import PathUtils as PU\n",
    "from misc_tools import AudioCut, ARPABET\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_dir = train_cut_word_\n",
    "train_guide_path = os.path.join(src_, \"guide_train.csv\")\n",
    "valid_guide_path = os.path.join(src_, \"guide_validation.csv\")\n",
    "test_guide_path = os.path.join(src_, \"guide_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in guide file\n",
    "guide_file = pd.read_csv(valid_guide_path)\n",
    "# filtering out is not necessary, since we only include wuid for encoded words\n",
    "guide_file = guide_file[~guide_file[\"segment_nostress\"].isin([\"sil\", \"sp\", \"spn\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_guide = guide_file.groupby('wuid').apply(lambda x: ([row[\"segment\"] for index, row in x.iterrows()]).tolist()\n",
    "words_guide_str = guide_file.groupby('wuid').apply(lambda x: (\" \".join([row[\"segment\"] for index, row in x.iterrows()]), x[\"wuid\"].iloc[0])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_span_to_list_indices(phoneme_str, pattern):\n",
    "    # Split the string into a list of phonemes\n",
    "    phonemes = phoneme_str.split()\n",
    "    # Calculate the cumulative lengths including spaces (add 1 for each space)\n",
    "    cumulative_lengths = [0]  # Start with 0 for the first phoneme\n",
    "    for phoneme in phonemes:\n",
    "        # Add the length of the current phoneme and a space (except for the last one)\n",
    "        cumulative_lengths.append(cumulative_lengths[-1] + len(phoneme) + 1)\n",
    "    # Find all matches using re.finditer\n",
    "    matches = list(re.finditer(pattern, phoneme_str))\n",
    "    # Map regex span indices to phoneme list indices\n",
    "    match_indices = []\n",
    "    for match in matches:\n",
    "        start, end = match.span()\n",
    "        # Find the phoneme list index corresponding to the start of the match\n",
    "        list_start = next(i for i, length in enumerate(cumulative_lengths) if length > start) - 1\n",
    "        # Find the phoneme list index corresponding to the end of the match (subtract 1 because end is exclusive)\n",
    "        list_end = next(i for i, length in enumerate(cumulative_lengths) if length >= end) - 1\n",
    "        match_indices.append((list_start, list_end))\n",
    "    return match_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_pattern = '(?!S) [PTK] (?!R)'\n",
    "# sibstop_pattern = 'S [PTK] (?!R)'\n",
    "# note that although we only list single-letter vowels, \n",
    "# we in fact include all vowels because the all vowels start with one of the listed letters\n",
    "# the subidx always include pre-stop-vowel. But for Xstop, we don't need the pre\n",
    "stop_pattern = '(?!S) [PTK] [AOEIUY]'\n",
    "sibstop_pattern = 'S [PTK] [AOEIUY]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstop_indices = [name for i, (word, name) in enumerate(words_guide_str) if re.search(stop_pattern, word)]\n",
    "sibstop_indices = [name for i, (word, name) in enumerate(words_guide_str) if re.search(sibstop_pattern, word)]\n",
    "sibstop_subidx = [regex_span_to_list_indices(word, sibstop_pattern) for i, (word, name) in enumerate(words_guide_str) if re.search(sibstop_pattern, word)]\n",
    "Xstop_subidx = [regex_span_to_list_indices(word, stop_pattern) for i, (word, name) in enumerate(words_guide_str) if re.search(stop_pattern, word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the risk of including /t/ for ST but excluding it in XT. \n",
    "\n",
    "New selection: this time, we select only those preceding vowels. THerefore, during evaluation, we need to account for the vowels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9127, 2361)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xstop_indices), len(sibstop_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new version, we also include the following vowels as part of the training set. THis will introduce more noise, but if the trainign is also successful, we can check the attention performances towards both sides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(df, name_list, target_idx_list=None): \n",
    "    pre_list = []   # pre can be sibilant or others\n",
    "    pre_path = []\n",
    "    pre_startTime = []\n",
    "    pre_endTimte = []\n",
    "    stop_list = []\n",
    "    stop_path = []\n",
    "    stop_startTime = []\n",
    "    stop_endTime = []\n",
    "    vowel_list = []\n",
    "    vowel_path = []\n",
    "    vowel_startTime = []\n",
    "    vowel_endTime = []\n",
    "    speaker_list = []\n",
    "    wuid_list = []\n",
    "    if target_idx_list is None:\n",
    "        raise ValueError(\"target_idx_list cannot be None\")\n",
    "    else:\n",
    "        for name, target_idx in zip(name_list, target_idx_list): \n",
    "            # this is one word, there might be multiple matching cases\n",
    "            word_phonemes = df[df[\"wuid\"] == name]\n",
    "            for target in target_idx: \n",
    "                target = [i + 1 for i in target]    # Add 1 here because in_id starts from 1\n",
    "                target_phonemes = word_phonemes[word_phonemes[\"in_id\"].isin(target)]\n",
    "                pre = target_phonemes.iloc[0]\n",
    "                stop = target_phonemes.iloc[1]\n",
    "                vowel = target_phonemes.iloc[2]\n",
    "                pre_list.append(pre[\"segment_nostress\"])\n",
    "                pre_path.append(pre[\"phone_path\"])\n",
    "                pre_startTime.append(pre[\"startTime\"])\n",
    "                pre_endTimte.append(pre[\"endTime\"])\n",
    "\n",
    "                stop_list.append(stop[\"segment_nostress\"])\n",
    "                stop_path.append(stop[\"phone_path\"])\n",
    "                stop_startTime.append(stop[\"startTime\"])\n",
    "                stop_endTime.append(stop[\"endTime\"])\n",
    "\n",
    "                vowel_list.append(vowel[\"segment_nostress\"])\n",
    "                vowel_path.append(vowel[\"phone_path\"])\n",
    "                vowel_startTime.append(vowel[\"startTime\"])\n",
    "                vowel_endTime.append(vowel[\"endTime\"])\n",
    "\n",
    "                speaker_list.append(stop[\"speaker\"])\n",
    "                wuid_list.append(name)\n",
    "        out_dict = {\n",
    "            \"pre\": pre_list, \n",
    "            \"stop\": stop_list,\n",
    "            \"vowel\": vowel_list, \n",
    "            \"pre_path\": pre_path, \n",
    "            \"stop_path\": stop_path, \n",
    "            \"vowel_path\": vowel_path,\n",
    "            \"pre_startTime\": pre_startTime, \n",
    "            \"pre_endTime\": pre_endTimte, \n",
    "            \"stop_startTime\": stop_startTime,\n",
    "            \"stop_endTime\": stop_endTime,\n",
    "            \"vowel_startTime\": vowel_startTime,\n",
    "            \"vowel_endTime\": vowel_endTime,\n",
    "            \"speaker\": speaker_list,\n",
    "            \"wuid\": wuid_list\n",
    "        }\n",
    "        outdf = pd.DataFrame(out_dict)\n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "sibstopsdf = generate_table(guide_file, sibstop_indices, sibstop_subidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsdf = generate_table(guide_file, Xstop_indices, Xstop_subidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([sibstopsdf, stopsdf], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre</th>\n",
       "      <th>stop</th>\n",
       "      <th>pre_path</th>\n",
       "      <th>stop_path</th>\n",
       "      <th>pre_startTime</th>\n",
       "      <th>pre_endTime</th>\n",
       "      <th>stop_startTime</th>\n",
       "      <th>stop_endTime</th>\n",
       "      <th>speaker</th>\n",
       "      <th>wuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>P</td>\n",
       "      <td>103/1240/0000/103-1240-0000-0002.flac</td>\n",
       "      <td>103/1240/0000/103-1240-0000-0003.flac</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>103</td>\n",
       "      <td>103-1240-0000-0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IH</td>\n",
       "      <td>P</td>\n",
       "      <td>103/1240/0000/103-1240-0000-0077.flac</td>\n",
       "      <td>103/1240/0000/103-1240-0000-0078.flac</td>\n",
       "      <td>8.82</td>\n",
       "      <td>8.88</td>\n",
       "      <td>8.88</td>\n",
       "      <td>9.00</td>\n",
       "      <td>103</td>\n",
       "      <td>103-1240-0000-0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA</td>\n",
       "      <td>P</td>\n",
       "      <td>103/1240/0000/103-1240-0000-0125.flac</td>\n",
       "      <td>103/1240/0000/103-1240-0000-0126.flac</td>\n",
       "      <td>12.54</td>\n",
       "      <td>12.62</td>\n",
       "      <td>12.62</td>\n",
       "      <td>12.71</td>\n",
       "      <td>103</td>\n",
       "      <td>103-1240-0000-0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IH</td>\n",
       "      <td>P</td>\n",
       "      <td>103/1240/0001/103-1240-0001-0052.flac</td>\n",
       "      <td>103/1240/0001/103-1240-0001-0053.flac</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.59</td>\n",
       "      <td>4.59</td>\n",
       "      <td>4.65</td>\n",
       "      <td>103</td>\n",
       "      <td>103-1240-0001-0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AH</td>\n",
       "      <td>K</td>\n",
       "      <td>103/1240/0001/103-1240-0001-0069.flac</td>\n",
       "      <td>103/1240/0001/103-1240-0001-0070.flac</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.58</td>\n",
       "      <td>5.58</td>\n",
       "      <td>5.63</td>\n",
       "      <td>103</td>\n",
       "      <td>103-1240-0001-0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>ER</td>\n",
       "      <td>P</td>\n",
       "      <td>909/131045/0042/909-131045-0042-0162.flac</td>\n",
       "      <td>909/131045/0042/909-131045-0042-0163.flac</td>\n",
       "      <td>12.97</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.13</td>\n",
       "      <td>909</td>\n",
       "      <td>909-131045-0042-0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6791</th>\n",
       "      <td>EH</td>\n",
       "      <td>K</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0036.flac</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0037.flac</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.04</td>\n",
       "      <td>909</td>\n",
       "      <td>909-131045-0043-0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6792</th>\n",
       "      <td>EH</td>\n",
       "      <td>K</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0074.flac</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0075.flac</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.38</td>\n",
       "      <td>7.38</td>\n",
       "      <td>7.48</td>\n",
       "      <td>909</td>\n",
       "      <td>909-131045-0043-0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6793</th>\n",
       "      <td>IY</td>\n",
       "      <td>P</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0112.flac</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0113.flac</td>\n",
       "      <td>10.60</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.77</td>\n",
       "      <td>909</td>\n",
       "      <td>909-131045-0043-0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6794</th>\n",
       "      <td>AH</td>\n",
       "      <td>P</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0162.flac</td>\n",
       "      <td>909/131045/0043/909-131045-0043-0163.flac</td>\n",
       "      <td>14.53</td>\n",
       "      <td>14.58</td>\n",
       "      <td>14.58</td>\n",
       "      <td>14.70</td>\n",
       "      <td>909</td>\n",
       "      <td>909-131045-0043-0035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6795 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pre stop                                   pre_path  \\\n",
       "0     AE    P      103/1240/0000/103-1240-0000-0002.flac   \n",
       "1     IH    P      103/1240/0000/103-1240-0000-0077.flac   \n",
       "2     AA    P      103/1240/0000/103-1240-0000-0125.flac   \n",
       "3     IH    P      103/1240/0001/103-1240-0001-0052.flac   \n",
       "4     AH    K      103/1240/0001/103-1240-0001-0069.flac   \n",
       "...   ..  ...                                        ...   \n",
       "6790  ER    P  909/131045/0042/909-131045-0042-0162.flac   \n",
       "6791  EH    K  909/131045/0043/909-131045-0043-0036.flac   \n",
       "6792  EH    K  909/131045/0043/909-131045-0043-0074.flac   \n",
       "6793  IY    P  909/131045/0043/909-131045-0043-0112.flac   \n",
       "6794  AH    P  909/131045/0043/909-131045-0043-0162.flac   \n",
       "\n",
       "                                      stop_path  pre_startTime  pre_endTime  \\\n",
       "0         103/1240/0000/103-1240-0000-0003.flac           0.57         0.63   \n",
       "1         103/1240/0000/103-1240-0000-0078.flac           8.82         8.88   \n",
       "2         103/1240/0000/103-1240-0000-0126.flac          12.54        12.62   \n",
       "3         103/1240/0001/103-1240-0001-0053.flac           4.54         4.59   \n",
       "4         103/1240/0001/103-1240-0001-0070.flac           5.53         5.58   \n",
       "...                                         ...            ...          ...   \n",
       "6790  909/131045/0042/909-131045-0042-0163.flac          12.97        13.01   \n",
       "6791  909/131045/0043/909-131045-0043-0037.flac           2.87         2.97   \n",
       "6792  909/131045/0043/909-131045-0043-0075.flac           7.25         7.38   \n",
       "6793  909/131045/0043/909-131045-0043-0113.flac          10.60        10.70   \n",
       "6794  909/131045/0043/909-131045-0043-0163.flac          14.53        14.58   \n",
       "\n",
       "      stop_startTime  stop_endTime  speaker                  wuid  \n",
       "0               0.63          0.70      103    103-1240-0000-0000  \n",
       "1               8.88          9.00      103    103-1240-0000-0017  \n",
       "2              12.62         12.71      103    103-1240-0000-0028  \n",
       "3               4.59          4.65      103    103-1240-0001-0016  \n",
       "4               5.58          5.63      103    103-1240-0001-0020  \n",
       "...              ...           ...      ...                   ...  \n",
       "6790           13.01         13.13      909  909-131045-0042-0037  \n",
       "6791            2.97          3.04      909  909-131045-0043-0007  \n",
       "6792            7.38          7.48      909  909-131045-0043-0014  \n",
       "6793           10.70         10.77      909  909-131045-0043-0023  \n",
       "6794           14.58         14.70      909  909-131045-0043-0035  \n",
       "\n",
       "[6795 rows x 10 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"phi_type\"] = np.where(result_df[\"sibilant\"].isna(), \"T\", \"ST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = result_df[result_df[\"phi_type\"] == \"T\"]\n",
    "stg = result_df[result_df[\"phi_type\"] == \"ST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.to_csv(os.path.join(src_, \"phi-T-guide.csv\"), index=False)\n",
    "stg.to_csv(os.path.join(src_, \"phi-ST-guide.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
