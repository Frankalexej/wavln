{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sibilant + stop Deaspiration Phenomenon Selection\n",
    "\n",
    "Here we want to work out how we can select only those instances (words) with only target seqs. But one problem is that we don't have teh exact recording files on that granularity level. We only have cut words and cut phones. But our target is something like two or three phones. This is a problem. \n",
    "\n",
    "However, considering that our target is not very long, I am thinking of finding all valid instances and integrate them into recordings. Then each time we train, read from the integrated recordings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE   # one type of clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import block_diag\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from model_padding import generate_mask_from_lengths_mat, mask_it\n",
    "from paths import *\n",
    "from misc_my_utils import *\n",
    "from model_loss import *\n",
    "from model_model import CTCPredNetV1 as TheLearner\n",
    "from model_dataset import WordDatasetPath as ThisDataset\n",
    "from model_dataset import Normalizer, DeNormalizer, TokenMap\n",
    "from model_dataset import MelSpecTransformDB as TheTransform\n",
    "from model_dataset import DS_Tools\n",
    "from reshandler import DictResHandler\n",
    "from misc_progress_bar import draw_progress_bar\n",
    "from test_bnd_detect_tools import *\n",
    "from misc_tools import PathUtils as PU\n",
    "from misc_tools import AudioCut, ARPABET\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_dir = train_cut_word_\n",
    "train_guide_path = os.path.join(src_, \"guide_train.csv\")\n",
    "valid_guide_path = os.path.join(src_, \"guide_validation.csv\")\n",
    "test_guide_path = os.path.join(src_, \"guide_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in guide file\n",
    "guide_file = pd.read_csv(valid_guide_path)\n",
    "# filtering out is not necessary, since we only include wuid for encoded words\n",
    "guide_file = guide_file[~guide_file[\"segment_nostress\"].isin([\"sil\", \"sp\", \"spn\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_guide = guide_file.groupby('wuid').apply(lambda x: ([row[\"segment\"] for index, row in x.iterrows()]).tolist()\n",
    "words_guide_str = guide_file.groupby('wuid').apply(lambda x: (\" \".join([row[\"segment\"] for index, row in x.iterrows()]), x[\"wuid\"].iloc[0])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_span_to_list_indices(phoneme_str, pattern):\n",
    "    # Split the string into a list of phonemes\n",
    "    phonemes = phoneme_str.split()\n",
    "    # Calculate the cumulative lengths including spaces (add 1 for each space)\n",
    "    cumulative_lengths = [0]  # Start with 0 for the first phoneme\n",
    "    for phoneme in phonemes:\n",
    "        # Add the length of the current phoneme and a space (except for the last one)\n",
    "        cumulative_lengths.append(cumulative_lengths[-1] + len(phoneme) + 1)\n",
    "    # Find all matches using re.finditer\n",
    "    matches = list(re.finditer(pattern, phoneme_str))\n",
    "    # Map regex span indices to phoneme list indices\n",
    "    match_indices = []\n",
    "    for match in matches:\n",
    "        start, end = match.span()\n",
    "        # Find the phoneme list index corresponding to the start of the match\n",
    "        list_start = next(i for i, length in enumerate(cumulative_lengths) if length > start) - 1\n",
    "        # Find the phoneme list index corresponding to the end of the match (subtract 1 because end is exclusive)\n",
    "        list_end = next(i for i, length in enumerate(cumulative_lengths) if length >= end) - 1\n",
    "        match_indices.append((list_start, list_end))\n",
    "    return match_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_pattern = '(?!S) [PTK] (?!R)'\n",
    "# sibstop_pattern = 'S [PTK] (?!R)'\n",
    "# Xstop_pattern = stop_pattern\n",
    "# note that although we only list single-letter vowels, \n",
    "# we in fact include all vowels because the all vowels start with one of the listed letters\n",
    "# the subidx always include pre-stop-vowel. But for Xstop, we don't need the pre\n",
    "Xstop_pattern = '[^S] [PTK] [AOEIUY]'\n",
    "stop_pattern = '^[PTK] [AOEIUY]'\n",
    "sibstop_pattern = 'S [PTK] [AOEIUY]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we have wai da zheng zhao-ed, although the way of selecting was quite wrong if we add any more phoneme, the selected results seem quite right. The only problem is that it seems that the ST sequences have also been included in the XT set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between stop and Xstop. Stop is at word beginning (but sadly they were not used during previous runnings), and Xstop is word middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_indices = [name for i, (word, name) in enumerate(words_guide_str) if re.search(stop_pattern, word)]\n",
    "Xstop_indices = [name for i, (word, name) in enumerate(words_guide_str) if re.search(Xstop_pattern, word)]\n",
    "sibstop_indices = [name for i, (word, name) in enumerate(words_guide_str) if re.search(sibstop_pattern, word)]\n",
    "\n",
    "stop_subidx = [regex_span_to_list_indices(word, stop_pattern) for i, (word, name) in enumerate(words_guide_str) if re.search(stop_pattern, word)]\n",
    "Xstop_subidx = [regex_span_to_list_indices(word, Xstop_pattern) for i, (word, name) in enumerate(words_guide_str) if re.search(Xstop_pattern, word)]\n",
    "sibstop_subidx = [regex_span_to_list_indices(word, sibstop_pattern) for i, (word, name) in enumerate(words_guide_str) if re.search(sibstop_pattern, word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the risk of including /t/ for ST but excluding it in XT. \n",
    "\n",
    "New selection: this time, we select only those preceding vowels. THerefore, during evaluation, we need to account for the vowels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9859, 7050, 2361)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_indices), len(Xstop_indices), len(sibstop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 1)], [(2, 4)], [(2, 4)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_subidx[0], Xstop_subidx[0], sibstop_subidx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new version, we also include the following vowels as part of the training set. THis will introduce more noise, but if the trainign is also successful, we can check the attention performances towards both sides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(df, name_list, target_idx_list, has_pre=True): \n",
    "    pre_list = []   # pre can be sibilant or others\n",
    "    pre_path = []\n",
    "    pre_startTime = []\n",
    "    pre_endTimte = []\n",
    "    stop_list = []\n",
    "    stop_path = []\n",
    "    stop_startTime = []\n",
    "    stop_endTime = []\n",
    "    vowel_list = []\n",
    "    vowel_path = []\n",
    "    vowel_startTime = []\n",
    "    vowel_endTime = []\n",
    "    speaker_list = []\n",
    "    wuid_list = []\n",
    "    if has_pre:\n",
    "        for name, target_idx in zip(name_list, target_idx_list): \n",
    "            # this is one word, there might be multiple matching cases\n",
    "            word_phonemes = df[df[\"wuid\"] == name]\n",
    "            for target in target_idx: \n",
    "                target = [i + 1 for i in target]    # Add 1 here because in_id starts from 1\n",
    "                target_phonemes = word_phonemes[word_phonemes[\"in_id\"].isin(range(target[0], target[1] + 1))]   # the span includes both start and end\n",
    "                pre = target_phonemes.iloc[0]\n",
    "                stop = target_phonemes.iloc[1]\n",
    "                vowel = target_phonemes.iloc[2]\n",
    "                pre_list.append(pre[\"segment_nostress\"])\n",
    "                pre_path.append(pre[\"phone_path\"])\n",
    "                pre_startTime.append(pre[\"startTime\"])\n",
    "                pre_endTimte.append(pre[\"endTime\"])\n",
    "\n",
    "                stop_list.append(stop[\"segment_nostress\"])\n",
    "                stop_path.append(stop[\"phone_path\"])\n",
    "                stop_startTime.append(stop[\"startTime\"])\n",
    "                stop_endTime.append(stop[\"endTime\"])\n",
    "\n",
    "                vowel_list.append(vowel[\"segment_nostress\"])\n",
    "                vowel_path.append(vowel[\"phone_path\"])\n",
    "                vowel_startTime.append(vowel[\"startTime\"])\n",
    "                vowel_endTime.append(vowel[\"endTime\"])\n",
    "\n",
    "                speaker_list.append(stop[\"speaker\"])\n",
    "                wuid_list.append(name)\n",
    "        out_dict = {\n",
    "            \"pre\": pre_list, \n",
    "            \"stop\": stop_list,\n",
    "            \"vowel\": vowel_list, \n",
    "            \"pre_path\": pre_path, \n",
    "            \"stop_path\": stop_path, \n",
    "            \"vowel_path\": vowel_path,\n",
    "            \"pre_startTime\": pre_startTime, \n",
    "            \"pre_endTime\": pre_endTimte, \n",
    "            \"stop_startTime\": stop_startTime,\n",
    "            \"stop_endTime\": stop_endTime,\n",
    "            \"vowel_startTime\": vowel_startTime,\n",
    "            \"vowel_endTime\": vowel_endTime,\n",
    "            \"speaker\": speaker_list,\n",
    "            \"wuid\": wuid_list\n",
    "        }\n",
    "        outdf = pd.DataFrame(out_dict)\n",
    "    else:\n",
    "        for name, target_idx in zip(name_list, target_idx_list): \n",
    "            # this is one word, there might be multiple matching cases\n",
    "            word_phonemes = df[df[\"wuid\"] == name]\n",
    "            for target in target_idx: \n",
    "                target = [i + 1 for i in target]    # Add 1 here because in_id starts from 1\n",
    "                target_phonemes = word_phonemes[word_phonemes[\"in_id\"].isin(range(target[0], target[1] + 1))]\n",
    "                stop = target_phonemes.iloc[0]\n",
    "                vowel = target_phonemes.iloc[1]\n",
    "                pre_list.append(\"\")\n",
    "                pre_path.append(\"\")\n",
    "                pre_startTime.append(\"\")\n",
    "                pre_endTimte.append(\"\")\n",
    "\n",
    "                stop_list.append(stop[\"segment_nostress\"])\n",
    "                stop_path.append(stop[\"phone_path\"])\n",
    "                stop_startTime.append(stop[\"startTime\"])\n",
    "                stop_endTime.append(stop[\"endTime\"])\n",
    "\n",
    "                vowel_list.append(vowel[\"segment_nostress\"])\n",
    "                vowel_path.append(vowel[\"phone_path\"])\n",
    "                vowel_startTime.append(vowel[\"startTime\"])\n",
    "                vowel_endTime.append(vowel[\"endTime\"])\n",
    "\n",
    "                speaker_list.append(stop[\"speaker\"])\n",
    "                wuid_list.append(name)\n",
    "        out_dict = {\n",
    "            \"pre\": pre_list, \n",
    "            \"stop\": stop_list,\n",
    "            \"vowel\": vowel_list, \n",
    "            \"pre_path\": pre_path, \n",
    "            \"stop_path\": stop_path, \n",
    "            \"vowel_path\": vowel_path,\n",
    "            \"pre_startTime\": pre_startTime, \n",
    "            \"pre_endTime\": pre_endTimte, \n",
    "            \"stop_startTime\": stop_startTime,\n",
    "            \"stop_endTime\": stop_endTime,\n",
    "            \"vowel_startTime\": vowel_startTime,\n",
    "            \"vowel_endTime\": vowel_endTime,\n",
    "            \"speaker\": speaker_list,\n",
    "            \"wuid\": wuid_list\n",
    "        }\n",
    "        outdf = pd.DataFrame(out_dict)\n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sibstopsdf = generate_table(guide_file, sibstop_indices, sibstop_subidx, has_pre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstopsdf = generate_table(guide_file, Xstop_indices, Xstop_subidx, has_pre=True)    # this may not be used. Because variation is too large in terms of aspiration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsdf = generate_table(guide_file, stop_indices, stop_subidx, has_pre=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sibstopsdf[\"phi_type\"] = \"ST\"\n",
    "Xstopsdf[\"phi_type\"] = \"XT\"\n",
    "stopsdf[\"phi_type\"] = \"T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsdf.to_csv(os.path.join(src_, \"phi-T-guide.csv\"), index=False)\n",
    "sibstopsdf.to_csv(os.path.join(src_, \"phi-ST-guide.csv\"), index=False)\n",
    "Xstopsdf.to_csv(os.path.join(src_, \"phi-XT-guide.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
