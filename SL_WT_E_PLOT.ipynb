{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Word Training - English - Testing Session -Plots\n",
    "In this session, we will look into the working status of our HMRNN-based AE and plot:   \n",
    "- the progression plot (how hid_rs are progressing along the timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it, masked_loss\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import PhonLearn_Net\n",
    "from model import PhonLearn_Net\n",
    "# DirectPassModel, TwoRNNModel, TwoRNNAttn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnoWordWholeDatasetPlot(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset that loads cutted wave files from disk and returns input-output pairs for\n",
    "    training autoencoder. \n",
    "    \n",
    "    Version 3: wav -> mel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load_dir, load_control_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the class by reading a CSV file and merging the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        The function reads the CSV file from the provided control path, extracts the \"rec\" and \"idx\" columns,\n",
    "        and concatenates the values from these columns using an underscore. It then appends the \".wav\" extension\n",
    "        to each of the merged strings and converts the merged pandas Series to a list, which is assigned to\n",
    "        the 'dataset' attribute of the class.\n",
    "\n",
    "        Args:\n",
    "        load_dir (str): The directory containing the files to load.\n",
    "        load_control_path (str): The path to the CSV file containing the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        Attributes:\n",
    "        dataset (list): A list of merged strings from the \"rec\" and \"idx\" columns, with the \".wav\" extension.\n",
    "        \"\"\"\n",
    "        control_file = pd.read_csv(load_control_path)\n",
    "        control_file = control_file[control_file['n_frames'] > 400]\n",
    "        control_file = control_file[control_file['duration'] <= 2.0]\n",
    "        \n",
    "        # Extract the \"rec\" and \"idx\" columns\n",
    "        rec_col = control_file['rec'].astype(str)\n",
    "        idx_col = control_file['idx'].astype(str).str.zfill(8)\n",
    "\n",
    "        # Extract the \"token\" and \"produced_segments\" columns\n",
    "        token_col = control_file['token'].astype(str)\n",
    "        produced_segments_col = control_file['produced_segments'].astype(str)\n",
    "        \n",
    "        # Merge the two columns by concatenating the strings with '_' and append extension name\n",
    "        merged_col = rec_col + '_' + idx_col + \".wav\"\n",
    "        \n",
    "        self.dataset = merged_col.tolist()\n",
    "        self.infoset = produced_segments_col.tolist()\n",
    "        self.load_dir = load_dir\n",
    "        self.transform = transform\n",
    "        self.info_rec_set = rec_col.tolist()\n",
    "        self.info_idx_set = idx_col.tolist()\n",
    "        self.info_token_set = token_col.tolist()\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of input-output pairs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (input_data, output_data) for the given index.\n",
    "\n",
    "        The function first checks if the provided index is a tensor, and if so, converts it to a list.\n",
    "        It then constructs the file path for the .wav file using the dataset attribute and the provided index.\n",
    "        The .wav file is loaded using torchaudio, and its data is normalized. If a transform is provided,\n",
    "        the data is transformed using the specified transform. Finally, the input_data and output_data are\n",
    "        set to the same data (creating a tuple), and the tuple is returned.\n",
    "\n",
    "        Args:\n",
    "        idx (int or torch.Tensor): The index of the desired data.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing input_data and output_data, both of which are the audio data\n",
    "               from the .wav file at the specified index.\n",
    "\n",
    "        Note: \n",
    "        This function assumes that the class has the following attributes:\n",
    "        - self.load_dir (str): The directory containing the .wav files.\n",
    "        - self.dataset (list): A list of .wav file names.\n",
    "        - self.transform (callable, optional): An optional transform to apply to the audio data.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        wav_name = os.path.join(self.load_dir,\n",
    "                                self.dataset[idx])\n",
    "        \n",
    "        data, sample_rate = torchaudio.load(wav_name, normalize=True)\n",
    "        if self.transform:\n",
    "            data = self.transform(data, sr=sample_rate)\n",
    "        \n",
    "        info = self.infoset[idx]\n",
    "        # extra info for completing a csv\n",
    "        info_rec = self.info_rec_set[idx]\n",
    "        info_idx = self.info_idx_set[idx]\n",
    "        info_token = self.info_token_set[idx]\n",
    "        \n",
    "        \n",
    "        # # Prepare for possible in-out discrepencies in the future\n",
    "        # input_data = data\n",
    "        # output_data = data\n",
    "        \n",
    "        return data, info, info_rec, info_idx, info_token\n",
    "\n",
    "def collate_fn(data):\n",
    "    xx, yy, aa, bb, cc = zip(*data)\n",
    "    # only working for one data at the moment\n",
    "    batch_first = True\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=batch_first, padding_value=0)\n",
    "    return xx_pad, x_lens, yy, aa, bb, cc\n",
    "\n",
    "\n",
    "class MyTransform(nn.Module): \n",
    "    def __init__(self, sample_rate, n_fft): \n",
    "        super().__init__()\n",
    "        # self.transform = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft, n_mels=64)\n",
    "        # self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        # self.transform = torchaudio.transforms.MFCC(n_mfcc=13)\n",
    "    \n",
    "    def forward(self, waveform, sr=16000): \n",
    "        # extract mfcc\n",
    "        feature = torchaudio.compliance.kaldi.mfcc(waveform, sample_frequency=sr)\n",
    "\n",
    "        # add deltas\n",
    "        d1 = torchaudio.functional.compute_deltas(feature)\n",
    "        d2 = torchaudio.functional.compute_deltas(d1)\n",
    "        feature = torch.cat([feature, d1, d2], dim=-1)\n",
    "\n",
    "        # Apply normalization (CMVN)\n",
    "        eps = 1e-9\n",
    "        mean = feature.mean(0, keepdim=True)\n",
    "        std = feature.std(0, keepdim=True, unbiased=False)\n",
    "        # print(feature.shape)\n",
    "        # print(mean, std)\n",
    "        feature = (feature - mean) / (std + eps)\n",
    "\n",
    "        # mel_spec = self.transform(waveform)\n",
    "        # # mel_spec = self.to_db(mel_spec)\n",
    "        # mel_spec = mel_spec.squeeze()\n",
    "        # mel_spec = mel_spec.permute(1, 0) # (F, L) -> (L, F)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# SEGMENTS_IN_CHUNK = 100  # set_size\n",
    "\n",
    "# INPUT_DIM = 128\n",
    "# OUTPUT_DIM = 128\n",
    "\n",
    "INPUT_DIM = 39\n",
    "OUTPUT_DIM = 13\n",
    "\n",
    "INTER_DIM_0 = 16\n",
    "INTER_DIM_1 = 8\n",
    "INTER_DIM_2 = 3\n",
    "INTER_DIM_3 = 3\n",
    "\n",
    "SIZE_LIST = [INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "\n",
    "LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "# model = TwoRNNAttn(1.0, SIZE_LIST, in_size=INPUT_DIM, \n",
    "#                       in2_size=INTER_DIM_0, hid_size=INTER_DIM_3, out_size=OUTPUT_DIM)\n",
    "model = PhonLearn_Net(1.0, SIZE_LIST, in_size=INPUT_DIM, \n",
    "                      in2_size=INTER_DIM_0, hid_size=INTER_DIM_3, out_size=OUTPUT_DIM)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhonLearn_Net(\n",
       "  (encoder): Encoder(\n",
       "    (lin_1): LinearPack(\n",
       "      (linear): Linear(in_features=39, out_features=16, bias=True)\n",
       "      (relu): Tanh()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (rnn): HM_LSTM(\n",
       "      (cell_1): HM_LSTMCell()\n",
       "      (cell_2): HM_LSTMCell()\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lin_1): LinearPack(\n",
       "      (linear): Linear(in_features=13, out_features=3, bias=True)\n",
       "      (relu): Tanh()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (rnn): LSTM(3, 16, batch_first=True)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (w_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (w_k): Linear(in_features=3, out_features=16, bias=True)\n",
       "      (w_v): Linear(in_features=3, out_features=16, bias=True)\n",
       "    )\n",
       "    (lin_2): LinearPack(\n",
       "      (linear): Linear(in_features=16, out_features=13, bias=True)\n",
       "      (relu): Tanh()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "# READ = False\n",
    "READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    # valid_losses.read()\n",
    "    # train_losses.read()\n",
    "\n",
    "    # model_name = last_model_name\n",
    "    model_raw_name = \"PT_0623152604_35_full\"\n",
    "    model_name = model_raw_name + \".pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "    model = PhonLearn_Net(1.0, SIZE_LIST, in_size=INPUT_DIM, \n",
    "                      in2_size=INTER_DIM_0, hid_size=INTER_DIM_3, out_size=OUTPUT_DIM)\n",
    "    model.load_state_dict(state)\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [],
   "source": [
    "mytrans = MyTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT)\n",
    "ds = AnnoWordWholeDatasetPlot(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "# small_len = int(0.1 * len(ds))\n",
    "# other_len = len(ds) - small_len\n",
    "\n",
    "# # Randomly split the dataset into train and validation sets\n",
    "# ds, other_ds = random_split(ds, [small_len, other_len])\n",
    "\n",
    "train_len = int(0.9995 * len(ds))\n",
    "valid_len = len(ds) - train_len\n",
    "\n",
    "# Randomly split the dataset into train and validation sets\n",
    "train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "# train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "valid_num = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneOut2ProgFrame(oneOut): \n",
    "    # oneOut is of tensor of shape (L, D)\n",
    "    df = pd.DataFrame(oneOut, columns=[\"dim_0\", \"dim_1\", \"dim_2\"])\n",
    "    df[\"timestep\"] = df.index\n",
    "    df = df[[\"timestep\", \"dim_0\", \"dim_1\", \"dim_2\"]]\n",
    "    return df\n",
    "def minmax(arr, a=-1, b=1): \n",
    "    min = arr.min()\n",
    "    max = arr.max()\n",
    "    return (b - a) * ((arr - min) / (max - min)) + a\n",
    "def operate_on(arr): \n",
    "    # return minmax(arr)\n",
    "    return arr\n",
    "def framify(these_hids): \n",
    "    # these are token categories to be included\n",
    "    # these hids are the corresponding hids\n",
    "    # these numtags are the corresponding tags, named using indices in these\n",
    "    # these_hids = st.zscore(these_hids, axis=0)\n",
    "    df = pd.DataFrame(data=these_hids)\n",
    "    # df = df.rename(columns={0: \"dim_0\", 1: \"dim_1\", 2: \"dim_2\"})\n",
    "    df['dim_0_norm'] = operate_on(df['dim_0'])\n",
    "    df['dim_1_norm'] = operate_on(df['dim_1'])\n",
    "    df['dim_2_norm'] = operate_on(df['dim_2'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_rec, info_idx, info_token, info_produce_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot3d(X): \n",
    "#     config = {\n",
    "#     'toImageButtonOptions': {\n",
    "#         'format': 'png', # one of png, svg, jpeg, webp\n",
    "#         'filename': 'custom_image',\n",
    "#         'height': 1280,\n",
    "#         'width': 1280,\n",
    "#         'scale': 1 # Multiply title/legend/axis/canvas sizes by this factor\n",
    "#     }\n",
    "#     }\n",
    "#     fig = px.scatter_3d(framify(X), x=\"dim_0_norm\", y=\"dim_1_norm\", z=\"dim_2_norm\", animation_frame=\"timestep\")\n",
    "#     fig.update_traces(marker=dict(size=2),\n",
    "#                     selector=dict(mode='markers'))\n",
    "#     fig.update_layout(\n",
    "#         scene = dict(\n",
    "#             xaxis = dict(nticks=8, range=[-1,1],),\n",
    "#                         yaxis = dict(nticks=8, range=[-1,1],),\n",
    "#                         zaxis = dict(nticks=8, range=[-1,1],),),)\n",
    "#     fig.update_layout(legend= {'itemsizing': 'constant'})\n",
    "#     # fig.update_layout(legend_title_text='Phone')\n",
    "#     fig.update_layout(\n",
    "#         legend=dict(\n",
    "#             x=0,\n",
    "#             y=1,\n",
    "#             title_font_family=\"Times New Roman\",\n",
    "#             font=dict(\n",
    "#                 family=\"Times New Roman\",\n",
    "#                 size=36,\n",
    "#                 color=\"black\"\n",
    "#             ),\n",
    "#             # bgcolor=\"LightSteelBlue\",\n",
    "#             bordercolor=\"Black\",\n",
    "#             borderwidth=1\n",
    "#         )\n",
    "#     )\n",
    "#     fig.update_layout(\n",
    "#         margin=dict(l=0, r=0, t=0, b=0),\n",
    "#     )\n",
    "#     camera = dict(\n",
    "#         eye=dict(x=0., y=0., z=2.5)\n",
    "#     )\n",
    "#     fig.update_layout(scene_camera=camera)\n",
    "#     html_plot = fig.to_html(full_html=False, config=config)\n",
    "#     # fig.show(config=config)\n",
    "#     return html_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3dtrajectory(X): \n",
    "    config = {\n",
    "    'toImageButtonOptions': {\n",
    "        'format': 'png', # one of png, svg, jpeg, webp\n",
    "        'filename': 'custom_image',\n",
    "        'height': 1280,\n",
    "        'width': 1280,\n",
    "        'scale': 1 # Multiply title/legend/axis/canvas sizes by this factor\n",
    "    }\n",
    "    }\n",
    "\n",
    "    fig = px.line_3d(framify(X), x=\"dim_0_norm\", y=\"dim_1_norm\", z=\"dim_2_norm\")\n",
    "    # fig.update_traces(marker=dict(size=5),\n",
    "    #                 selector=dict(mode='markers'))\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis = dict(nticks=8, range=[-1,1],),\n",
    "                        yaxis = dict(nticks=8, range=[-1,1],),\n",
    "                        zaxis = dict(nticks=8, range=[-1,1],),),)\n",
    "    # fig.update_layout(legend= {'itemsizing': 'constant'})\n",
    "    # fig.update_layout(legend_title_text='Phone')\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "            title_font_family=\"Times New Roman\",\n",
    "            font=dict(\n",
    "                family=\"Times New Roman\",\n",
    "                size=36,\n",
    "                color=\"black\"\n",
    "            ),\n",
    "            # bgcolor=\"LightSteelBlue\",\n",
    "            bordercolor=\"Black\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "    )\n",
    "    camera = dict(\n",
    "        eye=dict(x=0., y=0., z=2.5)\n",
    "    )\n",
    "    fig.update_layout(scene_camera=camera)\n",
    "    html_plot = fig.to_html(full_html=False, config=config)\n",
    "    # fig.show(config=config)\n",
    "    return html_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(htmlplot, info_rec, info_idx, info_token, info_produce_segs, model_serialnum=\"\"): \n",
    "    save_html_path = os.path.join(word_plot_path, \"{}_{}_{}.html\".format(model_serialnum, info_rec, info_idx).zfill(8))\n",
    "    with open(save_html_path, \"w\") as f: \n",
    "        f.write('<meta charset=\"UTF-8\">')\n",
    "        f.write(\"<h3>Rec: {}</h3>\".format(\"{}_{}\".format(info_rec, info_idx).zfill(8)))\n",
    "        f.write(\"<h3>Token: {}</h3>\".format(info_token))\n",
    "        f.write(\"<h3>Produced Segments: {}</h3>\".format(info_produce_segs))\n",
    "        f.write(\"<hr>\")\n",
    "        f.write(htmlplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def infer(model_num=\"\"): \n",
    "    model.eval()\n",
    "    for idx, (x, x_lens, info, info_rec, info_idx, info_token) in enumerate(valid_loader):\n",
    "        info = info[0]\n",
    "        info_rec = info_rec[0]\n",
    "        info_idx = info_idx[0]\n",
    "        info_token = info_token[0]\n",
    "\n",
    "        x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "        \n",
    "        x = x.to(device)\n",
    "\n",
    "        hid_r, z_1, z_2 = model.encode(x, x_mask)\n",
    "\n",
    "        hid_r = hid_r.cpu().detach().numpy()\n",
    "        res_df = oneOut2ProgFrame(hid_r[0]) # one in batch\n",
    "        res_df = framify(res_df)\n",
    "        htmlplot = plot3dtrajectory(res_df)\n",
    "        save_html(htmlplot, info_rec, info_idx, info_token, info, model_serialnum=model_raw_name)\n",
    "        print(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m----> 2\u001b[0m     infer()\n",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m, in \u001b[0;36minfer\u001b[1;34m(model_num)\u001b[0m\n\u001b[0;32m      9\u001b[0m x_mask \u001b[39m=\u001b[39m generate_mask_from_lengths_mat(x_lens, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m hid_r, z_1, z_2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(x, x_mask)\n\u001b[0;32m     15\u001b[0m hid_r \u001b[39m=\u001b[39m hid_r\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     16\u001b[0m res_df \u001b[39m=\u001b[39m oneOut2ProgFrame(hid_r[\u001b[39m0\u001b[39m]) \u001b[39m# one in batch\u001b[39;00m\n",
      "File \u001b[1;32ma:\\Projects\\wavln\\scripts\\model.py:131\u001b[0m, in \u001b[0;36mPhonLearn_Net.encode\u001b[1;34m(self, inputs, in_mask)\u001b[0m\n\u001b[0;32m    129\u001b[0m batch_size \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m    130\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39minits(batch_size\u001b[39m=\u001b[39mbatch_size, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mencode(inputs, in_mask, hidden)\n",
      "File \u001b[1;32ma:\\Projects\\wavln\\scripts\\model.py:49\u001b[0m, in \u001b[0;36mEncoder.encode\u001b[1;34m(self, inputs, in_mask, hidden)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, inputs, in_mask, hidden): \n\u001b[0;32m     48\u001b[0m     enc_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_1(inputs) \u001b[39m# (B, L, I) -> (B, L, I2)\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     h_1, h_2, z_1, z_2, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(enc_x, hidden) \u001b[39m# (B, L, I2) -> (B, L, S0) -> (B, L, S1)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     h_2 \u001b[39m=\u001b[39m mask_it(h_2, in_mask)\n\u001b[0;32m     51\u001b[0m     \u001b[39m# hid_r = self.lin_2(h_2) # (B, L, S1) -> (B, L, H)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[39m# hid_r = mask_it(hid_r, in_mask)\u001b[39;00m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ma:\\Projects\\wavln\\scripts\\layers.py:114\u001b[0m, in \u001b[0;36mHM_LSTM.forward\u001b[1;34m(self, inputs, hidden)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(time_steps):\n\u001b[0;32m    113\u001b[0m     h_t1, c_t1, z_t1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_1(c\u001b[39m=\u001b[39mc_t1, h_bottom\u001b[39m=\u001b[39minputs[:, t, :]\u001b[39m.\u001b[39mt(), h\u001b[39m=\u001b[39mh_t1, h_top\u001b[39m=\u001b[39mh_t2, z\u001b[39m=\u001b[39mz_t1, z_bottom\u001b[39m=\u001b[39mz_one)\n\u001b[1;32m--> 114\u001b[0m     h_t2, c_t2, z_t2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcell_2(c\u001b[39m=\u001b[39;49mc_t2, h_bottom\u001b[39m=\u001b[39;49mh_t1, h\u001b[39m=\u001b[39;49mh_t2, h_top\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, z\u001b[39m=\u001b[39;49mz_t2, z_bottom\u001b[39m=\u001b[39;49mz_t1)  \u001b[39m# 0.01s used\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     h_1 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [h_t1\u001b[39m.\u001b[39mt()]\n\u001b[0;32m    116\u001b[0m     h_2 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [h_t2\u001b[39m.\u001b[39mt()]\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ma:\\Projects\\wavln\\scripts\\layers.py:49\u001b[0m, in \u001b[0;36mHM_LSTMCell.forward\u001b[1;34m(self, c, h_bottom, h, h_top, z, z_bottom)\u001b[0m\n\u001b[0;32m     46\u001b[0m s_bottomup_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mU_11, h)\n\u001b[0;32m     47\u001b[0m s_bottomup \u001b[39m=\u001b[39m z_bottom\u001b[39m.\u001b[39mexpand_as(s_bottomup_) \u001b[39m*\u001b[39m s_bottomup_\n\u001b[1;32m---> 49\u001b[0m f_s \u001b[39m=\u001b[39m s_recur \u001b[39m+\u001b[39m s_topdown \u001b[39m+\u001b[39m s_bottomup \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand_as(s_recur)\n\u001b[0;32m     50\u001b[0m \u001b[39m# f_s.size = (4 * hidden_size + 1) * batch_size\u001b[39;00m\n\u001b[0;32m     51\u001b[0m f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(f_s[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, :])  \u001b[39m# hidden_size * batch_size\u001b[39;00m\n",
      "File \u001b[1;32ma:\\ProgramData\\anaconda3\\envs\\wavln\\lib\\site-packages\\torch\\nn\\modules\\module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
