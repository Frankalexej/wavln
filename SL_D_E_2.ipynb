{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Direct - English\n",
    "Version 1: In this version we make the model \"simple\": make the encoder RNN into normal RNN first and try to see the result.  \n",
    "Version 2: Learning is not very much. Following Dr Coupe's advice we try simpler model structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import pickle\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from recorder import *\n",
    "from loss import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PhxLearner, SimplerPhxLearner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规范用语；规定两种方式：全加载；按rec加载（舍弃了按chunk加载，处理起来更简单）\n",
    "# RandomPhoneDataset; AnnoPhoneDataset; AnnoSeqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset that loads cutted wave files from disk and returns input-output pairs for\n",
    "    training autoencoder. \n",
    "    \n",
    "    Version 3: wav -> mel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load_dir, load_control_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the class by reading a CSV file and merging the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        The function reads the CSV file from the provided control path, extracts the \"rec\" and \"idx\" columns,\n",
    "        and concatenates the values from these columns using an underscore. It then appends the \".wav\" extension\n",
    "        to each of the merged strings and converts the merged pandas Series to a list, which is assigned to\n",
    "        the 'dataset' attribute of the class.\n",
    "\n",
    "        Args:\n",
    "        load_dir (str): The directory containing the files to load.\n",
    "        load_control_path (str): The path to the CSV file containing the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        Attributes:\n",
    "        dataset (list): A list of merged strings from the \"rec\" and \"idx\" columns, with the \".wav\" extension.\n",
    "        \"\"\"\n",
    "        control_file = pd.read_csv(load_control_path)\n",
    "        control_file = control_file[control_file['n_frames'] > 400]\n",
    "        control_file = control_file[control_file['duration'] <= 2.0]\n",
    "        \n",
    "        # Extract the \"rec\" and \"idx\" columns\n",
    "        rec_col = control_file['rec'].astype(str)\n",
    "        idx_col = control_file['idx'].astype(str).str.zfill(8)\n",
    "        \n",
    "        # Merge the two columns by concatenating the strings with '_' and append extension name\n",
    "        merged_col = rec_col + '_' + idx_col + \".wav\"\n",
    "        \n",
    "        self.dataset = merged_col.tolist()\n",
    "        self.load_dir = load_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of input-output pairs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (input_data, output_data) for the given index.\n",
    "\n",
    "        The function first checks if the provided index is a tensor, and if so, converts it to a list.\n",
    "        It then constructs the file path for the .wav file using the dataset attribute and the provided index.\n",
    "        The .wav file is loaded using torchaudio, and its data is normalized. If a transform is provided,\n",
    "        the data is transformed using the specified transform. Finally, the input_data and output_data are\n",
    "        set to the same data (creating a tuple), and the tuple is returned.\n",
    "\n",
    "        Args:\n",
    "        idx (int or torch.Tensor): The index of the desired data.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing input_data and output_data, both of which are the audio data\n",
    "               from the .wav file at the specified index.\n",
    "\n",
    "        Note: \n",
    "        This function assumes that the class has the following attributes:\n",
    "        - self.load_dir (str): The directory containing the .wav files.\n",
    "        - self.dataset (list): A list of .wav file names.\n",
    "        - self.transform (callable, optional): An optional transform to apply to the audio data.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        wav_name = os.path.join(self.load_dir,\n",
    "                                self.dataset[idx])\n",
    "        \n",
    "        data, sample_rate = torchaudio.load(wav_name, normalize=True)\n",
    "        if self.transform:\n",
    "            data = self.transform(data, sr=sample_rate)\n",
    "        \n",
    "        # # Prepare for possible in-out discrepencies in the future\n",
    "        # input_data = data\n",
    "        # output_data = data\n",
    "        \n",
    "        return data\n",
    "\n",
    "def collate_fn(xx):\n",
    "    # only working for one data at the moment\n",
    "    batch_first = True\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=batch_first, padding_value=0)\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "\n",
    "class MyTransform(nn.Module): \n",
    "    def __init__(self, sample_rate, n_fft): \n",
    "        super().__init__()\n",
    "        # self.transform = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft, n_mels=64)\n",
    "        # self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        # self.transform = torchaudio.transforms.MFCC(n_mfcc=13)\n",
    "    \n",
    "    def forward(self, waveform, sr=16000): \n",
    "        # extract mfcc\n",
    "        feature = torchaudio.compliance.kaldi.mfcc(waveform, sample_frequency=sr)\n",
    "\n",
    "        # add deltas\n",
    "        d1 = torchaudio.functional.compute_deltas(feature)\n",
    "        d2 = torchaudio.functional.compute_deltas(d1)\n",
    "        feature = torch.cat([feature, d1, d2], dim=-1)\n",
    "\n",
    "        # Apply normalization (CMVN)\n",
    "        eps = 1e-9\n",
    "        mean = feature.mean(0, keepdim=True)\n",
    "        std = feature.std(0, keepdim=True, unbiased=False)\n",
    "        # print(feature.shape)\n",
    "        # print(mean, std)\n",
    "        feature = (feature - mean) / (std + eps)\n",
    "\n",
    "        # mel_spec = self.transform(waveform)\n",
    "        # # mel_spec = self.to_db(mel_spec)\n",
    "        # mel_spec = mel_spec.squeeze()\n",
    "        # mel_spec = mel_spec.permute(1, 0) # (F, L) -> (L, F)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIM = 39\n",
    "OUTPUT_DIM = 13\n",
    "\n",
    "INTER_DIM_0 = 64\n",
    "INTER_DIM_1 = 16\n",
    "INTER_DIM_2 = 3\n",
    "# INTER_DIM_3 = 3\n",
    "\n",
    "ENC_SIZE_LIST = [INPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "DEC_SIZE_LIST = [OUTPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "\n",
    "LOADER_WORKER = 16\n",
    "# LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "masked_recon_loss = MaskedLoss(recon_loss)\n",
    "model_loss = masked_recon_loss\n",
    "\n",
    "# model = PhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=1)\n",
    "model = SimplerPhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=2)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplerPhxLearner(\n",
       "  (encoder): RLEncoder(\n",
       "    (rnn): LSTM(39, 16, num_layers=2, batch_first=True)\n",
       "    (lin_2): LinearPack(\n",
       "      (linear): Linear(in_features=16, out_features=3, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): RALDecoder(\n",
       "    (rnn): LSTM(13, 3, num_layers=2, batch_first=True)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (lin_3): LinearPack(\n",
       "      (linear): Linear(in_features=3, out_features=13, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6275"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "ts = str(get_timestamp())\n",
    "# ts = \"0623152604\"\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "# save_train1hist_name = \"train_hist_recon{}.hst\".format(ts)\n",
    "# save_train2hist_name = \"train_hist_reg{}.hst\".format(ts)\n",
    "\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)\n",
    "# save_val1hist_name = \"val_hist_recon{}.hst\".format(ts)\n",
    "# save_val2hist_name = \"val_hist_reg{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "# train_recon_losses = LossRecorder(model_save_dir + save_train1hist_name)\n",
    "# train_reg_losses = LossRecorder(model_save_dir + save_train2hist_name)\n",
    "\n",
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "# valid_recon_losses = LossRecorder(model_save_dir + save_val1hist_name)\n",
    "# valid_reg_losses = LossRecorder(model_save_dir + save_val2hist_name)\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "READ = False\n",
    "# READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    valid_losses.read()\n",
    "    train_losses.read()\n",
    "\n",
    "    # model_name = last_model_namec\n",
    "    model_name = \"PT_0623152604_29_full.pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "    model = PhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=1)\n",
    "    \n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [],
   "source": [
    "mytrans = MyTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT)\n",
    "ds = PhoneDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "\n",
    "# # this is to reduce the size of the dataset when the training power is not sufficient\n",
    "# small_len = int(0.1 * len(ds))\n",
    "# other_len = len(ds) - small_len\n",
    "\n",
    "# # # Randomly split the dataset into train and validation sets\n",
    "# ds, other_ds = random_split(ds, [small_len, other_len])\n",
    "\n",
    "train_len = int(0.8 * len(ds))\n",
    "valid_len = len(ds) - train_len\n",
    "\n",
    "# Randomly split the dataset into train and validation sets\n",
    "train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "valid_num = len(valid_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BASE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y = x[:, :, :13]    # extract MFCC-only data\n",
    "            \n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            # 这个函数计算的是全局梯度范数\n",
    "            # torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            # parameters: an iterable of Variables that will have gradients normalized\n",
    "            # max_norm: max norm of the gradients(阈值设定)\n",
    "            # norm_type: type of the used p-norm. Can be'inf'for infinity norm(定义范数类型)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            y = x[:, :, :13]    # extract MFCC-only data\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss  1.105 in Step 0\n",
      "Training loss  1.029 in Step 100\n",
      "Training loss  1.009 in Step 200\n",
      "Training loss  1.002 in Step 300\n",
      "Training loss  1.000 in Step 400\n",
      "Training loss  1.000 in Step 500\n",
      "Training loss  1.000 in Step 600\n",
      "Training loss  0.999 in Step 700\n",
      "Training loss  0.983 in Step 800\n",
      "Training loss  0.975 in Step 900\n",
      "Training loss  0.958 in Step 1000\n",
      "Training loss  0.954 in Step 1100\n",
      "Training loss  0.958 in Step 1200\n",
      "Training loss  0.943 in Step 1300\n",
      "Training loss  0.951 in Step 1400\n",
      "Training loss  0.956 in Step 1500\n",
      "Training loss  0.954 in Step 1600\n",
      "Training loss  0.940 in Step 1700\n",
      "※※※Training loss  0.981※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.949 in Step 0\n",
      "Valid loss  0.950 in Step 100\n",
      "Valid loss  0.948 in Step 200\n",
      "Valid loss  0.955 in Step 300\n",
      "Valid loss  0.941 in Step 400\n",
      "※※※Valid loss  0.947※※※\n",
      "Epoch 1\n",
      "Training loss  0.938 in Step 0\n",
      "Training loss  0.948 in Step 100\n",
      "Training loss  0.939 in Step 200\n",
      "Training loss  0.933 in Step 300\n",
      "Training loss  0.936 in Step 400\n",
      "Training loss  0.922 in Step 500\n",
      "Training loss  0.916 in Step 600\n",
      "Training loss  0.928 in Step 700\n",
      "Training loss  0.932 in Step 800\n",
      "Training loss  0.929 in Step 900\n",
      "Training loss  0.935 in Step 1000\n",
      "Training loss  0.935 in Step 1100\n",
      "Training loss  0.930 in Step 1200\n",
      "Training loss  0.932 in Step 1300\n",
      "Training loss  0.916 in Step 1400\n",
      "Training loss  0.914 in Step 1500\n",
      "Training loss  0.940 in Step 1600\n",
      "Training loss  0.916 in Step 1700\n",
      "※※※Training loss  0.931※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.917 in Step 0\n",
      "Valid loss  0.916 in Step 100\n",
      "Valid loss  0.917 in Step 200\n",
      "Valid loss  0.925 in Step 300\n",
      "Valid loss  0.912 in Step 400\n",
      "※※※Valid loss  0.918※※※\n",
      "Epoch 2\n",
      "Training loss  0.931 in Step 0\n",
      "Training loss  0.921 in Step 100\n",
      "Training loss  0.917 in Step 200\n",
      "Training loss  0.918 in Step 300\n",
      "Training loss  0.909 in Step 400\n",
      "Training loss  0.900 in Step 500\n",
      "Training loss  0.901 in Step 600\n",
      "Training loss  0.895 in Step 700\n",
      "Training loss  0.901 in Step 800\n",
      "Training loss  0.915 in Step 900\n",
      "Training loss  0.876 in Step 1000\n",
      "Training loss  0.886 in Step 1100\n",
      "Training loss  0.890 in Step 1200\n",
      "Training loss  0.884 in Step 1300\n",
      "Training loss  0.899 in Step 1400\n",
      "Training loss  0.897 in Step 1500\n",
      "Training loss  0.887 in Step 1600\n",
      "Training loss  0.891 in Step 1700\n",
      "※※※Training loss  0.902※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.888 in Step 0\n",
      "Valid loss  0.890 in Step 100\n",
      "Valid loss  0.889 in Step 200\n",
      "Valid loss  0.893 in Step 300\n",
      "Valid loss  0.889 in Step 400\n",
      "※※※Valid loss  0.892※※※\n",
      "Epoch 3\n",
      "Training loss  0.890 in Step 0\n",
      "Training loss  0.908 in Step 100\n",
      "Training loss  0.895 in Step 200\n",
      "Training loss  0.894 in Step 300\n",
      "Training loss  0.891 in Step 400\n",
      "Training loss  0.893 in Step 500\n",
      "Training loss  0.899 in Step 600\n",
      "Training loss  0.873 in Step 700\n",
      "Training loss  0.887 in Step 800\n",
      "Training loss  0.875 in Step 900\n",
      "Training loss  0.866 in Step 1000\n",
      "Training loss  0.892 in Step 1100\n",
      "Training loss  0.883 in Step 1200\n",
      "Training loss  0.892 in Step 1300\n",
      "Training loss  0.873 in Step 1400\n",
      "Training loss  0.885 in Step 1500\n",
      "Training loss  0.865 in Step 1600\n",
      "Training loss  0.875 in Step 1700\n",
      "※※※Training loss  0.885※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.873 in Step 0\n",
      "Valid loss  0.875 in Step 100\n",
      "Valid loss  0.877 in Step 200\n",
      "Valid loss  0.884 in Step 300\n",
      "Valid loss  0.879 in Step 400\n",
      "※※※Valid loss  0.879※※※\n",
      "Epoch 4\n",
      "Training loss  0.874 in Step 0\n",
      "Training loss  0.871 in Step 100\n",
      "Training loss  0.887 in Step 200\n",
      "Training loss  0.888 in Step 300\n",
      "Training loss  0.873 in Step 400\n",
      "Training loss  0.876 in Step 500\n",
      "Training loss  0.900 in Step 600\n",
      "Training loss  0.864 in Step 700\n",
      "Training loss  0.874 in Step 800\n",
      "Training loss  0.879 in Step 900\n",
      "Training loss  0.862 in Step 1000\n",
      "Training loss  0.879 in Step 1100\n",
      "Training loss  0.874 in Step 1200\n",
      "Training loss  0.866 in Step 1300\n",
      "Training loss  0.880 in Step 1400\n",
      "Training loss  0.876 in Step 1500\n",
      "Training loss  0.871 in Step 1600\n",
      "Training loss  0.878 in Step 1700\n",
      "※※※Training loss  0.876※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.862 in Step 0\n",
      "Valid loss  0.875 in Step 100\n",
      "Valid loss  0.872 in Step 200\n",
      "Valid loss  0.874 in Step 300\n",
      "Valid loss  0.876 in Step 400\n",
      "※※※Valid loss  0.874※※※\n",
      "Epoch 5\n",
      "Training loss  0.873 in Step 0\n",
      "Training loss  0.864 in Step 100\n",
      "Training loss  0.864 in Step 200\n",
      "Training loss  0.873 in Step 300\n",
      "Training loss  0.870 in Step 400\n",
      "Training loss  0.888 in Step 500\n",
      "Training loss  0.881 in Step 600\n",
      "Training loss  0.858 in Step 700\n",
      "Training loss  0.883 in Step 800\n",
      "Training loss  0.868 in Step 900\n",
      "Training loss  0.865 in Step 1000\n",
      "Training loss  0.873 in Step 1100\n",
      "Training loss  0.871 in Step 1200\n",
      "Training loss  0.871 in Step 1300\n",
      "Training loss  0.858 in Step 1400\n",
      "Training loss  0.874 in Step 1500\n",
      "Training loss  0.852 in Step 1600\n",
      "Training loss  0.876 in Step 1700\n",
      "※※※Training loss  0.870※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.857 in Step 0\n",
      "Valid loss  0.869 in Step 100\n",
      "Valid loss  0.868 in Step 200\n",
      "Valid loss  0.871 in Step 300\n",
      "Valid loss  0.873 in Step 400\n",
      "※※※Valid loss  0.870※※※\n",
      "Epoch 6\n",
      "Training loss  0.873 in Step 0\n",
      "Training loss  0.866 in Step 100\n",
      "Training loss  0.855 in Step 200\n",
      "Training loss  0.872 in Step 300\n",
      "Training loss  0.868 in Step 400\n",
      "Training loss  0.880 in Step 500\n",
      "Training loss  0.850 in Step 600\n",
      "Training loss  0.870 in Step 700\n",
      "Training loss  0.854 in Step 800\n",
      "Training loss  0.874 in Step 900\n",
      "Training loss  0.863 in Step 1000\n",
      "Training loss  0.873 in Step 1100\n",
      "Training loss  0.872 in Step 1200\n",
      "Training loss  0.849 in Step 1300\n",
      "Training loss  0.857 in Step 1400\n",
      "Training loss  0.874 in Step 1500\n",
      "Training loss  0.870 in Step 1600\n",
      "Training loss  0.872 in Step 1700\n",
      "※※※Training loss  0.867※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.851 in Step 0\n",
      "Valid loss  0.865 in Step 100\n",
      "Valid loss  0.860 in Step 200\n",
      "Valid loss  0.862 in Step 300\n",
      "Valid loss  0.869 in Step 400\n",
      "※※※Valid loss  0.864※※※\n",
      "Epoch 7\n",
      "Training loss  0.847 in Step 0\n",
      "Training loss  0.870 in Step 100\n",
      "Training loss  0.861 in Step 200\n",
      "Training loss  0.857 in Step 300\n",
      "Training loss  0.865 in Step 400\n",
      "Training loss  0.863 in Step 500\n",
      "Training loss  0.872 in Step 600\n",
      "Training loss  0.859 in Step 700\n",
      "Training loss  0.854 in Step 800\n",
      "Training loss  0.861 in Step 900\n",
      "Training loss  0.862 in Step 1000\n",
      "Training loss  0.873 in Step 1100\n",
      "Training loss  0.865 in Step 1200\n",
      "Training loss  0.849 in Step 1300\n",
      "Training loss  0.854 in Step 1400\n",
      "Training loss  0.857 in Step 1500\n",
      "Training loss  0.887 in Step 1600\n",
      "Training loss  0.865 in Step 1700\n",
      "※※※Training loss  0.862※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.847 in Step 0\n",
      "Valid loss  0.862 in Step 100\n",
      "Valid loss  0.857 in Step 200\n",
      "Valid loss  0.858 in Step 300\n",
      "Valid loss  0.866 in Step 400\n",
      "※※※Valid loss  0.861※※※\n",
      "Epoch 8\n",
      "Training loss  0.863 in Step 0\n",
      "Training loss  0.855 in Step 100\n",
      "Training loss  0.861 in Step 200\n",
      "Training loss  0.857 in Step 300\n",
      "Training loss  0.862 in Step 400\n",
      "Training loss  0.853 in Step 500\n",
      "Training loss  0.857 in Step 600\n",
      "Training loss  0.871 in Step 700\n",
      "Training loss  0.861 in Step 800\n",
      "Training loss  0.862 in Step 900\n",
      "Training loss  0.876 in Step 1000\n",
      "Training loss  0.859 in Step 1100\n",
      "Training loss  0.854 in Step 1200\n",
      "Training loss  0.881 in Step 1300\n",
      "Training loss  0.864 in Step 1400\n",
      "Training loss  0.862 in Step 1500\n",
      "Training loss  0.861 in Step 1600\n",
      "Training loss  0.845 in Step 1700\n",
      "※※※Training loss  0.859※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.847 in Step 0\n",
      "Valid loss  0.862 in Step 100\n",
      "Valid loss  0.854 in Step 200\n",
      "Valid loss  0.855 in Step 300\n",
      "Valid loss  0.864 in Step 400\n",
      "※※※Valid loss  0.859※※※\n",
      "Epoch 9\n",
      "Training loss  0.868 in Step 0\n",
      "Training loss  0.867 in Step 100\n",
      "Training loss  0.843 in Step 200\n",
      "Training loss  0.859 in Step 300\n",
      "Training loss  0.859 in Step 400\n",
      "Training loss  0.869 in Step 500\n",
      "Training loss  0.861 in Step 600\n",
      "Training loss  0.838 in Step 700\n",
      "Training loss  0.860 in Step 800\n",
      "Training loss  0.869 in Step 900\n",
      "Training loss  0.849 in Step 1000\n",
      "Training loss  0.856 in Step 1100\n",
      "Training loss  0.851 in Step 1200\n",
      "Training loss  0.865 in Step 1300\n",
      "Training loss  0.859 in Step 1400\n",
      "Training loss  0.846 in Step 1500\n",
      "Training loss  0.864 in Step 1600\n",
      "Training loss  0.857 in Step 1700\n",
      "※※※Training loss  0.857※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.847 in Step 0\n",
      "Valid loss  0.859 in Step 100\n",
      "Valid loss  0.851 in Step 200\n",
      "Valid loss  0.855 in Step 300\n",
      "Valid loss  0.862 in Step 400\n",
      "※※※Valid loss  0.858※※※\n",
      "Epoch 10\n",
      "Training loss  0.876 in Step 0\n",
      "Training loss  0.859 in Step 100\n",
      "Training loss  0.858 in Step 200\n",
      "Training loss  0.848 in Step 300\n",
      "Training loss  0.854 in Step 400\n",
      "Training loss  0.861 in Step 500\n",
      "Training loss  0.842 in Step 600\n",
      "Training loss  0.844 in Step 700\n",
      "Training loss  0.845 in Step 800\n",
      "Training loss  0.849 in Step 900\n",
      "Training loss  0.837 in Step 1000\n",
      "Training loss  0.854 in Step 1100\n",
      "Training loss  0.867 in Step 1200\n",
      "Training loss  0.848 in Step 1300\n",
      "Training loss  0.864 in Step 1400\n",
      "Training loss  0.855 in Step 1500\n",
      "Training loss  0.847 in Step 1600\n",
      "Training loss  0.866 in Step 1700\n",
      "※※※Training loss  0.854※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.850 in Step 0\n",
      "Valid loss  0.858 in Step 100\n",
      "Valid loss  0.848 in Step 200\n",
      "Valid loss  0.857 in Step 300\n",
      "Valid loss  0.860 in Step 400\n",
      "※※※Valid loss  0.857※※※\n",
      "Epoch 11\n",
      "Training loss  0.851 in Step 0\n",
      "Training loss  0.856 in Step 100\n",
      "Training loss  0.850 in Step 200\n",
      "Training loss  0.866 in Step 300\n",
      "Training loss  0.845 in Step 400\n",
      "Training loss  0.861 in Step 500\n",
      "Training loss  0.840 in Step 600\n",
      "Training loss  0.841 in Step 700\n",
      "Training loss  0.842 in Step 800\n",
      "Training loss  0.848 in Step 900\n",
      "Training loss  0.854 in Step 1000\n",
      "Training loss  0.854 in Step 1100\n",
      "Training loss  0.840 in Step 1200\n",
      "Training loss  0.844 in Step 1300\n",
      "Training loss  0.854 in Step 1400\n",
      "Training loss  0.850 in Step 1500\n",
      "Training loss  0.855 in Step 1600\n",
      "Training loss  0.838 in Step 1700\n",
      "※※※Training loss  0.850※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.839 in Step 0\n",
      "Valid loss  0.853 in Step 100\n",
      "Valid loss  0.843 in Step 200\n",
      "Valid loss  0.849 in Step 300\n",
      "Valid loss  0.850 in Step 400\n",
      "※※※Valid loss  0.851※※※\n",
      "Epoch 12\n",
      "Training loss  0.854 in Step 0\n",
      "Training loss  0.836 in Step 100\n",
      "Training loss  0.833 in Step 200\n",
      "Training loss  0.849 in Step 300\n",
      "Training loss  0.837 in Step 400\n",
      "Training loss  0.856 in Step 500\n",
      "Training loss  0.840 in Step 600\n",
      "Training loss  0.835 in Step 700\n",
      "Training loss  0.861 in Step 800\n",
      "Training loss  0.853 in Step 900\n",
      "Training loss  0.833 in Step 1000\n",
      "Training loss  0.848 in Step 1100\n",
      "Training loss  0.838 in Step 1200\n",
      "Training loss  0.850 in Step 1300\n",
      "Training loss  0.854 in Step 1400\n",
      "Training loss  0.843 in Step 1500\n",
      "Training loss  0.862 in Step 1600\n",
      "Training loss  0.845 in Step 1700\n",
      "※※※Training loss  0.847※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.837 in Step 0\n",
      "Valid loss  0.847 in Step 100\n",
      "Valid loss  0.837 in Step 200\n",
      "Valid loss  0.844 in Step 300\n",
      "Valid loss  0.846 in Step 400\n",
      "※※※Valid loss  0.846※※※\n",
      "Epoch 13\n",
      "Training loss  0.851 in Step 0\n",
      "Training loss  0.835 in Step 100\n",
      "Training loss  0.837 in Step 200\n",
      "Training loss  0.845 in Step 300\n",
      "Training loss  0.825 in Step 400\n",
      "Training loss  0.839 in Step 500\n",
      "Training loss  0.849 in Step 600\n",
      "Training loss  0.834 in Step 700\n",
      "Training loss  0.859 in Step 800\n",
      "Training loss  0.840 in Step 900\n",
      "Training loss  0.854 in Step 1000\n",
      "Training loss  0.836 in Step 1100\n",
      "Training loss  0.839 in Step 1200\n",
      "Training loss  0.849 in Step 1300\n",
      "Training loss  0.853 in Step 1400\n",
      "Training loss  0.838 in Step 1500\n",
      "Training loss  0.849 in Step 1600\n",
      "Training loss  0.823 in Step 1700\n",
      "※※※Training loss  0.844※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.833 in Step 0\n",
      "Valid loss  0.841 in Step 100\n",
      "Valid loss  0.836 in Step 200\n",
      "Valid loss  0.841 in Step 300\n",
      "Valid loss  0.844 in Step 400\n",
      "※※※Valid loss  0.844※※※\n",
      "Epoch 14\n",
      "Training loss  0.851 in Step 0\n",
      "Training loss  0.844 in Step 100\n",
      "Training loss  0.828 in Step 200\n",
      "Training loss  0.848 in Step 300\n",
      "Training loss  0.841 in Step 400\n",
      "Training loss  0.838 in Step 500\n",
      "Training loss  0.842 in Step 600\n",
      "Training loss  0.835 in Step 700\n",
      "Training loss  0.865 in Step 800\n",
      "Training loss  0.848 in Step 900\n",
      "Training loss  0.835 in Step 1000\n",
      "Training loss  0.849 in Step 1100\n",
      "Training loss  0.856 in Step 1200\n",
      "Training loss  0.836 in Step 1300\n",
      "Training loss  0.825 in Step 1400\n",
      "Training loss  0.840 in Step 1500\n",
      "Training loss  0.831 in Step 1600\n",
      "Training loss  0.830 in Step 1700\n",
      "※※※Training loss  0.841※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.830 in Step 0\n",
      "Valid loss  0.841 in Step 100\n",
      "Valid loss  0.833 in Step 200\n",
      "Valid loss  0.840 in Step 300\n",
      "Valid loss  0.839 in Step 400\n",
      "※※※Valid loss  0.842※※※\n",
      "Epoch 15\n",
      "Training loss  0.825 in Step 0\n",
      "Training loss  0.833 in Step 100\n",
      "Training loss  0.838 in Step 200\n",
      "Training loss  0.827 in Step 300\n",
      "Training loss  0.835 in Step 400\n",
      "Training loss  0.826 in Step 500\n",
      "Training loss  0.835 in Step 600\n",
      "Training loss  0.830 in Step 700\n",
      "Training loss  0.838 in Step 800\n",
      "Training loss  0.841 in Step 900\n",
      "Training loss  0.845 in Step 1000\n",
      "Training loss  0.844 in Step 1100\n",
      "Training loss  0.846 in Step 1200\n",
      "Training loss  0.849 in Step 1300\n",
      "Training loss  0.835 in Step 1400\n",
      "Training loss  0.843 in Step 1500\n",
      "Training loss  0.847 in Step 1600\n",
      "Training loss  0.858 in Step 1700\n",
      "※※※Training loss  0.839※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.831 in Step 0\n",
      "Valid loss  0.834 in Step 100\n",
      "Valid loss  0.830 in Step 200\n",
      "Valid loss  0.834 in Step 300\n",
      "Valid loss  0.835 in Step 400\n",
      "※※※Valid loss  0.840※※※\n",
      "Epoch 16\n",
      "Training loss  0.828 in Step 0\n",
      "Training loss  0.830 in Step 100\n",
      "Training loss  0.835 in Step 200\n",
      "Training loss  0.849 in Step 300\n",
      "Training loss  0.840 in Step 400\n",
      "Training loss  0.846 in Step 500\n",
      "Training loss  0.852 in Step 600\n",
      "Training loss  0.837 in Step 700\n",
      "Training loss  0.835 in Step 800\n",
      "Training loss  0.846 in Step 900\n",
      "Training loss  0.831 in Step 1000\n",
      "Training loss  0.834 in Step 1100\n",
      "Training loss  0.828 in Step 1200\n",
      "Training loss  0.834 in Step 1300\n",
      "Training loss  0.854 in Step 1400\n",
      "Training loss  0.830 in Step 1500\n",
      "Training loss  0.848 in Step 1600\n",
      "Training loss  0.823 in Step 1700\n",
      "※※※Training loss  0.838※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.829 in Step 0\n",
      "Valid loss  0.833 in Step 100\n",
      "Valid loss  0.825 in Step 200\n",
      "Valid loss  0.833 in Step 300\n",
      "Valid loss  0.834 in Step 400\n",
      "※※※Valid loss  0.837※※※\n",
      "Epoch 17\n",
      "Training loss  0.830 in Step 0\n",
      "Training loss  0.840 in Step 100\n",
      "Training loss  0.837 in Step 200\n",
      "Training loss  0.833 in Step 300\n",
      "Training loss  0.843 in Step 400\n",
      "Training loss  0.850 in Step 500\n",
      "Training loss  0.827 in Step 600\n",
      "Training loss  0.833 in Step 700\n",
      "Training loss  0.816 in Step 800\n",
      "Training loss  0.836 in Step 900\n",
      "Training loss  0.816 in Step 1000\n",
      "Training loss  0.838 in Step 1100\n",
      "Training loss  0.826 in Step 1200\n",
      "Training loss  0.843 in Step 1300\n",
      "Training loss  0.830 in Step 1400\n",
      "Training loss  0.824 in Step 1500\n",
      "Training loss  0.829 in Step 1600\n",
      "Training loss  0.836 in Step 1700\n",
      "※※※Training loss  0.835※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.830 in Step 0\n",
      "Valid loss  0.830 in Step 100\n",
      "Valid loss  0.826 in Step 200\n",
      "Valid loss  0.831 in Step 300\n",
      "Valid loss  0.834 in Step 400\n",
      "※※※Valid loss  0.837※※※\n",
      "Epoch 18\n",
      "Training loss  0.830 in Step 0\n",
      "Training loss  0.828 in Step 100\n",
      "Training loss  0.836 in Step 200\n",
      "Training loss  0.837 in Step 300\n",
      "Training loss  0.831 in Step 400\n",
      "Training loss  0.826 in Step 500\n",
      "Training loss  0.846 in Step 600\n",
      "Training loss  0.827 in Step 700\n",
      "Training loss  0.837 in Step 800\n",
      "Training loss  0.831 in Step 900\n",
      "Training loss  0.837 in Step 1000\n",
      "Training loss  0.834 in Step 1100\n",
      "Training loss  0.848 in Step 1200\n",
      "Training loss  0.836 in Step 1300\n",
      "Training loss  0.824 in Step 1400\n",
      "Training loss  0.834 in Step 1500\n",
      "Training loss  0.833 in Step 1600\n",
      "Training loss  0.833 in Step 1700\n",
      "※※※Training loss  0.834※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.825 in Step 0\n",
      "Valid loss  0.827 in Step 100\n",
      "Valid loss  0.823 in Step 200\n",
      "Valid loss  0.830 in Step 300\n",
      "Valid loss  0.831 in Step 400\n",
      "※※※Valid loss  0.833※※※\n",
      "Epoch 19\n",
      "Training loss  0.815 in Step 0\n",
      "Training loss  0.839 in Step 100\n",
      "Training loss  0.824 in Step 200\n",
      "Training loss  0.840 in Step 300\n",
      "Training loss  0.835 in Step 400\n",
      "Training loss  0.834 in Step 500\n",
      "Training loss  0.826 in Step 600\n",
      "Training loss  0.851 in Step 700\n",
      "Training loss  0.821 in Step 800\n",
      "Training loss  0.837 in Step 900\n",
      "Training loss  0.821 in Step 1000\n",
      "Training loss  0.844 in Step 1100\n",
      "Training loss  0.832 in Step 1200\n",
      "Training loss  0.835 in Step 1300\n",
      "Training loss  0.823 in Step 1400\n",
      "Training loss  0.848 in Step 1500\n",
      "Training loss  0.817 in Step 1600\n",
      "Training loss  0.852 in Step 1700\n",
      "※※※Training loss  0.832※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.822 in Step 0\n",
      "Valid loss  0.828 in Step 100\n",
      "Valid loss  0.823 in Step 200\n",
      "Valid loss  0.829 in Step 300\n",
      "Valid loss  0.831 in Step 400\n",
      "※※※Valid loss  0.834※※※\n",
      "Epoch 20\n",
      "Training loss  0.834 in Step 0\n",
      "Training loss  0.813 in Step 100\n",
      "Training loss  0.827 in Step 200\n",
      "Training loss  0.831 in Step 300\n",
      "Training loss  0.829 in Step 400\n",
      "Training loss  0.817 in Step 500\n",
      "Training loss  0.839 in Step 600\n",
      "Training loss  0.827 in Step 700\n",
      "Training loss  0.828 in Step 800\n",
      "Training loss  0.830 in Step 900\n",
      "Training loss  0.822 in Step 1000\n",
      "Training loss  0.831 in Step 1100\n",
      "Training loss  0.830 in Step 1200\n",
      "Training loss  0.831 in Step 1300\n",
      "Training loss  0.821 in Step 1400\n",
      "Training loss  0.827 in Step 1500\n",
      "Training loss  0.831 in Step 1600\n",
      "Training loss  0.825 in Step 1700\n",
      "※※※Training loss  0.830※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.818 in Step 0\n",
      "Valid loss  0.824 in Step 100\n",
      "Valid loss  0.819 in Step 200\n",
      "Valid loss  0.823 in Step 300\n",
      "Valid loss  0.828 in Step 400\n",
      "※※※Valid loss  0.830※※※\n",
      "Epoch 21\n",
      "Training loss  0.824 in Step 0\n",
      "Training loss  0.825 in Step 100\n",
      "Training loss  0.831 in Step 200\n",
      "Training loss  0.820 in Step 300\n",
      "Training loss  0.833 in Step 400\n",
      "Training loss  0.821 in Step 500\n",
      "Training loss  0.857 in Step 600\n",
      "Training loss  0.826 in Step 700\n",
      "Training loss  0.821 in Step 800\n",
      "Training loss  0.823 in Step 900\n",
      "Training loss  0.817 in Step 1000\n",
      "Training loss  0.819 in Step 1100\n",
      "Training loss  0.849 in Step 1200\n",
      "Training loss  0.824 in Step 1300\n",
      "Training loss  0.823 in Step 1400\n",
      "Training loss  0.813 in Step 1500\n",
      "Training loss  0.828 in Step 1600\n",
      "Training loss  0.831 in Step 1700\n",
      "※※※Training loss  0.829※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.815 in Step 0\n",
      "Valid loss  0.821 in Step 100\n",
      "Valid loss  0.817 in Step 200\n",
      "Valid loss  0.821 in Step 300\n",
      "Valid loss  0.826 in Step 400\n",
      "※※※Valid loss  0.828※※※\n",
      "Epoch 22\n",
      "Training loss  0.826 in Step 0\n",
      "Training loss  0.820 in Step 100\n",
      "Training loss  0.827 in Step 200\n",
      "Training loss  0.823 in Step 300\n",
      "Training loss  0.840 in Step 400\n",
      "Training loss  0.837 in Step 500\n",
      "Training loss  0.817 in Step 600\n",
      "Training loss  0.827 in Step 700\n",
      "Training loss  0.808 in Step 800\n",
      "Training loss  0.821 in Step 900\n",
      "Training loss  0.823 in Step 1000\n",
      "Training loss  0.845 in Step 1100\n",
      "Training loss  0.828 in Step 1200\n",
      "Training loss  0.824 in Step 1300\n",
      "Training loss  0.827 in Step 1400\n",
      "Training loss  0.836 in Step 1500\n",
      "Training loss  0.836 in Step 1600\n",
      "Training loss  0.829 in Step 1700\n",
      "※※※Training loss  0.826※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.815 in Step 0\n",
      "Valid loss  0.820 in Step 100\n",
      "Valid loss  0.814 in Step 200\n",
      "Valid loss  0.819 in Step 300\n",
      "Valid loss  0.822 in Step 400\n",
      "※※※Valid loss  0.826※※※\n",
      "Epoch 23\n",
      "Training loss  0.822 in Step 0\n",
      "Training loss  0.824 in Step 100\n",
      "Training loss  0.827 in Step 200\n",
      "Training loss  0.821 in Step 300\n",
      "Training loss  0.832 in Step 400\n",
      "Training loss  0.834 in Step 500\n",
      "Training loss  0.818 in Step 600\n",
      "Training loss  0.820 in Step 700\n",
      "Training loss  0.817 in Step 800\n",
      "Training loss  0.838 in Step 900\n",
      "Training loss  0.813 in Step 1000\n",
      "Training loss  0.825 in Step 1100\n",
      "Training loss  0.817 in Step 1200\n",
      "Training loss  0.833 in Step 1300\n",
      "Training loss  0.821 in Step 1400\n",
      "Training loss  0.811 in Step 1500\n",
      "Training loss  0.831 in Step 1600\n",
      "Training loss  0.816 in Step 1700\n",
      "※※※Training loss  0.825※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.818 in Step 0\n",
      "Valid loss  0.822 in Step 100\n",
      "Valid loss  0.817 in Step 200\n",
      "Valid loss  0.826 in Step 300\n",
      "Valid loss  0.826 in Step 400\n",
      "※※※Valid loss  0.830※※※\n",
      "Epoch 24\n",
      "Training loss  0.832 in Step 0\n",
      "Training loss  0.839 in Step 100\n",
      "Training loss  0.838 in Step 200\n",
      "Training loss  0.836 in Step 300\n",
      "Training loss  0.816 in Step 400\n",
      "Training loss  0.820 in Step 500\n",
      "Training loss  0.817 in Step 600\n",
      "Training loss  0.829 in Step 700\n",
      "Training loss  0.824 in Step 800\n",
      "Training loss  0.820 in Step 900\n",
      "Training loss  0.821 in Step 1000\n",
      "Training loss  0.839 in Step 1100\n",
      "Training loss  0.807 in Step 1200\n",
      "Training loss  0.848 in Step 1300\n",
      "Training loss  0.834 in Step 1400\n",
      "Training loss  0.825 in Step 1500\n",
      "Training loss  0.830 in Step 1600\n",
      "Training loss  0.821 in Step 1700\n",
      "※※※Training loss  0.824※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.812 in Step 0\n",
      "Valid loss  0.821 in Step 100\n",
      "Valid loss  0.812 in Step 200\n",
      "Valid loss  0.817 in Step 300\n",
      "Valid loss  0.821 in Step 400\n",
      "※※※Valid loss  0.825※※※\n",
      "Epoch 25\n",
      "Training loss  0.825 in Step 0\n",
      "Training loss  0.809 in Step 100\n",
      "Training loss  0.824 in Step 200\n",
      "Training loss  0.815 in Step 300\n",
      "Training loss  0.827 in Step 400\n",
      "Training loss  0.818 in Step 500\n",
      "Training loss  0.827 in Step 600\n",
      "Training loss  0.819 in Step 700\n",
      "Training loss  0.824 in Step 800\n",
      "Training loss  0.837 in Step 900\n",
      "Training loss  0.811 in Step 1000\n",
      "Training loss  0.838 in Step 1100\n",
      "Training loss  0.827 in Step 1200\n",
      "Training loss  0.813 in Step 1300\n",
      "Training loss  0.813 in Step 1400\n",
      "Training loss  0.817 in Step 1500\n",
      "Training loss  0.811 in Step 1600\n",
      "Training loss  0.809 in Step 1700\n",
      "※※※Training loss  0.822※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.812 in Step 0\n",
      "Valid loss  0.820 in Step 100\n",
      "Valid loss  0.812 in Step 200\n",
      "Valid loss  0.815 in Step 300\n",
      "Valid loss  0.821 in Step 400\n",
      "※※※Valid loss  0.825※※※\n",
      "Epoch 26\n",
      "Training loss  0.818 in Step 0\n",
      "Training loss  0.819 in Step 100\n",
      "Training loss  0.814 in Step 200\n",
      "Training loss  0.810 in Step 300\n",
      "Training loss  0.822 in Step 400\n",
      "Training loss  0.817 in Step 500\n",
      "Training loss  0.808 in Step 600\n",
      "Training loss  0.795 in Step 700\n",
      "Training loss  0.809 in Step 800\n",
      "Training loss  0.827 in Step 900\n",
      "Training loss  0.828 in Step 1000\n",
      "Training loss  0.839 in Step 1100\n",
      "Training loss  0.832 in Step 1200\n",
      "Training loss  0.828 in Step 1300\n",
      "Training loss  0.817 in Step 1400\n",
      "Training loss  0.803 in Step 1500\n",
      "Training loss  0.832 in Step 1600\n",
      "Training loss  0.837 in Step 1700\n",
      "※※※Training loss  0.822※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.808 in Step 0\n",
      "Valid loss  0.817 in Step 100\n",
      "Valid loss  0.811 in Step 200\n",
      "Valid loss  0.813 in Step 300\n",
      "Valid loss  0.820 in Step 400\n",
      "※※※Valid loss  0.823※※※\n",
      "Epoch 27\n",
      "Training loss  0.825 in Step 0\n",
      "Training loss  0.811 in Step 100\n",
      "Training loss  0.836 in Step 200\n",
      "Training loss  0.816 in Step 300\n",
      "Training loss  0.816 in Step 400\n",
      "Training loss  0.847 in Step 500\n",
      "Training loss  0.818 in Step 600\n",
      "Training loss  0.811 in Step 700\n",
      "Training loss  0.828 in Step 800\n",
      "Training loss  0.820 in Step 900\n",
      "Training loss  0.824 in Step 1000\n",
      "Training loss  0.813 in Step 1100\n",
      "Training loss  0.831 in Step 1200\n",
      "Training loss  0.823 in Step 1300\n",
      "Training loss  0.806 in Step 1400\n",
      "Training loss  0.818 in Step 1500\n",
      "Training loss  0.818 in Step 1600\n",
      "Training loss  0.821 in Step 1700\n",
      "※※※Training loss  0.821※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.811 in Step 0\n",
      "Valid loss  0.819 in Step 100\n",
      "Valid loss  0.809 in Step 200\n",
      "Valid loss  0.814 in Step 300\n",
      "Valid loss  0.819 in Step 400\n",
      "※※※Valid loss  0.822※※※\n",
      "Epoch 28\n",
      "Training loss  0.823 in Step 0\n",
      "Training loss  0.828 in Step 100\n",
      "Training loss  0.809 in Step 200\n",
      "Training loss  0.817 in Step 300\n",
      "Training loss  0.818 in Step 400\n",
      "Training loss  0.816 in Step 500\n",
      "Training loss  0.818 in Step 600\n",
      "Training loss  0.812 in Step 700\n",
      "Training loss  0.840 in Step 800\n",
      "Training loss  0.825 in Step 900\n",
      "Training loss  0.823 in Step 1000\n",
      "Training loss  0.810 in Step 1100\n",
      "Training loss  0.824 in Step 1200\n",
      "Training loss  0.816 in Step 1300\n",
      "Training loss  0.838 in Step 1400\n",
      "Training loss  0.812 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.811 in Step 1700\n",
      "※※※Training loss  0.820※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.809 in Step 0\n",
      "Valid loss  0.820 in Step 100\n",
      "Valid loss  0.811 in Step 200\n",
      "Valid loss  0.814 in Step 300\n",
      "Valid loss  0.821 in Step 400\n",
      "※※※Valid loss  0.823※※※\n",
      "Epoch 29\n",
      "Training loss  0.819 in Step 0\n",
      "Training loss  0.812 in Step 100\n",
      "Training loss  0.807 in Step 200\n",
      "Training loss  0.797 in Step 300\n",
      "Training loss  0.822 in Step 400\n",
      "Training loss  0.812 in Step 500\n",
      "Training loss  0.822 in Step 600\n",
      "Training loss  0.829 in Step 700\n",
      "Training loss  0.825 in Step 800\n",
      "Training loss  0.832 in Step 900\n",
      "Training loss  0.829 in Step 1000\n",
      "Training loss  0.810 in Step 1100\n",
      "Training loss  0.832 in Step 1200\n",
      "Training loss  0.819 in Step 1300\n",
      "Training loss  0.823 in Step 1400\n",
      "Training loss  0.812 in Step 1500\n",
      "Training loss  0.809 in Step 1600\n",
      "Training loss  0.824 in Step 1700\n",
      "※※※Training loss  0.820※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.808 in Step 0\n",
      "Valid loss  0.816 in Step 100\n",
      "Valid loss  0.807 in Step 200\n",
      "Valid loss  0.811 in Step 300\n",
      "Valid loss  0.816 in Step 400\n",
      "※※※Valid loss  0.819※※※\n",
      "Epoch 30\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.828 in Step 100\n",
      "Training loss  0.819 in Step 200\n",
      "Training loss  0.810 in Step 300\n",
      "Training loss  0.821 in Step 400\n",
      "Training loss  0.814 in Step 500\n",
      "Training loss  0.811 in Step 600\n",
      "Training loss  0.810 in Step 700\n",
      "Training loss  0.811 in Step 800\n",
      "Training loss  0.806 in Step 900\n",
      "Training loss  0.820 in Step 1000\n",
      "Training loss  0.827 in Step 1100\n",
      "Training loss  0.802 in Step 1200\n",
      "Training loss  0.824 in Step 1300\n",
      "Training loss  0.830 in Step 1400\n",
      "Training loss  0.808 in Step 1500\n",
      "Training loss  0.814 in Step 1600\n",
      "Training loss  0.836 in Step 1700\n",
      "※※※Training loss  0.819※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.809 in Step 0\n",
      "Valid loss  0.817 in Step 100\n",
      "Valid loss  0.810 in Step 200\n",
      "Valid loss  0.814 in Step 300\n",
      "Valid loss  0.816 in Step 400\n",
      "※※※Valid loss  0.821※※※\n",
      "Epoch 31\n",
      "Training loss  0.830 in Step 0\n",
      "Training loss  0.817 in Step 100\n",
      "Training loss  0.815 in Step 200\n",
      "Training loss  0.803 in Step 300\n",
      "Training loss  0.822 in Step 400\n",
      "Training loss  0.822 in Step 500\n",
      "Training loss  0.805 in Step 600\n",
      "Training loss  0.803 in Step 700\n",
      "Training loss  0.810 in Step 800\n",
      "Training loss  0.813 in Step 900\n",
      "Training loss  0.820 in Step 1000\n",
      "Training loss  0.810 in Step 1100\n",
      "Training loss  0.809 in Step 1200\n",
      "Training loss  0.819 in Step 1300\n",
      "Training loss  0.830 in Step 1400\n",
      "Training loss  0.810 in Step 1500\n",
      "Training loss  0.830 in Step 1600\n",
      "Training loss  0.802 in Step 1700\n",
      "※※※Training loss  0.819※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.804 in Step 0\n",
      "Valid loss  0.818 in Step 100\n",
      "Valid loss  0.807 in Step 200\n",
      "Valid loss  0.812 in Step 300\n",
      "Valid loss  0.815 in Step 400\n",
      "※※※Valid loss  0.819※※※\n",
      "Epoch 32\n",
      "Training loss  0.833 in Step 0\n",
      "Training loss  0.815 in Step 100\n",
      "Training loss  0.807 in Step 200\n",
      "Training loss  0.817 in Step 300\n",
      "Training loss  0.811 in Step 400\n",
      "Training loss  0.818 in Step 500\n",
      "Training loss  0.812 in Step 600\n",
      "Training loss  0.818 in Step 700\n",
      "Training loss  0.805 in Step 800\n",
      "Training loss  0.839 in Step 900\n",
      "Training loss  0.826 in Step 1000\n",
      "Training loss  0.829 in Step 1100\n",
      "Training loss  0.809 in Step 1200\n",
      "Training loss  0.834 in Step 1300\n",
      "Training loss  0.793 in Step 1400\n",
      "Training loss  0.830 in Step 1500\n",
      "Training loss  0.822 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.817※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.811 in Step 0\n",
      "Valid loss  0.819 in Step 100\n",
      "Valid loss  0.809 in Step 200\n",
      "Valid loss  0.812 in Step 300\n",
      "Valid loss  0.818 in Step 400\n",
      "※※※Valid loss  0.820※※※\n",
      "Epoch 33\n",
      "Training loss  0.822 in Step 0\n",
      "Training loss  0.804 in Step 100\n",
      "Training loss  0.829 in Step 200\n",
      "Training loss  0.823 in Step 300\n",
      "Training loss  0.799 in Step 400\n",
      "Training loss  0.822 in Step 500\n",
      "Training loss  0.813 in Step 600\n",
      "Training loss  0.807 in Step 700\n",
      "Training loss  0.812 in Step 800\n",
      "Training loss  0.812 in Step 900\n",
      "Training loss  0.827 in Step 1000\n",
      "Training loss  0.812 in Step 1100\n",
      "Training loss  0.835 in Step 1200\n",
      "Training loss  0.818 in Step 1300\n",
      "Training loss  0.813 in Step 1400\n",
      "Training loss  0.809 in Step 1500\n",
      "Training loss  0.800 in Step 1600\n",
      "Training loss  0.834 in Step 1700\n",
      "※※※Training loss  0.817※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.810 in Step 0\n",
      "Valid loss  0.813 in Step 100\n",
      "Valid loss  0.805 in Step 200\n",
      "Valid loss  0.809 in Step 300\n",
      "Valid loss  0.814 in Step 400\n",
      "※※※Valid loss  0.818※※※\n",
      "Epoch 34\n",
      "Training loss  0.818 in Step 0\n",
      "Training loss  0.812 in Step 100\n",
      "Training loss  0.820 in Step 200\n",
      "Training loss  0.805 in Step 300\n",
      "Training loss  0.817 in Step 400\n",
      "Training loss  0.828 in Step 500\n",
      "Training loss  0.800 in Step 600\n",
      "Training loss  0.809 in Step 700\n",
      "Training loss  0.830 in Step 800\n",
      "Training loss  0.824 in Step 900\n",
      "Training loss  0.831 in Step 1000\n",
      "Training loss  0.831 in Step 1100\n",
      "Training loss  0.816 in Step 1200\n",
      "Training loss  0.813 in Step 1300\n",
      "Training loss  0.824 in Step 1400\n",
      "Training loss  0.828 in Step 1500\n",
      "Training loss  0.808 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.816※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.806 in Step 0\n",
      "Valid loss  0.812 in Step 100\n",
      "Valid loss  0.804 in Step 200\n",
      "Valid loss  0.809 in Step 300\n",
      "Valid loss  0.813 in Step 400\n",
      "※※※Valid loss  0.818※※※\n",
      "Epoch 35\n",
      "Training loss  0.809 in Step 0\n",
      "Training loss  0.813 in Step 100\n",
      "Training loss  0.826 in Step 200\n",
      "Training loss  0.813 in Step 300\n",
      "Training loss  0.829 in Step 400\n",
      "Training loss  0.806 in Step 500\n",
      "Training loss  0.825 in Step 600\n",
      "Training loss  0.821 in Step 700\n",
      "Training loss  0.813 in Step 800\n",
      "Training loss  0.796 in Step 900\n",
      "Training loss  0.791 in Step 1000\n",
      "Training loss  0.822 in Step 1100\n",
      "Training loss  0.809 in Step 1200\n",
      "Training loss  0.819 in Step 1300\n",
      "Training loss  0.791 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.820 in Step 1600\n",
      "Training loss  0.820 in Step 1700\n",
      "※※※Training loss  0.816※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.803 in Step 0\n",
      "Valid loss  0.815 in Step 100\n",
      "Valid loss  0.804 in Step 200\n",
      "Valid loss  0.808 in Step 300\n",
      "Valid loss  0.812 in Step 400\n",
      "※※※Valid loss  0.816※※※\n",
      "Epoch 36\n",
      "Training loss  0.828 in Step 0\n",
      "Training loss  0.810 in Step 100\n",
      "Training loss  0.811 in Step 200\n",
      "Training loss  0.822 in Step 300\n",
      "Training loss  0.810 in Step 400\n",
      "Training loss  0.812 in Step 500\n",
      "Training loss  0.818 in Step 600\n",
      "Training loss  0.819 in Step 700\n",
      "Training loss  0.821 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.809 in Step 1000\n",
      "Training loss  0.821 in Step 1100\n",
      "Training loss  0.812 in Step 1200\n",
      "Training loss  0.838 in Step 1300\n",
      "Training loss  0.813 in Step 1400\n",
      "Training loss  0.818 in Step 1500\n",
      "Training loss  0.801 in Step 1600\n",
      "Training loss  0.818 in Step 1700\n",
      "※※※Training loss  0.815※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.811 in Step 0\n",
      "Valid loss  0.818 in Step 100\n",
      "Valid loss  0.810 in Step 200\n",
      "Valid loss  0.813 in Step 300\n",
      "Valid loss  0.819 in Step 400\n",
      "※※※Valid loss  0.823※※※\n",
      "Epoch 37\n",
      "Training loss  0.793 in Step 0\n",
      "Training loss  0.835 in Step 100\n",
      "Training loss  0.810 in Step 200\n",
      "Training loss  0.840 in Step 300\n",
      "Training loss  0.797 in Step 400\n",
      "Training loss  0.817 in Step 500\n",
      "Training loss  0.805 in Step 600\n",
      "Training loss  0.827 in Step 700\n",
      "Training loss  0.798 in Step 800\n",
      "Training loss  0.817 in Step 900\n",
      "Training loss  0.824 in Step 1000\n",
      "Training loss  0.817 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.792 in Step 1300\n",
      "Training loss  0.811 in Step 1400\n",
      "Training loss  0.823 in Step 1500\n",
      "Training loss  0.833 in Step 1600\n",
      "Training loss  0.828 in Step 1700\n",
      "※※※Training loss  0.815※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.805 in Step 0\n",
      "Valid loss  0.814 in Step 100\n",
      "Valid loss  0.805 in Step 200\n",
      "Valid loss  0.809 in Step 300\n",
      "Valid loss  0.814 in Step 400\n",
      "※※※Valid loss  0.818※※※\n",
      "Epoch 38\n",
      "Training loss  0.819 in Step 0\n",
      "Training loss  0.815 in Step 100\n",
      "Training loss  0.818 in Step 200\n",
      "Training loss  0.817 in Step 300\n",
      "Training loss  0.808 in Step 400\n",
      "Training loss  0.796 in Step 500\n",
      "Training loss  0.810 in Step 600\n",
      "Training loss  0.813 in Step 700\n",
      "Training loss  0.822 in Step 800\n",
      "Training loss  0.803 in Step 900\n",
      "Training loss  0.808 in Step 1000\n",
      "Training loss  0.815 in Step 1100\n",
      "Training loss  0.807 in Step 1200\n",
      "Training loss  0.792 in Step 1300\n",
      "Training loss  0.831 in Step 1400\n",
      "Training loss  0.819 in Step 1500\n",
      "Training loss  0.835 in Step 1600\n",
      "Training loss  0.816 in Step 1700\n",
      "※※※Training loss  0.814※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.810 in Step 0\n",
      "Valid loss  0.813 in Step 100\n",
      "Valid loss  0.808 in Step 200\n",
      "Valid loss  0.811 in Step 300\n",
      "Valid loss  0.817 in Step 400\n",
      "※※※Valid loss  0.819※※※\n",
      "Epoch 39\n",
      "Training loss  0.806 in Step 0\n",
      "Training loss  0.794 in Step 100\n",
      "Training loss  0.821 in Step 200\n",
      "Training loss  0.812 in Step 300\n",
      "Training loss  0.836 in Step 400\n",
      "Training loss  0.808 in Step 500\n",
      "Training loss  0.813 in Step 600\n",
      "Training loss  0.804 in Step 700\n",
      "Training loss  0.824 in Step 800\n",
      "Training loss  0.823 in Step 900\n",
      "Training loss  0.824 in Step 1000\n",
      "Training loss  0.809 in Step 1100\n",
      "Training loss  0.804 in Step 1200\n",
      "Training loss  0.823 in Step 1300\n",
      "Training loss  0.807 in Step 1400\n",
      "Training loss  0.795 in Step 1500\n",
      "Training loss  0.815 in Step 1600\n",
      "Training loss  0.820 in Step 1700\n",
      "※※※Training loss  0.814※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.802 in Step 0\n",
      "Valid loss  0.810 in Step 100\n",
      "Valid loss  0.802 in Step 200\n",
      "Valid loss  0.806 in Step 300\n",
      "Valid loss  0.811 in Step 400\n",
      "※※※Valid loss  0.814※※※\n",
      "Epoch 40\n",
      "Training loss  0.820 in Step 0\n",
      "Training loss  0.825 in Step 100\n",
      "Training loss  0.801 in Step 200\n",
      "Training loss  0.817 in Step 300\n",
      "Training loss  0.815 in Step 400\n",
      "Training loss  0.814 in Step 500\n",
      "Training loss  0.828 in Step 600\n",
      "Training loss  0.835 in Step 700\n",
      "Training loss  0.811 in Step 800\n",
      "Training loss  0.813 in Step 900\n",
      "Training loss  0.818 in Step 1000\n",
      "Training loss  0.813 in Step 1100\n",
      "Training loss  0.808 in Step 1200\n",
      "Training loss  0.808 in Step 1300\n",
      "Training loss  0.800 in Step 1400\n",
      "Training loss  0.811 in Step 1500\n",
      "Training loss  0.805 in Step 1600\n",
      "Training loss  0.805 in Step 1700\n",
      "※※※Training loss  0.814※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.799 in Step 0\n",
      "Valid loss  0.812 in Step 100\n",
      "Valid loss  0.803 in Step 200\n",
      "Valid loss  0.805 in Step 300\n",
      "Valid loss  0.813 in Step 400\n",
      "※※※Valid loss  0.814※※※\n",
      "Epoch 41\n",
      "Training loss  0.799 in Step 0\n",
      "Training loss  0.820 in Step 100\n",
      "Training loss  0.815 in Step 200\n",
      "Training loss  0.822 in Step 300\n",
      "Training loss  0.814 in Step 400\n",
      "Training loss  0.806 in Step 500\n",
      "Training loss  0.811 in Step 600\n",
      "Training loss  0.811 in Step 700\n",
      "Training loss  0.805 in Step 800\n",
      "Training loss  0.811 in Step 900\n",
      "Training loss  0.830 in Step 1000\n",
      "Training loss  0.813 in Step 1100\n",
      "Training loss  0.833 in Step 1200\n",
      "Training loss  0.829 in Step 1300\n",
      "Training loss  0.827 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.814 in Step 1600\n",
      "Training loss  0.802 in Step 1700\n",
      "※※※Training loss  0.812※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.802 in Step 0\n",
      "Valid loss  0.810 in Step 100\n",
      "Valid loss  0.803 in Step 200\n",
      "Valid loss  0.805 in Step 300\n",
      "Valid loss  0.810 in Step 400\n",
      "※※※Valid loss  0.813※※※\n",
      "Epoch 42\n",
      "Training loss  0.829 in Step 0\n",
      "Training loss  0.825 in Step 100\n",
      "Training loss  0.796 in Step 200\n",
      "Training loss  0.816 in Step 300\n",
      "Training loss  0.809 in Step 400\n",
      "Training loss  0.805 in Step 500\n",
      "Training loss  0.823 in Step 600\n",
      "Training loss  0.796 in Step 700\n",
      "Training loss  0.802 in Step 800\n",
      "Training loss  0.803 in Step 900\n",
      "Training loss  0.816 in Step 1000\n",
      "Training loss  0.822 in Step 1100\n",
      "Training loss  0.816 in Step 1200\n",
      "Training loss  0.805 in Step 1300\n",
      "Training loss  0.805 in Step 1400\n",
      "Training loss  0.821 in Step 1500\n",
      "Training loss  0.805 in Step 1600\n",
      "Training loss  0.815 in Step 1700\n",
      "※※※Training loss  0.812※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.802 in Step 0\n",
      "Valid loss  0.818 in Step 100\n",
      "Valid loss  0.805 in Step 200\n",
      "Valid loss  0.806 in Step 300\n",
      "Valid loss  0.813 in Step 400\n",
      "※※※Valid loss  0.814※※※\n",
      "Epoch 43\n",
      "Training loss  0.820 in Step 0\n",
      "Training loss  0.791 in Step 100\n",
      "Training loss  0.801 in Step 200\n",
      "Training loss  0.813 in Step 300\n",
      "Training loss  0.809 in Step 400\n",
      "Training loss  0.811 in Step 500\n",
      "Training loss  0.809 in Step 600\n",
      "Training loss  0.823 in Step 700\n",
      "Training loss  0.793 in Step 800\n",
      "Training loss  0.813 in Step 900\n",
      "Training loss  0.798 in Step 1000\n",
      "Training loss  0.788 in Step 1100\n",
      "Training loss  0.800 in Step 1200\n",
      "Training loss  0.804 in Step 1300\n",
      "Training loss  0.810 in Step 1400\n",
      "Training loss  0.809 in Step 1500\n",
      "Training loss  0.804 in Step 1600\n",
      "Training loss  0.806 in Step 1700\n",
      "※※※Training loss  0.811※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.800 in Step 0\n",
      "Valid loss  0.809 in Step 100\n",
      "Valid loss  0.800 in Step 200\n",
      "Valid loss  0.803 in Step 300\n",
      "Valid loss  0.809 in Step 400\n",
      "※※※Valid loss  0.812※※※\n",
      "Epoch 44\n",
      "Training loss  0.814 in Step 0\n",
      "Training loss  0.822 in Step 100\n",
      "Training loss  0.822 in Step 200\n",
      "Training loss  0.808 in Step 300\n",
      "Training loss  0.804 in Step 400\n",
      "Training loss  0.815 in Step 500\n",
      "Training loss  0.812 in Step 600\n",
      "Training loss  0.811 in Step 700\n",
      "Training loss  0.813 in Step 800\n",
      "Training loss  0.806 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.809 in Step 1100\n",
      "Training loss  0.825 in Step 1200\n",
      "Training loss  0.822 in Step 1300\n",
      "Training loss  0.822 in Step 1400\n",
      "Training loss  0.799 in Step 1500\n",
      "Training loss  0.817 in Step 1600\n",
      "Training loss  0.817 in Step 1700\n",
      "※※※Training loss  0.811※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.800 in Step 0\n",
      "Valid loss  0.810 in Step 100\n",
      "Valid loss  0.804 in Step 200\n",
      "Valid loss  0.807 in Step 300\n",
      "Valid loss  0.809 in Step 400\n",
      "※※※Valid loss  0.813※※※\n",
      "Epoch 45\n",
      "Training loss  0.828 in Step 0\n",
      "Training loss  0.806 in Step 100\n",
      "Training loss  0.799 in Step 200\n",
      "Training loss  0.820 in Step 300\n",
      "Training loss  0.810 in Step 400\n",
      "Training loss  0.815 in Step 500\n",
      "Training loss  0.811 in Step 600\n",
      "Training loss  0.791 in Step 700\n",
      "Training loss  0.794 in Step 800\n",
      "Training loss  0.815 in Step 900\n",
      "Training loss  0.819 in Step 1000\n",
      "Training loss  0.821 in Step 1100\n",
      "Training loss  0.801 in Step 1200\n",
      "Training loss  0.807 in Step 1300\n",
      "Training loss  0.806 in Step 1400\n",
      "Training loss  0.821 in Step 1500\n",
      "Training loss  0.802 in Step 1600\n",
      "Training loss  0.795 in Step 1700\n",
      "※※※Training loss  0.811※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.801 in Step 0\n",
      "Valid loss  0.806 in Step 100\n",
      "Valid loss  0.799 in Step 200\n",
      "Valid loss  0.803 in Step 300\n",
      "Valid loss  0.810 in Step 400\n",
      "※※※Valid loss  0.811※※※\n",
      "Epoch 46\n",
      "Training loss  0.811 in Step 0\n",
      "Training loss  0.799 in Step 100\n",
      "Training loss  0.802 in Step 200\n",
      "Training loss  0.810 in Step 300\n",
      "Training loss  0.816 in Step 400\n",
      "Training loss  0.811 in Step 500\n",
      "Training loss  0.806 in Step 600\n",
      "Training loss  0.805 in Step 700\n",
      "Training loss  0.808 in Step 800\n",
      "Training loss  0.810 in Step 900\n",
      "Training loss  0.823 in Step 1000\n",
      "Training loss  0.816 in Step 1100\n",
      "Training loss  0.799 in Step 1200\n",
      "Training loss  0.800 in Step 1300\n",
      "Training loss  0.795 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.793 in Step 1600\n",
      "Training loss  0.787 in Step 1700\n",
      "※※※Training loss  0.809※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.795 in Step 0\n",
      "Valid loss  0.808 in Step 100\n",
      "Valid loss  0.801 in Step 200\n",
      "Valid loss  0.803 in Step 300\n",
      "Valid loss  0.808 in Step 400\n",
      "※※※Valid loss  0.810※※※\n",
      "Epoch 47\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.786 in Step 200\n",
      "Training loss  0.814 in Step 300\n",
      "Training loss  0.797 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.806 in Step 600\n",
      "Training loss  0.796 in Step 700\n",
      "Training loss  0.792 in Step 800\n",
      "Training loss  0.819 in Step 900\n",
      "Training loss  0.813 in Step 1000\n",
      "Training loss  0.800 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.807 in Step 1300\n",
      "Training loss  0.801 in Step 1400\n",
      "Training loss  0.808 in Step 1500\n",
      "Training loss  0.808 in Step 1600\n",
      "Training loss  0.815 in Step 1700\n",
      "※※※Training loss  0.809※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.794 in Step 0\n",
      "Valid loss  0.807 in Step 100\n",
      "Valid loss  0.797 in Step 200\n",
      "Valid loss  0.800 in Step 300\n",
      "Valid loss  0.808 in Step 400\n",
      "※※※Valid loss  0.809※※※\n",
      "Epoch 48\n",
      "Training loss  0.810 in Step 0\n",
      "Training loss  0.800 in Step 100\n",
      "Training loss  0.797 in Step 200\n",
      "Training loss  0.806 in Step 300\n",
      "Training loss  0.818 in Step 400\n",
      "Training loss  0.817 in Step 500\n",
      "Training loss  0.805 in Step 600\n",
      "Training loss  0.814 in Step 700\n",
      "Training loss  0.808 in Step 800\n",
      "Training loss  0.825 in Step 900\n",
      "Training loss  0.819 in Step 1000\n",
      "Training loss  0.805 in Step 1100\n",
      "Training loss  0.810 in Step 1200\n",
      "Training loss  0.818 in Step 1300\n",
      "Training loss  0.816 in Step 1400\n",
      "Training loss  0.790 in Step 1500\n",
      "Training loss  0.810 in Step 1600\n",
      "Training loss  0.816 in Step 1700\n",
      "※※※Training loss  0.808※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.797 in Step 0\n",
      "Valid loss  0.806 in Step 100\n",
      "Valid loss  0.797 in Step 200\n",
      "Valid loss  0.801 in Step 300\n",
      "Valid loss  0.809 in Step 400\n",
      "※※※Valid loss  0.808※※※\n",
      "Epoch 49\n",
      "Training loss  0.804 in Step 0\n",
      "Training loss  0.811 in Step 100\n",
      "Training loss  0.814 in Step 200\n",
      "Training loss  0.792 in Step 300\n",
      "Training loss  0.813 in Step 400\n",
      "Training loss  0.814 in Step 500\n",
      "Training loss  0.805 in Step 600\n",
      "Training loss  0.812 in Step 700\n",
      "Training loss  0.817 in Step 800\n",
      "Training loss  0.803 in Step 900\n",
      "Training loss  0.819 in Step 1000\n",
      "Training loss  0.803 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.798 in Step 1300\n",
      "Training loss  0.814 in Step 1400\n",
      "Training loss  0.807 in Step 1500\n",
      "Training loss  0.807 in Step 1600\n",
      "Training loss  0.804 in Step 1700\n",
      "※※※Training loss  0.808※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.796 in Step 0\n",
      "Valid loss  0.807 in Step 100\n",
      "Valid loss  0.800 in Step 200\n",
      "Valid loss  0.804 in Step 300\n",
      "Valid loss  0.810 in Step 400\n",
      "※※※Valid loss  0.811※※※\n",
      "Epoch 50\n",
      "Training loss  0.803 in Step 0\n",
      "Training loss  0.808 in Step 100\n",
      "Training loss  0.813 in Step 200\n",
      "Training loss  0.812 in Step 300\n",
      "Training loss  0.812 in Step 400\n",
      "Training loss  0.845 in Step 500\n",
      "Training loss  0.817 in Step 600\n",
      "Training loss  0.818 in Step 700\n",
      "Training loss  0.812 in Step 800\n",
      "Training loss  0.809 in Step 900\n",
      "Training loss  0.809 in Step 1000\n",
      "Training loss  0.810 in Step 1100\n",
      "Training loss  0.810 in Step 1200\n",
      "Training loss  0.809 in Step 1300\n",
      "Training loss  0.821 in Step 1400\n",
      "Training loss  0.808 in Step 1500\n",
      "Training loss  0.814 in Step 1600\n",
      "Training loss  0.797 in Step 1700\n",
      "※※※Training loss  0.808※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.797 in Step 0\n",
      "Valid loss  0.805 in Step 100\n",
      "Valid loss  0.797 in Step 200\n",
      "Valid loss  0.800 in Step 300\n",
      "Valid loss  0.807 in Step 400\n",
      "※※※Valid loss  0.809※※※\n",
      "Epoch 51\n",
      "Training loss  0.820 in Step 0\n",
      "Training loss  0.802 in Step 100\n",
      "Training loss  0.793 in Step 200\n",
      "Training loss  0.814 in Step 300\n",
      "Training loss  0.815 in Step 400\n",
      "Training loss  0.788 in Step 500\n",
      "Training loss  0.788 in Step 600\n",
      "Training loss  0.809 in Step 700\n",
      "Training loss  0.817 in Step 800\n",
      "Training loss  0.808 in Step 900\n",
      "Training loss  0.787 in Step 1000\n",
      "Training loss  0.810 in Step 1100\n",
      "Training loss  0.793 in Step 1200\n",
      "Training loss  0.824 in Step 1300\n",
      "Training loss  0.819 in Step 1400\n",
      "Training loss  0.815 in Step 1500\n",
      "Training loss  0.816 in Step 1600\n",
      "Training loss  0.815 in Step 1700\n",
      "※※※Training loss  0.807※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.798 in Step 0\n",
      "Valid loss  0.803 in Step 100\n",
      "Valid loss  0.796 in Step 200\n",
      "Valid loss  0.799 in Step 300\n",
      "Valid loss  0.806 in Step 400\n",
      "※※※Valid loss  0.808※※※\n",
      "Epoch 52\n",
      "Training loss  0.826 in Step 0\n",
      "Training loss  0.809 in Step 100\n",
      "Training loss  0.794 in Step 200\n",
      "Training loss  0.801 in Step 300\n",
      "Training loss  0.831 in Step 400\n",
      "Training loss  0.812 in Step 500\n",
      "Training loss  0.819 in Step 600\n",
      "Training loss  0.798 in Step 700\n",
      "Training loss  0.820 in Step 800\n",
      "Training loss  0.814 in Step 900\n",
      "Training loss  0.790 in Step 1000\n",
      "Training loss  0.814 in Step 1100\n",
      "Training loss  0.807 in Step 1200\n",
      "Training loss  0.809 in Step 1300\n",
      "Training loss  0.819 in Step 1400\n",
      "Training loss  0.808 in Step 1500\n",
      "Training loss  0.805 in Step 1600\n",
      "Training loss  0.802 in Step 1700\n",
      "※※※Training loss  0.807※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.801 in Step 0\n",
      "Valid loss  0.806 in Step 100\n",
      "Valid loss  0.798 in Step 200\n",
      "Valid loss  0.799 in Step 300\n",
      "Valid loss  0.808 in Step 400\n",
      "※※※Valid loss  0.808※※※\n",
      "Epoch 53\n",
      "Training loss  0.808 in Step 0\n",
      "Training loss  0.816 in Step 100\n",
      "Training loss  0.816 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.807 in Step 600\n",
      "Training loss  0.795 in Step 700\n",
      "Training loss  0.809 in Step 800\n",
      "Training loss  0.811 in Step 900\n",
      "Training loss  0.803 in Step 1000\n",
      "Training loss  0.823 in Step 1100\n",
      "Training loss  0.813 in Step 1200\n",
      "Training loss  0.799 in Step 1300\n",
      "Training loss  0.810 in Step 1400\n",
      "Training loss  0.818 in Step 1500\n",
      "Training loss  0.798 in Step 1600\n",
      "Training loss  0.813 in Step 1700\n",
      "※※※Training loss  0.806※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.795 in Step 0\n",
      "Valid loss  0.803 in Step 100\n",
      "Valid loss  0.796 in Step 200\n",
      "Valid loss  0.797 in Step 300\n",
      "Valid loss  0.806 in Step 400\n",
      "※※※Valid loss  0.806※※※\n",
      "Epoch 54\n",
      "Training loss  0.784 in Step 0\n",
      "Training loss  0.793 in Step 100\n",
      "Training loss  0.803 in Step 200\n",
      "Training loss  0.813 in Step 300\n",
      "Training loss  0.811 in Step 400\n",
      "Training loss  0.809 in Step 500\n",
      "Training loss  0.802 in Step 600\n",
      "Training loss  0.806 in Step 700\n",
      "Training loss  0.820 in Step 800\n",
      "Training loss  0.823 in Step 900\n",
      "Training loss  0.816 in Step 1000\n",
      "Training loss  0.810 in Step 1100\n",
      "Training loss  0.808 in Step 1200\n",
      "Training loss  0.812 in Step 1300\n",
      "Training loss  0.810 in Step 1400\n",
      "Training loss  0.773 in Step 1500\n",
      "Training loss  0.801 in Step 1600\n",
      "Training loss  0.798 in Step 1700\n",
      "※※※Training loss  0.806※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.795 in Step 0\n",
      "Valid loss  0.803 in Step 100\n",
      "Valid loss  0.793 in Step 200\n",
      "Valid loss  0.799 in Step 300\n",
      "Valid loss  0.805 in Step 400\n",
      "※※※Valid loss  0.807※※※\n",
      "Epoch 55\n",
      "Training loss  0.810 in Step 0\n",
      "Training loss  0.815 in Step 100\n",
      "Training loss  0.823 in Step 200\n",
      "Training loss  0.803 in Step 300\n",
      "Training loss  0.818 in Step 400\n",
      "Training loss  0.810 in Step 500\n",
      "Training loss  0.817 in Step 600\n",
      "Training loss  0.820 in Step 700\n",
      "Training loss  0.797 in Step 800\n",
      "Training loss  0.820 in Step 900\n",
      "Training loss  0.821 in Step 1000\n",
      "Training loss  0.805 in Step 1100\n",
      "Training loss  0.808 in Step 1200\n",
      "Training loss  0.806 in Step 1300\n",
      "Training loss  0.803 in Step 1400\n",
      "Training loss  0.803 in Step 1500\n",
      "Training loss  0.799 in Step 1600\n",
      "Training loss  0.794 in Step 1700\n",
      "※※※Training loss  0.805※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.797 in Step 0\n",
      "Valid loss  0.803 in Step 100\n",
      "Valid loss  0.796 in Step 200\n",
      "Valid loss  0.798 in Step 300\n",
      "Valid loss  0.806 in Step 400\n",
      "※※※Valid loss  0.808※※※\n",
      "Epoch 56\n",
      "Training loss  0.818 in Step 0\n",
      "Training loss  0.802 in Step 100\n",
      "Training loss  0.819 in Step 200\n",
      "Training loss  0.781 in Step 300\n",
      "Training loss  0.809 in Step 400\n",
      "Training loss  0.815 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.811 in Step 700\n",
      "Training loss  0.806 in Step 800\n",
      "Training loss  0.829 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.809 in Step 1100\n",
      "Training loss  0.797 in Step 1200\n",
      "Training loss  0.820 in Step 1300\n",
      "Training loss  0.797 in Step 1400\n",
      "Training loss  0.814 in Step 1500\n",
      "Training loss  0.798 in Step 1600\n",
      "Training loss  0.807 in Step 1700\n",
      "※※※Training loss  0.805※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.795 in Step 0\n",
      "Valid loss  0.802 in Step 100\n",
      "Valid loss  0.794 in Step 200\n",
      "Valid loss  0.797 in Step 300\n",
      "Valid loss  0.803 in Step 400\n",
      "※※※Valid loss  0.806※※※\n",
      "Epoch 57\n",
      "Training loss  0.822 in Step 0\n",
      "Training loss  0.810 in Step 100\n",
      "Training loss  0.798 in Step 200\n",
      "Training loss  0.812 in Step 300\n",
      "Training loss  0.804 in Step 400\n",
      "Training loss  0.795 in Step 500\n",
      "Training loss  0.815 in Step 600\n",
      "Training loss  0.808 in Step 700\n",
      "Training loss  0.797 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.798 in Step 1000\n",
      "Training loss  0.792 in Step 1100\n",
      "Training loss  0.802 in Step 1200\n",
      "Training loss  0.805 in Step 1300\n",
      "Training loss  0.807 in Step 1400\n",
      "Training loss  0.801 in Step 1500\n",
      "Training loss  0.814 in Step 1600\n",
      "Training loss  0.816 in Step 1700\n",
      "※※※Training loss  0.805※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.797 in Step 0\n",
      "Valid loss  0.800 in Step 100\n",
      "Valid loss  0.794 in Step 200\n",
      "Valid loss  0.798 in Step 300\n",
      "Valid loss  0.804 in Step 400\n",
      "※※※Valid loss  0.807※※※\n",
      "Epoch 58\n",
      "Training loss  0.809 in Step 0\n",
      "Training loss  0.812 in Step 100\n",
      "Training loss  0.799 in Step 200\n",
      "Training loss  0.795 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.799 in Step 500\n",
      "Training loss  0.791 in Step 600\n",
      "Training loss  0.803 in Step 700\n",
      "Training loss  0.807 in Step 800\n",
      "Training loss  0.791 in Step 900\n",
      "Training loss  0.807 in Step 1000\n",
      "Training loss  0.821 in Step 1100\n",
      "Training loss  0.791 in Step 1200\n",
      "Training loss  0.787 in Step 1300\n",
      "Training loss  0.801 in Step 1400\n",
      "Training loss  0.796 in Step 1500\n",
      "Training loss  0.790 in Step 1600\n",
      "Training loss  0.799 in Step 1700\n",
      "※※※Training loss  0.804※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.794 in Step 0\n",
      "Valid loss  0.801 in Step 100\n",
      "Valid loss  0.792 in Step 200\n",
      "Valid loss  0.797 in Step 300\n",
      "Valid loss  0.803 in Step 400\n",
      "※※※Valid loss  0.806※※※\n",
      "Epoch 59\n",
      "Training loss  0.817 in Step 0\n",
      "Training loss  0.782 in Step 100\n",
      "Training loss  0.807 in Step 200\n",
      "Training loss  0.808 in Step 300\n",
      "Training loss  0.799 in Step 400\n",
      "Training loss  0.785 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.799 in Step 700\n",
      "Training loss  0.797 in Step 800\n",
      "Training loss  0.811 in Step 900\n",
      "Training loss  0.798 in Step 1000\n",
      "Training loss  0.787 in Step 1100\n",
      "Training loss  0.805 in Step 1200\n",
      "Training loss  0.808 in Step 1300\n",
      "Training loss  0.802 in Step 1400\n",
      "Training loss  0.813 in Step 1500\n",
      "Training loss  0.811 in Step 1600\n",
      "Training loss  0.793 in Step 1700\n",
      "※※※Training loss  0.803※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.795 in Step 0\n",
      "Valid loss  0.803 in Step 100\n",
      "Valid loss  0.793 in Step 200\n",
      "Valid loss  0.797 in Step 300\n",
      "Valid loss  0.802 in Step 400\n",
      "※※※Valid loss  0.805※※※\n",
      "Epoch 60\n",
      "Training loss  0.799 in Step 0\n",
      "Training loss  0.807 in Step 100\n",
      "Training loss  0.803 in Step 200\n",
      "Training loss  0.818 in Step 300\n",
      "Training loss  0.814 in Step 400\n",
      "Training loss  0.799 in Step 500\n",
      "Training loss  0.815 in Step 600\n",
      "Training loss  0.806 in Step 700\n",
      "Training loss  0.794 in Step 800\n",
      "Training loss  0.811 in Step 900\n",
      "Training loss  0.812 in Step 1000\n",
      "Training loss  0.811 in Step 1100\n",
      "Training loss  0.796 in Step 1200\n",
      "Training loss  0.794 in Step 1300\n",
      "Training loss  0.796 in Step 1400\n",
      "Training loss  0.813 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.803※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.797 in Step 0\n",
      "Valid loss  0.803 in Step 100\n",
      "Valid loss  0.792 in Step 200\n",
      "Valid loss  0.797 in Step 300\n",
      "Valid loss  0.801 in Step 400\n",
      "※※※Valid loss  0.805※※※\n",
      "Epoch 61\n",
      "Training loss  0.830 in Step 0\n",
      "Training loss  0.789 in Step 100\n",
      "Training loss  0.815 in Step 200\n",
      "Training loss  0.798 in Step 300\n",
      "Training loss  0.817 in Step 400\n",
      "Training loss  0.814 in Step 500\n",
      "Training loss  0.812 in Step 600\n",
      "Training loss  0.801 in Step 700\n",
      "Training loss  0.795 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.795 in Step 1000\n",
      "Training loss  0.800 in Step 1100\n",
      "Training loss  0.800 in Step 1200\n",
      "Training loss  0.803 in Step 1300\n",
      "Training loss  0.810 in Step 1400\n",
      "Training loss  0.799 in Step 1500\n",
      "Training loss  0.794 in Step 1600\n",
      "Training loss  0.796 in Step 1700\n",
      "※※※Training loss  0.803※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.792 in Step 0\n",
      "Valid loss  0.801 in Step 100\n",
      "Valid loss  0.792 in Step 200\n",
      "Valid loss  0.796 in Step 300\n",
      "Valid loss  0.800 in Step 400\n",
      "※※※Valid loss  0.804※※※\n",
      "Epoch 62\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.802 in Step 200\n",
      "Training loss  0.796 in Step 300\n",
      "Training loss  0.790 in Step 400\n",
      "Training loss  0.804 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.810 in Step 700\n",
      "Training loss  0.805 in Step 800\n",
      "Training loss  0.806 in Step 900\n",
      "Training loss  0.814 in Step 1000\n",
      "Training loss  0.789 in Step 1100\n",
      "Training loss  0.803 in Step 1200\n",
      "Training loss  0.805 in Step 1300\n",
      "Training loss  0.785 in Step 1400\n",
      "Training loss  0.807 in Step 1500\n",
      "Training loss  0.801 in Step 1600\n",
      "Training loss  0.789 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.793 in Step 0\n",
      "Valid loss  0.800 in Step 100\n",
      "Valid loss  0.793 in Step 200\n",
      "Valid loss  0.796 in Step 300\n",
      "Valid loss  0.800 in Step 400\n",
      "※※※Valid loss  0.804※※※\n",
      "Epoch 63\n",
      "Training loss  0.804 in Step 0\n",
      "Training loss  0.810 in Step 100\n",
      "Training loss  0.787 in Step 200\n",
      "Training loss  0.799 in Step 300\n",
      "Training loss  0.802 in Step 400\n",
      "Training loss  0.793 in Step 500\n",
      "Training loss  0.792 in Step 600\n",
      "Training loss  0.792 in Step 700\n",
      "Training loss  0.791 in Step 800\n",
      "Training loss  0.794 in Step 900\n",
      "Training loss  0.800 in Step 1000\n",
      "Training loss  0.803 in Step 1100\n",
      "Training loss  0.797 in Step 1200\n",
      "Training loss  0.804 in Step 1300\n",
      "Training loss  0.803 in Step 1400\n",
      "Training loss  0.803 in Step 1500\n",
      "Training loss  0.780 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.790 in Step 0\n",
      "Valid loss  0.799 in Step 100\n",
      "Valid loss  0.794 in Step 200\n",
      "Valid loss  0.797 in Step 300\n",
      "Valid loss  0.800 in Step 400\n",
      "※※※Valid loss  0.804※※※\n",
      "Epoch 64\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.794 in Step 100\n",
      "Training loss  0.797 in Step 200\n",
      "Training loss  0.791 in Step 300\n",
      "Training loss  0.784 in Step 400\n",
      "Training loss  0.810 in Step 500\n",
      "Training loss  0.806 in Step 600\n",
      "Training loss  0.791 in Step 700\n",
      "Training loss  0.826 in Step 800\n",
      "Training loss  0.791 in Step 900\n",
      "Training loss  0.795 in Step 1000\n",
      "Training loss  0.778 in Step 1100\n",
      "Training loss  0.789 in Step 1200\n",
      "Training loss  0.815 in Step 1300\n",
      "Training loss  0.811 in Step 1400\n",
      "Training loss  0.787 in Step 1500\n",
      "Training loss  0.831 in Step 1600\n",
      "Training loss  0.807 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.793 in Step 0\n",
      "Valid loss  0.798 in Step 100\n",
      "Valid loss  0.792 in Step 200\n",
      "Valid loss  0.795 in Step 300\n",
      "Valid loss  0.798 in Step 400\n",
      "※※※Valid loss  0.804※※※\n",
      "Epoch 65\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.782 in Step 100\n",
      "Training loss  0.818 in Step 200\n",
      "Training loss  0.800 in Step 300\n",
      "Training loss  0.792 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.802 in Step 600\n",
      "Training loss  0.796 in Step 700\n",
      "Training loss  0.783 in Step 800\n",
      "Training loss  0.810 in Step 900\n",
      "Training loss  0.801 in Step 1000\n",
      "Training loss  0.802 in Step 1100\n",
      "Training loss  0.808 in Step 1200\n",
      "Training loss  0.814 in Step 1300\n",
      "Training loss  0.809 in Step 1400\n",
      "Training loss  0.785 in Step 1500\n",
      "Training loss  0.783 in Step 1600\n",
      "Training loss  0.810 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.790 in Step 0\n",
      "Valid loss  0.795 in Step 100\n",
      "Valid loss  0.790 in Step 200\n",
      "Valid loss  0.794 in Step 300\n",
      "Valid loss  0.799 in Step 400\n",
      "※※※Valid loss  0.802※※※\n",
      "Epoch 66\n",
      "Training loss  0.781 in Step 0\n",
      "Training loss  0.792 in Step 100\n",
      "Training loss  0.819 in Step 200\n",
      "Training loss  0.788 in Step 300\n",
      "Training loss  0.793 in Step 400\n",
      "Training loss  0.799 in Step 500\n",
      "Training loss  0.810 in Step 600\n",
      "Training loss  0.805 in Step 700\n",
      "Training loss  0.784 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.794 in Step 1000\n",
      "Training loss  0.806 in Step 1100\n",
      "Training loss  0.818 in Step 1200\n",
      "Training loss  0.803 in Step 1300\n",
      "Training loss  0.814 in Step 1400\n",
      "Training loss  0.813 in Step 1500\n",
      "Training loss  0.795 in Step 1600\n",
      "Training loss  0.810 in Step 1700\n",
      "※※※Training loss  0.802※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.791 in Step 0\n",
      "Valid loss  0.796 in Step 100\n",
      "Valid loss  0.792 in Step 200\n",
      "Valid loss  0.794 in Step 300\n",
      "Valid loss  0.799 in Step 400\n",
      "※※※Valid loss  0.802※※※\n",
      "Epoch 67\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.808 in Step 100\n",
      "Training loss  0.798 in Step 200\n",
      "Training loss  0.797 in Step 300\n",
      "Training loss  0.802 in Step 400\n",
      "Training loss  0.807 in Step 500\n",
      "Training loss  0.802 in Step 600\n",
      "Training loss  0.799 in Step 700\n",
      "Training loss  0.785 in Step 800\n",
      "Training loss  0.813 in Step 900\n",
      "Training loss  0.812 in Step 1000\n",
      "Training loss  0.802 in Step 1100\n",
      "Training loss  0.808 in Step 1200\n",
      "Training loss  0.812 in Step 1300\n",
      "Training loss  0.789 in Step 1400\n",
      "Training loss  0.798 in Step 1500\n",
      "Training loss  0.813 in Step 1600\n",
      "Training loss  0.794 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.792 in Step 0\n",
      "Valid loss  0.798 in Step 100\n",
      "Valid loss  0.791 in Step 200\n",
      "Valid loss  0.795 in Step 300\n",
      "Valid loss  0.799 in Step 400\n",
      "※※※Valid loss  0.803※※※\n",
      "Epoch 68\n",
      "Training loss  0.797 in Step 0\n",
      "Training loss  0.792 in Step 100\n",
      "Training loss  0.819 in Step 200\n",
      "Training loss  0.781 in Step 300\n",
      "Training loss  0.816 in Step 400\n",
      "Training loss  0.802 in Step 500\n",
      "Training loss  0.787 in Step 600\n",
      "Training loss  0.808 in Step 700\n",
      "Training loss  0.797 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.793 in Step 1200\n",
      "Training loss  0.817 in Step 1300\n",
      "Training loss  0.796 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.801 in Step 1600\n",
      "Training loss  0.810 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.791 in Step 0\n",
      "Valid loss  0.797 in Step 100\n",
      "Valid loss  0.791 in Step 200\n",
      "Valid loss  0.795 in Step 300\n",
      "Valid loss  0.797 in Step 400\n",
      "※※※Valid loss  0.803※※※\n",
      "Epoch 69\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.815 in Step 100\n",
      "Training loss  0.791 in Step 200\n",
      "Training loss  0.810 in Step 300\n",
      "Training loss  0.788 in Step 400\n",
      "Training loss  0.802 in Step 500\n",
      "Training loss  0.789 in Step 600\n",
      "Training loss  0.797 in Step 700\n",
      "Training loss  0.791 in Step 800\n",
      "Training loss  0.790 in Step 900\n",
      "Training loss  0.807 in Step 1000\n",
      "Training loss  0.805 in Step 1100\n",
      "Training loss  0.790 in Step 1200\n",
      "Training loss  0.803 in Step 1300\n",
      "Training loss  0.799 in Step 1400\n",
      "Training loss  0.812 in Step 1500\n",
      "Training loss  0.807 in Step 1600\n",
      "Training loss  0.811 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.791 in Step 0\n",
      "Valid loss  0.797 in Step 100\n",
      "Valid loss  0.789 in Step 200\n",
      "Valid loss  0.793 in Step 300\n",
      "Valid loss  0.797 in Step 400\n",
      "※※※Valid loss  0.802※※※\n",
      "Epoch 70\n",
      "Training loss  0.789 in Step 0\n",
      "Training loss  0.783 in Step 100\n",
      "Training loss  0.812 in Step 200\n",
      "Training loss  0.787 in Step 300\n",
      "Training loss  0.793 in Step 400\n",
      "Training loss  0.802 in Step 500\n",
      "Training loss  0.785 in Step 600\n",
      "Training loss  0.803 in Step 700\n",
      "Training loss  0.820 in Step 800\n",
      "Training loss  0.803 in Step 900\n",
      "Training loss  0.799 in Step 1000\n",
      "Training loss  0.802 in Step 1100\n",
      "Training loss  0.804 in Step 1200\n",
      "Training loss  0.794 in Step 1300\n",
      "Training loss  0.792 in Step 1400\n",
      "Training loss  0.792 in Step 1500\n",
      "Training loss  0.827 in Step 1600\n",
      "Training loss  0.797 in Step 1700\n",
      "※※※Training loss  0.800※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.792 in Step 0\n",
      "Valid loss  0.799 in Step 100\n",
      "Valid loss  0.791 in Step 200\n",
      "Valid loss  0.797 in Step 300\n",
      "Valid loss  0.802 in Step 400\n",
      "※※※Valid loss  0.803※※※\n",
      "Epoch 71\n",
      "Training loss  0.789 in Step 0\n",
      "Training loss  0.785 in Step 100\n",
      "Training loss  0.823 in Step 200\n",
      "Training loss  0.797 in Step 300\n",
      "Training loss  0.811 in Step 400\n",
      "Training loss  0.804 in Step 500\n",
      "Training loss  0.786 in Step 600\n",
      "Training loss  0.799 in Step 700\n",
      "Training loss  0.818 in Step 800\n",
      "Training loss  0.805 in Step 900\n",
      "Training loss  0.808 in Step 1000\n",
      "Training loss  0.804 in Step 1100\n",
      "Training loss  0.801 in Step 1200\n",
      "Training loss  0.800 in Step 1300\n",
      "Training loss  0.806 in Step 1400\n",
      "Training loss  0.809 in Step 1500\n",
      "Training loss  0.811 in Step 1600\n",
      "Training loss  0.780 in Step 1700\n",
      "※※※Training loss  0.800※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.790 in Step 0\n",
      "Valid loss  0.798 in Step 100\n",
      "Valid loss  0.790 in Step 200\n",
      "Valid loss  0.795 in Step 300\n",
      "Valid loss  0.798 in Step 400\n",
      "※※※Valid loss  0.802※※※\n",
      "Epoch 72\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.817 in Step 100\n",
      "Training loss  0.788 in Step 200\n",
      "Training loss  0.792 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.791 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.798 in Step 700\n",
      "Training loss  0.788 in Step 800\n",
      "Training loss  0.795 in Step 900\n",
      "Training loss  0.812 in Step 1000\n",
      "Training loss  0.812 in Step 1100\n",
      "Training loss  0.810 in Step 1200\n",
      "Training loss  0.795 in Step 1300\n",
      "Training loss  0.802 in Step 1400\n",
      "Training loss  0.796 in Step 1500\n",
      "Training loss  0.806 in Step 1600\n",
      "Training loss  0.805 in Step 1700\n",
      "※※※Training loss  0.800※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.788 in Step 0\n",
      "Valid loss  0.795 in Step 100\n",
      "Valid loss  0.788 in Step 200\n",
      "Valid loss  0.792 in Step 300\n",
      "Valid loss  0.796 in Step 400\n",
      "※※※Valid loss  0.800※※※\n",
      "Epoch 73\n",
      "Training loss  0.808 in Step 0\n",
      "Training loss  0.807 in Step 100\n",
      "Training loss  0.798 in Step 200\n",
      "Training loss  0.784 in Step 300\n",
      "Training loss  0.808 in Step 400\n",
      "Training loss  0.794 in Step 500\n",
      "Training loss  0.800 in Step 600\n",
      "Training loss  0.795 in Step 700\n",
      "Training loss  0.783 in Step 800\n",
      "Training loss  0.825 in Step 900\n",
      "Training loss  0.799 in Step 1000\n",
      "Training loss  0.802 in Step 1100\n",
      "Training loss  0.783 in Step 1200\n",
      "Training loss  0.799 in Step 1300\n",
      "Training loss  0.805 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.807 in Step 1600\n",
      "Training loss  0.810 in Step 1700\n",
      "※※※Training loss  0.800※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.787 in Step 0\n",
      "Valid loss  0.795 in Step 100\n",
      "Valid loss  0.789 in Step 200\n",
      "Valid loss  0.791 in Step 300\n",
      "Valid loss  0.796 in Step 400\n",
      "※※※Valid loss  0.800※※※\n",
      "Epoch 74\n",
      "Training loss  0.810 in Step 0\n",
      "Training loss  0.810 in Step 100\n",
      "Training loss  0.804 in Step 200\n",
      "Training loss  0.801 in Step 300\n",
      "Training loss  0.792 in Step 400\n",
      "Training loss  0.796 in Step 500\n",
      "Training loss  0.808 in Step 600\n",
      "Training loss  0.797 in Step 700\n",
      "Training loss  0.793 in Step 800\n",
      "Training loss  0.816 in Step 900\n",
      "Training loss  0.800 in Step 1000\n",
      "Training loss  0.809 in Step 1100\n",
      "Training loss  0.805 in Step 1200\n",
      "Training loss  0.793 in Step 1300\n",
      "Training loss  0.802 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.789 in Step 1600\n",
      "Training loss  0.802 in Step 1700\n",
      "※※※Training loss  0.799※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.791 in Step 0\n",
      "Valid loss  0.796 in Step 100\n",
      "Valid loss  0.791 in Step 200\n",
      "Valid loss  0.793 in Step 300\n",
      "Valid loss  0.798 in Step 400\n",
      "※※※Valid loss  0.802※※※\n",
      "Epoch 75\n",
      "Training loss  0.804 in Step 0\n",
      "Training loss  0.794 in Step 100\n",
      "Training loss  0.779 in Step 200\n",
      "Training loss  0.795 in Step 300\n",
      "Training loss  0.804 in Step 400\n",
      "Training loss  0.812 in Step 500\n",
      "Training loss  0.804 in Step 600\n",
      "Training loss  0.795 in Step 700\n",
      "Training loss  0.812 in Step 800\n",
      "Training loss  0.775 in Step 900\n",
      "Training loss  0.805 in Step 1000\n",
      "Training loss  0.799 in Step 1100\n",
      "Training loss  0.789 in Step 1200\n",
      "Training loss  0.794 in Step 1300\n",
      "Training loss  0.789 in Step 1400\n",
      "Training loss  0.808 in Step 1500\n",
      "Training loss  0.781 in Step 1600\n",
      "Training loss  0.805 in Step 1700\n",
      "※※※Training loss  0.800※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.790 in Step 0\n",
      "Valid loss  0.801 in Step 100\n",
      "Valid loss  0.790 in Step 200\n",
      "Valid loss  0.795 in Step 300\n",
      "Valid loss  0.796 in Step 400\n",
      "※※※Valid loss  0.801※※※\n",
      "Epoch 76\n",
      "Training loss  0.811 in Step 0\n",
      "Training loss  0.797 in Step 100\n",
      "Training loss  0.789 in Step 200\n",
      "Training loss  0.796 in Step 300\n",
      "Training loss  0.806 in Step 400\n",
      "Training loss  0.806 in Step 500\n",
      "Training loss  0.796 in Step 600\n",
      "Training loss  0.781 in Step 700\n",
      "Training loss  0.801 in Step 800\n",
      "Training loss  0.793 in Step 900\n",
      "Training loss  0.786 in Step 1000\n",
      "Training loss  0.792 in Step 1100\n",
      "Training loss  0.818 in Step 1200\n",
      "Training loss  0.786 in Step 1300\n",
      "Training loss  0.801 in Step 1400\n",
      "Training loss  0.792 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.800※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.789 in Step 0\n",
      "Valid loss  0.798 in Step 100\n",
      "Valid loss  0.791 in Step 200\n",
      "Valid loss  0.793 in Step 300\n",
      "Valid loss  0.795 in Step 400\n",
      "※※※Valid loss  0.801※※※\n",
      "Epoch 77\n",
      "Training loss  0.806 in Step 0\n",
      "Training loss  0.800 in Step 100\n",
      "Training loss  0.805 in Step 200\n",
      "Training loss  0.795 in Step 300\n",
      "Training loss  0.796 in Step 400\n",
      "Training loss  0.785 in Step 500\n",
      "Training loss  0.808 in Step 600\n",
      "Training loss  0.797 in Step 700\n",
      "Training loss  0.784 in Step 800\n",
      "Training loss  0.796 in Step 900\n",
      "Training loss  0.792 in Step 1000\n",
      "Training loss  0.802 in Step 1100\n",
      "Training loss  0.821 in Step 1200\n",
      "Training loss  0.805 in Step 1300\n",
      "Training loss  0.806 in Step 1400\n",
      "Training loss  0.800 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.811 in Step 1700\n",
      "※※※Training loss  0.799※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.791 in Step 0\n",
      "Valid loss  0.796 in Step 100\n",
      "Valid loss  0.789 in Step 200\n",
      "Valid loss  0.793 in Step 300\n",
      "Valid loss  0.796 in Step 400\n",
      "※※※Valid loss  0.801※※※\n",
      "Epoch 78\n",
      "Training loss  0.802 in Step 0\n",
      "Training loss  0.796 in Step 100\n",
      "Training loss  0.811 in Step 200\n",
      "Training loss  0.817 in Step 300\n",
      "Training loss  0.816 in Step 400\n",
      "Training loss  0.785 in Step 500\n",
      "Training loss  0.819 in Step 600\n",
      "Training loss  0.777 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.814 in Step 900\n",
      "Training loss  0.792 in Step 1000\n",
      "Training loss  0.813 in Step 1100\n",
      "Training loss  0.797 in Step 1200\n",
      "Training loss  0.807 in Step 1300\n",
      "Training loss  0.789 in Step 1400\n",
      "Training loss  0.793 in Step 1500\n",
      "Training loss  0.810 in Step 1600\n",
      "Training loss  0.798 in Step 1700\n",
      "※※※Training loss  0.798※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.792 in Step 0\n",
      "Valid loss  0.800 in Step 100\n",
      "Valid loss  0.793 in Step 200\n",
      "Valid loss  0.796 in Step 300\n",
      "Valid loss  0.800 in Step 400\n",
      "※※※Valid loss  0.804※※※\n",
      "Epoch 79\n",
      "Training loss  0.803 in Step 0\n",
      "Training loss  0.790 in Step 100\n",
      "Training loss  0.807 in Step 200\n",
      "Training loss  0.793 in Step 300\n",
      "Training loss  0.791 in Step 400\n",
      "Training loss  0.781 in Step 500\n",
      "Training loss  0.795 in Step 600\n",
      "Training loss  0.804 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.805 in Step 900\n",
      "Training loss  0.796 in Step 1000\n",
      "Training loss  0.803 in Step 1100\n",
      "Training loss  0.807 in Step 1200\n",
      "Training loss  0.815 in Step 1300\n",
      "Training loss  0.788 in Step 1400\n",
      "Training loss  0.812 in Step 1500\n",
      "Training loss  0.809 in Step 1600\n",
      "Training loss  0.818 in Step 1700\n",
      "※※※Training loss  0.798※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.792 in Step 0\n",
      "Valid loss  0.794 in Step 100\n",
      "Valid loss  0.788 in Step 200\n",
      "Valid loss  0.789 in Step 300\n",
      "Valid loss  0.793 in Step 400\n",
      "※※※Valid loss  0.799※※※\n",
      "Epoch 80\n",
      "Training loss  0.795 in Step 0\n",
      "Training loss  0.789 in Step 100\n",
      "Training loss  0.810 in Step 200\n",
      "Training loss  0.810 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.813 in Step 500\n",
      "Training loss  0.790 in Step 600\n",
      "Training loss  0.802 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.788 in Step 1000\n",
      "Training loss  0.795 in Step 1100\n",
      "Training loss  0.801 in Step 1200\n",
      "Training loss  0.795 in Step 1300\n",
      "Training loss  0.816 in Step 1400\n",
      "Training loss  0.810 in Step 1500\n",
      "Training loss  0.779 in Step 1600\n",
      "Training loss  0.778 in Step 1700\n",
      "※※※Training loss  0.798※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.787 in Step 0\n",
      "Valid loss  0.793 in Step 100\n",
      "Valid loss  0.786 in Step 200\n",
      "Valid loss  0.789 in Step 300\n",
      "Valid loss  0.794 in Step 400\n",
      "※※※Valid loss  0.797※※※\n",
      "Epoch 81\n",
      "Training loss  0.776 in Step 0\n",
      "Training loss  0.792 in Step 100\n",
      "Training loss  0.782 in Step 200\n",
      "Training loss  0.783 in Step 300\n",
      "Training loss  0.784 in Step 400\n",
      "Training loss  0.786 in Step 500\n",
      "Training loss  0.789 in Step 600\n",
      "Training loss  0.802 in Step 700\n",
      "Training loss  0.804 in Step 800\n",
      "Training loss  0.801 in Step 900\n",
      "Training loss  0.788 in Step 1000\n",
      "Training loss  0.792 in Step 1100\n",
      "Training loss  0.809 in Step 1200\n",
      "Training loss  0.785 in Step 1300\n",
      "Training loss  0.789 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.789 in Step 1700\n",
      "※※※Training loss  0.797※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.785 in Step 0\n",
      "Valid loss  0.794 in Step 100\n",
      "Valid loss  0.787 in Step 200\n",
      "Valid loss  0.790 in Step 300\n",
      "Valid loss  0.794 in Step 400\n",
      "※※※Valid loss  0.798※※※\n",
      "Epoch 82\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.779 in Step 100\n",
      "Training loss  0.807 in Step 200\n",
      "Training loss  0.795 in Step 300\n",
      "Training loss  0.795 in Step 400\n",
      "Training loss  0.790 in Step 500\n",
      "Training loss  0.794 in Step 600\n",
      "Training loss  0.812 in Step 700\n",
      "Training loss  0.792 in Step 800\n",
      "Training loss  0.783 in Step 900\n",
      "Training loss  0.807 in Step 1000\n",
      "Training loss  0.805 in Step 1100\n",
      "Training loss  0.784 in Step 1200\n",
      "Training loss  0.786 in Step 1300\n",
      "Training loss  0.787 in Step 1400\n",
      "Training loss  0.786 in Step 1500\n",
      "Training loss  0.788 in Step 1600\n",
      "Training loss  0.806 in Step 1700\n",
      "※※※Training loss  0.797※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.788 in Step 0\n",
      "Valid loss  0.796 in Step 100\n",
      "Valid loss  0.791 in Step 200\n",
      "Valid loss  0.792 in Step 300\n",
      "Valid loss  0.799 in Step 400\n",
      "※※※Valid loss  0.800※※※\n",
      "Epoch 83\n",
      "Training loss  0.792 in Step 0\n",
      "Training loss  0.793 in Step 100\n",
      "Training loss  0.799 in Step 200\n",
      "Training loss  0.812 in Step 300\n",
      "Training loss  0.798 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.792 in Step 600\n",
      "Training loss  0.791 in Step 700\n",
      "Training loss  0.807 in Step 800\n",
      "Training loss  0.793 in Step 900\n",
      "Training loss  0.811 in Step 1000\n",
      "Training loss  0.804 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.801 in Step 1300\n",
      "Training loss  0.797 in Step 1400\n",
      "Training loss  0.786 in Step 1500\n",
      "Training loss  0.806 in Step 1600\n",
      "Training loss  0.784 in Step 1700\n",
      "※※※Training loss  0.798※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.786 in Step 0\n",
      "Valid loss  0.796 in Step 100\n",
      "Valid loss  0.790 in Step 200\n",
      "Valid loss  0.789 in Step 300\n",
      "Valid loss  0.796 in Step 400\n",
      "※※※Valid loss  0.799※※※\n",
      "Epoch 84\n",
      "Training loss  0.784 in Step 0\n",
      "Training loss  0.803 in Step 100\n",
      "Training loss  0.791 in Step 200\n",
      "Training loss  0.781 in Step 300\n",
      "Training loss  0.796 in Step 400\n",
      "Training loss  0.783 in Step 500\n",
      "Training loss  0.799 in Step 600\n",
      "Training loss  0.797 in Step 700\n",
      "Training loss  0.770 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.811 in Step 1000\n",
      "Training loss  0.807 in Step 1100\n",
      "Training loss  0.793 in Step 1200\n",
      "Training loss  0.797 in Step 1300\n",
      "Training loss  0.781 in Step 1400\n",
      "Training loss  0.818 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.814 in Step 1700\n",
      "※※※Training loss  0.796※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.791 in Step 0\n",
      "Valid loss  0.801 in Step 100\n",
      "Valid loss  0.791 in Step 200\n",
      "Valid loss  0.795 in Step 300\n",
      "Valid loss  0.798 in Step 400\n",
      "※※※Valid loss  0.803※※※\n",
      "Epoch 85\n",
      "Training loss  0.802 in Step 0\n",
      "Training loss  0.803 in Step 100\n",
      "Training loss  0.795 in Step 200\n",
      "Training loss  0.794 in Step 300\n",
      "Training loss  0.806 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.788 in Step 600\n",
      "Training loss  0.796 in Step 700\n",
      "Training loss  0.800 in Step 800\n",
      "Training loss  0.788 in Step 900\n",
      "Training loss  0.784 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.784 in Step 1200\n",
      "Training loss  0.799 in Step 1300\n",
      "Training loss  0.807 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.795 in Step 1700\n",
      "※※※Training loss  0.797※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.785 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.787 in Step 200\n",
      "Valid loss  0.788 in Step 300\n",
      "Valid loss  0.793 in Step 400\n",
      "※※※Valid loss  0.797※※※\n",
      "Epoch 86\n",
      "Training loss  0.793 in Step 0\n",
      "Training loss  0.787 in Step 100\n",
      "Training loss  0.804 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.811 in Step 400\n",
      "Training loss  0.794 in Step 500\n",
      "Training loss  0.808 in Step 600\n",
      "Training loss  0.819 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.797 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.790 in Step 1100\n",
      "Training loss  0.779 in Step 1200\n",
      "Training loss  0.789 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.787 in Step 1500\n",
      "Training loss  0.793 in Step 1600\n",
      "Training loss  0.802 in Step 1700\n",
      "※※※Training loss  0.796※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.785 in Step 0\n",
      "Valid loss  0.791 in Step 100\n",
      "Valid loss  0.787 in Step 200\n",
      "Valid loss  0.789 in Step 300\n",
      "Valid loss  0.792 in Step 400\n",
      "※※※Valid loss  0.798※※※\n",
      "Epoch 87\n",
      "Training loss  0.797 in Step 0\n",
      "Training loss  0.776 in Step 100\n",
      "Training loss  0.797 in Step 200\n",
      "Training loss  0.796 in Step 300\n",
      "Training loss  0.797 in Step 400\n",
      "Training loss  0.795 in Step 500\n",
      "Training loss  0.799 in Step 600\n",
      "Training loss  0.793 in Step 700\n",
      "Training loss  0.787 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.820 in Step 1000\n",
      "Training loss  0.809 in Step 1100\n",
      "Training loss  0.784 in Step 1200\n",
      "Training loss  0.789 in Step 1300\n",
      "Training loss  0.791 in Step 1400\n",
      "Training loss  0.789 in Step 1500\n",
      "Training loss  0.777 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.795※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.784 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.785 in Step 200\n",
      "Valid loss  0.788 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.795※※※\n",
      "Epoch 88\n",
      "Training loss  0.801 in Step 0\n",
      "Training loss  0.774 in Step 100\n",
      "Training loss  0.797 in Step 200\n",
      "Training loss  0.810 in Step 300\n",
      "Training loss  0.783 in Step 400\n",
      "Training loss  0.798 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.784 in Step 700\n",
      "Training loss  0.800 in Step 800\n",
      "Training loss  0.773 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.805 in Step 1200\n",
      "Training loss  0.767 in Step 1300\n",
      "Training loss  0.799 in Step 1400\n",
      "Training loss  0.808 in Step 1500\n",
      "Training loss  0.791 in Step 1600\n",
      "Training loss  0.806 in Step 1700\n",
      "※※※Training loss  0.795※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.785 in Step 0\n",
      "Valid loss  0.789 in Step 100\n",
      "Valid loss  0.785 in Step 200\n",
      "Valid loss  0.788 in Step 300\n",
      "Valid loss  0.792 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 89\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.776 in Step 100\n",
      "Training loss  0.802 in Step 200\n",
      "Training loss  0.792 in Step 300\n",
      "Training loss  0.788 in Step 400\n",
      "Training loss  0.799 in Step 500\n",
      "Training loss  0.788 in Step 600\n",
      "Training loss  0.807 in Step 700\n",
      "Training loss  0.778 in Step 800\n",
      "Training loss  0.817 in Step 900\n",
      "Training loss  0.781 in Step 1000\n",
      "Training loss  0.797 in Step 1100\n",
      "Training loss  0.788 in Step 1200\n",
      "Training loss  0.777 in Step 1300\n",
      "Training loss  0.814 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.808 in Step 1600\n",
      "Training loss  0.794 in Step 1700\n",
      "※※※Training loss  0.795※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.788 in Step 0\n",
      "Valid loss  0.793 in Step 100\n",
      "Valid loss  0.788 in Step 200\n",
      "Valid loss  0.790 in Step 300\n",
      "Valid loss  0.795 in Step 400\n",
      "※※※Valid loss  0.799※※※\n",
      "Epoch 90\n",
      "Training loss  0.806 in Step 0\n",
      "Training loss  0.793 in Step 100\n",
      "Training loss  0.795 in Step 200\n",
      "Training loss  0.812 in Step 300\n",
      "Training loss  0.796 in Step 400\n",
      "Training loss  0.795 in Step 500\n",
      "Training loss  0.793 in Step 600\n",
      "Training loss  0.776 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.789 in Step 900\n",
      "Training loss  0.797 in Step 1000\n",
      "Training loss  0.790 in Step 1100\n",
      "Training loss  0.809 in Step 1200\n",
      "Training loss  0.814 in Step 1300\n",
      "Training loss  0.779 in Step 1400\n",
      "Training loss  0.787 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.785 in Step 1700\n",
      "※※※Training loss  0.795※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.784 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.787 in Step 300\n",
      "Valid loss  0.792 in Step 400\n",
      "※※※Valid loss  0.795※※※\n",
      "Epoch 91\n",
      "Training loss  0.804 in Step 0\n",
      "Training loss  0.801 in Step 100\n",
      "Training loss  0.801 in Step 200\n",
      "Training loss  0.791 in Step 300\n",
      "Training loss  0.817 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.784 in Step 700\n",
      "Training loss  0.801 in Step 800\n",
      "Training loss  0.795 in Step 900\n",
      "Training loss  0.786 in Step 1000\n",
      "Training loss  0.797 in Step 1100\n",
      "Training loss  0.789 in Step 1200\n",
      "Training loss  0.792 in Step 1300\n",
      "Training loss  0.812 in Step 1400\n",
      "Training loss  0.798 in Step 1500\n",
      "Training loss  0.786 in Step 1600\n",
      "Training loss  0.791 in Step 1700\n",
      "※※※Training loss  0.795※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.784 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.785 in Step 200\n",
      "Valid loss  0.786 in Step 300\n",
      "Valid loss  0.792 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 92\n",
      "Training loss  0.788 in Step 0\n",
      "Training loss  0.799 in Step 100\n",
      "Training loss  0.809 in Step 200\n",
      "Training loss  0.787 in Step 300\n",
      "Training loss  0.807 in Step 400\n",
      "Training loss  0.779 in Step 500\n",
      "Training loss  0.795 in Step 600\n",
      "Training loss  0.806 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.774 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.795 in Step 1100\n",
      "Training loss  0.770 in Step 1200\n",
      "Training loss  0.789 in Step 1300\n",
      "Training loss  0.803 in Step 1400\n",
      "Training loss  0.782 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.799 in Step 1700\n",
      "※※※Training loss  0.795※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.785 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.785 in Step 200\n",
      "Valid loss  0.788 in Step 300\n",
      "Valid loss  0.791 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 93\n",
      "Training loss  0.800 in Step 0\n",
      "Training loss  0.812 in Step 100\n",
      "Training loss  0.804 in Step 200\n",
      "Training loss  0.806 in Step 300\n",
      "Training loss  0.784 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.799 in Step 600\n",
      "Training loss  0.799 in Step 700\n",
      "Training loss  0.785 in Step 800\n",
      "Training loss  0.785 in Step 900\n",
      "Training loss  0.812 in Step 1000\n",
      "Training loss  0.806 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.772 in Step 1300\n",
      "Training loss  0.800 in Step 1400\n",
      "Training loss  0.775 in Step 1500\n",
      "Training loss  0.791 in Step 1600\n",
      "Training loss  0.786 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.782 in Step 0\n",
      "Valid loss  0.788 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.783 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.794※※※\n",
      "Epoch 94\n",
      "Training loss  0.787 in Step 0\n",
      "Training loss  0.780 in Step 100\n",
      "Training loss  0.787 in Step 200\n",
      "Training loss  0.782 in Step 300\n",
      "Training loss  0.795 in Step 400\n",
      "Training loss  0.790 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.780 in Step 700\n",
      "Training loss  0.814 in Step 800\n",
      "Training loss  0.802 in Step 900\n",
      "Training loss  0.782 in Step 1000\n",
      "Training loss  0.789 in Step 1100\n",
      "Training loss  0.778 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.804 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.793 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.783 in Step 0\n",
      "Valid loss  0.792 in Step 100\n",
      "Valid loss  0.786 in Step 200\n",
      "Valid loss  0.786 in Step 300\n",
      "Valid loss  0.791 in Step 400\n",
      "※※※Valid loss  0.795※※※\n",
      "Epoch 95\n",
      "Training loss  0.801 in Step 0\n",
      "Training loss  0.790 in Step 100\n",
      "Training loss  0.786 in Step 200\n",
      "Training loss  0.807 in Step 300\n",
      "Training loss  0.805 in Step 400\n",
      "Training loss  0.798 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.781 in Step 700\n",
      "Training loss  0.796 in Step 800\n",
      "Training loss  0.795 in Step 900\n",
      "Training loss  0.792 in Step 1000\n",
      "Training loss  0.791 in Step 1100\n",
      "Training loss  0.790 in Step 1200\n",
      "Training loss  0.787 in Step 1300\n",
      "Training loss  0.787 in Step 1400\n",
      "Training loss  0.792 in Step 1500\n",
      "Training loss  0.789 in Step 1600\n",
      "Training loss  0.788 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.800 in Step 0\n",
      "Valid loss  0.806 in Step 100\n",
      "Valid loss  0.798 in Step 200\n",
      "Valid loss  0.805 in Step 300\n",
      "Valid loss  0.806 in Step 400\n",
      "※※※Valid loss  0.811※※※\n",
      "Epoch 96\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.798 in Step 100\n",
      "Training loss  0.787 in Step 200\n",
      "Training loss  0.795 in Step 300\n",
      "Training loss  0.807 in Step 400\n",
      "Training loss  0.804 in Step 500\n",
      "Training loss  0.801 in Step 600\n",
      "Training loss  0.786 in Step 700\n",
      "Training loss  0.798 in Step 800\n",
      "Training loss  0.807 in Step 900\n",
      "Training loss  0.780 in Step 1000\n",
      "Training loss  0.804 in Step 1100\n",
      "Training loss  0.799 in Step 1200\n",
      "Training loss  0.803 in Step 1300\n",
      "Training loss  0.797 in Step 1400\n",
      "Training loss  0.774 in Step 1500\n",
      "Training loss  0.802 in Step 1600\n",
      "Training loss  0.800 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.786 in Step 0\n",
      "Valid loss  0.789 in Step 100\n",
      "Valid loss  0.786 in Step 200\n",
      "Valid loss  0.789 in Step 300\n",
      "Valid loss  0.794 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 97\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.794 in Step 100\n",
      "Training loss  0.805 in Step 200\n",
      "Training loss  0.807 in Step 300\n",
      "Training loss  0.797 in Step 400\n",
      "Training loss  0.791 in Step 500\n",
      "Training loss  0.782 in Step 600\n",
      "Training loss  0.794 in Step 700\n",
      "Training loss  0.781 in Step 800\n",
      "Training loss  0.784 in Step 900\n",
      "Training loss  0.793 in Step 1000\n",
      "Training loss  0.806 in Step 1100\n",
      "Training loss  0.811 in Step 1200\n",
      "Training loss  0.810 in Step 1300\n",
      "Training loss  0.809 in Step 1400\n",
      "Training loss  0.788 in Step 1500\n",
      "Training loss  0.782 in Step 1600\n",
      "Training loss  0.808 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.782 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.786 in Step 300\n",
      "Valid loss  0.793 in Step 400\n",
      "※※※Valid loss  0.794※※※\n",
      "Epoch 98\n",
      "Training loss  0.798 in Step 0\n",
      "Training loss  0.785 in Step 100\n",
      "Training loss  0.795 in Step 200\n",
      "Training loss  0.770 in Step 300\n",
      "Training loss  0.795 in Step 400\n",
      "Training loss  0.793 in Step 500\n",
      "Training loss  0.783 in Step 600\n",
      "Training loss  0.775 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.788 in Step 900\n",
      "Training loss  0.800 in Step 1000\n",
      "Training loss  0.805 in Step 1100\n",
      "Training loss  0.784 in Step 1200\n",
      "Training loss  0.806 in Step 1300\n",
      "Training loss  0.801 in Step 1400\n",
      "Training loss  0.814 in Step 1500\n",
      "Training loss  0.795 in Step 1600\n",
      "Training loss  0.799 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.779 in Step 0\n",
      "Valid loss  0.788 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.785 in Step 300\n",
      "Valid loss  0.789 in Step 400\n",
      "※※※Valid loss  0.794※※※\n",
      "Epoch 99\n",
      "Training loss  0.776 in Step 0\n",
      "Training loss  0.789 in Step 100\n",
      "Training loss  0.762 in Step 200\n",
      "Training loss  0.794 in Step 300\n",
      "Training loss  0.782 in Step 400\n",
      "Training loss  0.788 in Step 500\n",
      "Training loss  0.779 in Step 600\n",
      "Training loss  0.784 in Step 700\n",
      "Training loss  0.801 in Step 800\n",
      "Training loss  0.803 in Step 900\n",
      "Training loss  0.801 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.799 in Step 1200\n",
      "Training loss  0.779 in Step 1300\n",
      "Training loss  0.783 in Step 1400\n",
      "Training loss  0.812 in Step 1500\n",
      "Training loss  0.812 in Step 1600\n",
      "Training loss  0.800 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.785 in Step 0\n",
      "Valid loss  0.792 in Step 100\n",
      "Valid loss  0.786 in Step 200\n",
      "Valid loss  0.787 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 100\n",
      "Training loss  0.798 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.799 in Step 200\n",
      "Training loss  0.805 in Step 300\n",
      "Training loss  0.803 in Step 400\n",
      "Training loss  0.801 in Step 500\n",
      "Training loss  0.794 in Step 600\n",
      "Training loss  0.805 in Step 700\n",
      "Training loss  0.790 in Step 800\n",
      "Training loss  0.801 in Step 900\n",
      "Training loss  0.800 in Step 1000\n",
      "Training loss  0.795 in Step 1100\n",
      "Training loss  0.801 in Step 1200\n",
      "Training loss  0.767 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.804 in Step 1500\n",
      "Training loss  0.790 in Step 1600\n",
      "Training loss  0.775 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.783 in Step 0\n",
      "Valid loss  0.789 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.788 in Step 300\n",
      "Valid loss  0.793 in Step 400\n",
      "※※※Valid loss  0.795※※※\n",
      "Epoch 101\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.811 in Step 100\n",
      "Training loss  0.784 in Step 200\n",
      "Training loss  0.789 in Step 300\n",
      "Training loss  0.762 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.786 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.805 in Step 900\n",
      "Training loss  0.819 in Step 1000\n",
      "Training loss  0.786 in Step 1100\n",
      "Training loss  0.797 in Step 1200\n",
      "Training loss  0.781 in Step 1300\n",
      "Training loss  0.776 in Step 1400\n",
      "Training loss  0.780 in Step 1500\n",
      "Training loss  0.798 in Step 1600\n",
      "Training loss  0.776 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.776 in Step 0\n",
      "Valid loss  0.791 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.791 in Step 400\n",
      "※※※Valid loss  0.793※※※\n",
      "Epoch 102\n",
      "Training loss  0.810 in Step 0\n",
      "Training loss  0.780 in Step 100\n",
      "Training loss  0.798 in Step 200\n",
      "Training loss  0.772 in Step 300\n",
      "Training loss  0.799 in Step 400\n",
      "Training loss  0.787 in Step 500\n",
      "Training loss  0.793 in Step 600\n",
      "Training loss  0.780 in Step 700\n",
      "Training loss  0.808 in Step 800\n",
      "Training loss  0.792 in Step 900\n",
      "Training loss  0.787 in Step 1000\n",
      "Training loss  0.799 in Step 1100\n",
      "Training loss  0.777 in Step 1200\n",
      "Training loss  0.800 in Step 1300\n",
      "Training loss  0.793 in Step 1400\n",
      "Training loss  0.798 in Step 1500\n",
      "Training loss  0.809 in Step 1600\n",
      "Training loss  0.789 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.781 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.783 in Step 300\n",
      "Valid loss  0.789 in Step 400\n",
      "※※※Valid loss  0.793※※※\n",
      "Epoch 103\n",
      "Training loss  0.786 in Step 0\n",
      "Training loss  0.800 in Step 100\n",
      "Training loss  0.792 in Step 200\n",
      "Training loss  0.797 in Step 300\n",
      "Training loss  0.807 in Step 400\n",
      "Training loss  0.801 in Step 500\n",
      "Training loss  0.792 in Step 600\n",
      "Training loss  0.807 in Step 700\n",
      "Training loss  0.794 in Step 800\n",
      "Training loss  0.778 in Step 900\n",
      "Training loss  0.779 in Step 1000\n",
      "Training loss  0.795 in Step 1100\n",
      "Training loss  0.775 in Step 1200\n",
      "Training loss  0.783 in Step 1300\n",
      "Training loss  0.786 in Step 1400\n",
      "Training loss  0.768 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.795 in Step 1700\n",
      "※※※Training loss  0.792※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.780 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.781 in Step 200\n",
      "Valid loss  0.784 in Step 300\n",
      "Valid loss  0.787 in Step 400\n",
      "※※※Valid loss  0.792※※※\n",
      "Epoch 104\n",
      "Training loss  0.809 in Step 0\n",
      "Training loss  0.774 in Step 100\n",
      "Training loss  0.804 in Step 200\n",
      "Training loss  0.791 in Step 300\n",
      "Training loss  0.796 in Step 400\n",
      "Training loss  0.806 in Step 500\n",
      "Training loss  0.801 in Step 600\n",
      "Training loss  0.787 in Step 700\n",
      "Training loss  0.806 in Step 800\n",
      "Training loss  0.788 in Step 900\n",
      "Training loss  0.790 in Step 1000\n",
      "Training loss  0.786 in Step 1100\n",
      "Training loss  0.777 in Step 1200\n",
      "Training loss  0.776 in Step 1300\n",
      "Training loss  0.783 in Step 1400\n",
      "Training loss  0.795 in Step 1500\n",
      "Training loss  0.789 in Step 1600\n",
      "Training loss  0.809 in Step 1700\n",
      "※※※Training loss  0.791※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.782 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.787 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.794※※※\n",
      "Epoch 105\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.797 in Step 100\n",
      "Training loss  0.781 in Step 200\n",
      "Training loss  0.795 in Step 300\n",
      "Training loss  0.788 in Step 400\n",
      "Training loss  0.777 in Step 500\n",
      "Training loss  0.790 in Step 600\n",
      "Training loss  0.805 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.778 in Step 900\n",
      "Training loss  0.784 in Step 1000\n",
      "Training loss  0.812 in Step 1100\n",
      "Training loss  0.801 in Step 1200\n",
      "Training loss  0.797 in Step 1300\n",
      "Training loss  0.794 in Step 1400\n",
      "Training loss  0.820 in Step 1500\n",
      "Training loss  0.798 in Step 1600\n",
      "Training loss  0.813 in Step 1700\n",
      "※※※Training loss  0.792※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.783 in Step 0\n",
      "Valid loss  0.786 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.788 in Step 300\n",
      "Valid loss  0.791 in Step 400\n",
      "※※※Valid loss  0.794※※※\n",
      "Epoch 106\n",
      "Training loss  0.788 in Step 0\n",
      "Training loss  0.793 in Step 100\n",
      "Training loss  0.788 in Step 200\n",
      "Training loss  0.800 in Step 300\n",
      "Training loss  0.787 in Step 400\n",
      "Training loss  0.773 in Step 500\n",
      "Training loss  0.794 in Step 600\n",
      "Training loss  0.805 in Step 700\n",
      "Training loss  0.799 in Step 800\n",
      "Training loss  0.810 in Step 900\n",
      "Training loss  0.778 in Step 1000\n",
      "Training loss  0.791 in Step 1100\n",
      "Training loss  0.800 in Step 1200\n",
      "Training loss  0.804 in Step 1300\n",
      "Training loss  0.805 in Step 1400\n",
      "Training loss  0.793 in Step 1500\n",
      "Training loss  0.790 in Step 1600\n",
      "Training loss  0.788 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.781 in Step 0\n",
      "Valid loss  0.792 in Step 100\n",
      "Valid loss  0.785 in Step 200\n",
      "Valid loss  0.789 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 107\n",
      "Training loss  0.812 in Step 0\n",
      "Training loss  0.791 in Step 100\n",
      "Training loss  0.809 in Step 200\n",
      "Training loss  0.794 in Step 300\n",
      "Training loss  0.777 in Step 400\n",
      "Training loss  0.805 in Step 500\n",
      "Training loss  0.794 in Step 600\n",
      "Training loss  0.786 in Step 700\n",
      "Training loss  0.793 in Step 800\n",
      "Training loss  0.777 in Step 900\n",
      "Training loss  0.791 in Step 1000\n",
      "Training loss  0.781 in Step 1100\n",
      "Training loss  0.779 in Step 1200\n",
      "Training loss  0.795 in Step 1300\n",
      "Training loss  0.778 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.791※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.777 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.784 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.792※※※\n",
      "Epoch 108\n",
      "Training loss  0.803 in Step 0\n",
      "Training loss  0.795 in Step 100\n",
      "Training loss  0.796 in Step 200\n",
      "Training loss  0.806 in Step 300\n",
      "Training loss  0.823 in Step 400\n",
      "Training loss  0.794 in Step 500\n",
      "Training loss  0.779 in Step 600\n",
      "Training loss  0.795 in Step 700\n",
      "Training loss  0.783 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.800 in Step 1000\n",
      "Training loss  0.781 in Step 1100\n",
      "Training loss  0.815 in Step 1200\n",
      "Training loss  0.790 in Step 1300\n",
      "Training loss  0.792 in Step 1400\n",
      "Training loss  0.811 in Step 1500\n",
      "Training loss  0.796 in Step 1600\n",
      "Training loss  0.794 in Step 1700\n",
      "※※※Training loss  0.791※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.778 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.788 in Step 300\n",
      "Valid loss  0.787 in Step 400\n",
      "※※※Valid loss  0.793※※※\n",
      "Epoch 109\n",
      "Training loss  0.795 in Step 0\n",
      "Training loss  0.780 in Step 100\n",
      "Training loss  0.790 in Step 200\n",
      "Training loss  0.778 in Step 300\n",
      "Training loss  0.805 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.779 in Step 600\n",
      "Training loss  0.776 in Step 700\n",
      "Training loss  0.782 in Step 800\n",
      "Training loss  0.805 in Step 900\n",
      "Training loss  0.779 in Step 1000\n",
      "Training loss  0.777 in Step 1100\n",
      "Training loss  0.800 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.783 in Step 1400\n",
      "Training loss  0.789 in Step 1500\n",
      "Training loss  0.793 in Step 1600\n",
      "Training loss  0.804 in Step 1700\n",
      "※※※Training loss  0.792※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.776 in Step 0\n",
      "Valid loss  0.786 in Step 100\n",
      "Valid loss  0.781 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.787 in Step 400\n",
      "※※※Valid loss  0.791※※※\n",
      "Epoch 110\n",
      "Training loss  0.793 in Step 0\n",
      "Training loss  0.795 in Step 100\n",
      "Training loss  0.780 in Step 200\n",
      "Training loss  0.791 in Step 300\n",
      "Training loss  0.806 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.805 in Step 600\n",
      "Training loss  0.787 in Step 700\n",
      "Training loss  0.802 in Step 800\n",
      "Training loss  0.811 in Step 900\n",
      "Training loss  0.790 in Step 1000\n",
      "Training loss  0.794 in Step 1100\n",
      "Training loss  0.785 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.796 in Step 1400\n",
      "Training loss  0.793 in Step 1500\n",
      "Training loss  0.796 in Step 1600\n",
      "Training loss  0.775 in Step 1700\n",
      "※※※Training loss  0.790※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.780 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.786 in Step 300\n",
      "Valid loss  0.791 in Step 400\n",
      "※※※Valid loss  0.792※※※\n",
      "Epoch 111\n",
      "Training loss  0.800 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.776 in Step 200\n",
      "Training loss  0.799 in Step 300\n",
      "Training loss  0.795 in Step 400\n",
      "Training loss  0.791 in Step 500\n",
      "Training loss  0.785 in Step 600\n",
      "Training loss  0.795 in Step 700\n",
      "Training loss  0.783 in Step 800\n",
      "Training loss  0.813 in Step 900\n",
      "Training loss  0.787 in Step 1000\n",
      "Training loss  0.794 in Step 1100\n",
      "Training loss  0.810 in Step 1200\n",
      "Training loss  0.801 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.796 in Step 1500\n",
      "Training loss  0.788 in Step 1600\n",
      "Training loss  0.786 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.804 in Step 0\n",
      "Valid loss  0.802 in Step 100\n",
      "Valid loss  0.801 in Step 200\n",
      "Valid loss  0.809 in Step 300\n",
      "Valid loss  0.808 in Step 400\n",
      "※※※Valid loss  0.814※※※\n",
      "Epoch 112\n",
      "Training loss  0.801 in Step 0\n",
      "Training loss  0.788 in Step 100\n",
      "Training loss  0.775 in Step 200\n",
      "Training loss  0.776 in Step 300\n",
      "Training loss  0.819 in Step 400\n",
      "Training loss  0.781 in Step 500\n",
      "Training loss  0.775 in Step 600\n",
      "Training loss  0.809 in Step 700\n",
      "Training loss  0.808 in Step 800\n",
      "Training loss  0.799 in Step 900\n",
      "Training loss  0.781 in Step 1000\n",
      "Training loss  0.797 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.807 in Step 1300\n",
      "Training loss  0.778 in Step 1400\n",
      "Training loss  0.791 in Step 1500\n",
      "Training loss  0.771 in Step 1600\n",
      "Training loss  0.800 in Step 1700\n",
      "※※※Training loss  0.791※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.776 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.781 in Step 300\n",
      "Valid loss  0.787 in Step 400\n",
      "※※※Valid loss  0.793※※※\n",
      "Epoch 113\n",
      "Training loss  0.789 in Step 0\n",
      "Training loss  0.781 in Step 100\n",
      "Training loss  0.781 in Step 200\n",
      "Training loss  0.798 in Step 300\n",
      "Training loss  0.782 in Step 400\n",
      "Training loss  0.792 in Step 500\n",
      "Training loss  0.792 in Step 600\n",
      "Training loss  0.791 in Step 700\n",
      "Training loss  0.777 in Step 800\n",
      "Training loss  0.778 in Step 900\n",
      "Training loss  0.788 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.772 in Step 1200\n",
      "Training loss  0.778 in Step 1300\n",
      "Training loss  0.801 in Step 1400\n",
      "Training loss  0.789 in Step 1500\n",
      "Training loss  0.782 in Step 1600\n",
      "Training loss  0.798 in Step 1700\n",
      "※※※Training loss  0.790※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.778 in Step 0\n",
      "Valid loss  0.785 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.783 in Step 300\n",
      "Valid loss  0.788 in Step 400\n",
      "※※※Valid loss  0.791※※※\n",
      "Epoch 114\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.772 in Step 100\n",
      "Training loss  0.772 in Step 200\n",
      "Training loss  0.801 in Step 300\n",
      "Training loss  0.765 in Step 400\n",
      "Training loss  0.788 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.804 in Step 700\n",
      "Training loss  0.792 in Step 800\n",
      "Training loss  0.811 in Step 900\n",
      "Training loss  0.813 in Step 1000\n",
      "Training loss  0.787 in Step 1100\n",
      "Training loss  0.794 in Step 1200\n",
      "Training loss  0.788 in Step 1300\n",
      "Training loss  0.792 in Step 1400\n",
      "Training loss  0.801 in Step 1500\n",
      "Training loss  0.779 in Step 1600\n",
      "Training loss  0.777 in Step 1700\n",
      "※※※Training loss  0.791※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.784 in Step 0\n",
      "Valid loss  0.786 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.785 in Step 300\n",
      "Valid loss  0.791 in Step 400\n",
      "※※※Valid loss  0.792※※※\n",
      "Epoch 115\n",
      "Training loss  0.788 in Step 0\n",
      "Training loss  0.780 in Step 100\n",
      "Training loss  0.789 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.789 in Step 400\n",
      "Training loss  0.777 in Step 500\n",
      "Training loss  0.796 in Step 600\n",
      "Training loss  0.787 in Step 700\n",
      "Training loss  0.791 in Step 800\n",
      "Training loss  0.791 in Step 900\n",
      "Training loss  0.789 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.778 in Step 1200\n",
      "Training loss  0.784 in Step 1300\n",
      "Training loss  0.777 in Step 1400\n",
      "Training loss  0.768 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.770 in Step 1700\n",
      "※※※Training loss  0.790※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.774 in Step 0\n",
      "Valid loss  0.783 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.781 in Step 300\n",
      "Valid loss  0.785 in Step 400\n",
      "※※※Valid loss  0.789※※※\n",
      "Epoch 116\n",
      "Training loss  0.787 in Step 0\n",
      "Training loss  0.808 in Step 100\n",
      "Training loss  0.796 in Step 200\n",
      "Training loss  0.794 in Step 300\n",
      "Training loss  0.771 in Step 400\n",
      "Training loss  0.757 in Step 500\n",
      "Training loss  0.778 in Step 600\n",
      "Training loss  0.791 in Step 700\n",
      "Training loss  0.781 in Step 800\n",
      "Training loss  0.779 in Step 900\n",
      "Training loss  0.793 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.799 in Step 1200\n",
      "Training loss  0.793 in Step 1300\n",
      "Training loss  0.794 in Step 1400\n",
      "Training loss  0.793 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.770 in Step 1700\n",
      "※※※Training loss  0.789※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.786 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.787 in Step 400\n",
      "※※※Valid loss  0.790※※※\n",
      "Epoch 117\n",
      "Training loss  0.787 in Step 0\n",
      "Training loss  0.797 in Step 100\n",
      "Training loss  0.796 in Step 200\n",
      "Training loss  0.812 in Step 300\n",
      "Training loss  0.783 in Step 400\n",
      "Training loss  0.805 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.797 in Step 800\n",
      "Training loss  0.769 in Step 900\n",
      "Training loss  0.796 in Step 1000\n",
      "Training loss  0.776 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.783 in Step 1300\n",
      "Training loss  0.791 in Step 1400\n",
      "Training loss  0.807 in Step 1500\n",
      "Training loss  0.823 in Step 1600\n",
      "Training loss  0.794 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.782 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.788 in Step 200\n",
      "Valid loss  0.790 in Step 300\n",
      "Valid loss  0.791 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 118\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.775 in Step 100\n",
      "Training loss  0.795 in Step 200\n",
      "Training loss  0.780 in Step 300\n",
      "Training loss  0.782 in Step 400\n",
      "Training loss  0.790 in Step 500\n",
      "Training loss  0.802 in Step 600\n",
      "Training loss  0.777 in Step 700\n",
      "Training loss  0.777 in Step 800\n",
      "Training loss  0.799 in Step 900\n",
      "Training loss  0.802 in Step 1000\n",
      "Training loss  0.799 in Step 1100\n",
      "Training loss  0.809 in Step 1200\n",
      "Training loss  0.800 in Step 1300\n",
      "Training loss  0.785 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.790 in Step 1600\n",
      "Training loss  0.783 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.779 in Step 0\n",
      "Valid loss  0.790 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.789 in Step 300\n",
      "Valid loss  0.786 in Step 400\n",
      "※※※Valid loss  0.793※※※\n",
      "Epoch 119\n",
      "Training loss  0.789 in Step 0\n",
      "Training loss  0.798 in Step 100\n",
      "Training loss  0.790 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.802 in Step 400\n",
      "Training loss  0.806 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.798 in Step 700\n",
      "Training loss  0.769 in Step 800\n",
      "Training loss  0.781 in Step 900\n",
      "Training loss  0.790 in Step 1000\n",
      "Training loss  0.784 in Step 1100\n",
      "Training loss  0.765 in Step 1200\n",
      "Training loss  0.794 in Step 1300\n",
      "Training loss  0.781 in Step 1400\n",
      "Training loss  0.789 in Step 1500\n",
      "Training loss  0.776 in Step 1600\n",
      "Training loss  0.784 in Step 1700\n",
      "※※※Training loss  0.796※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.779 in Step 0\n",
      "Valid loss  0.788 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.786 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.794※※※\n",
      "Epoch 120\n",
      "Training loss  0.791 in Step 0\n",
      "Training loss  0.784 in Step 100\n",
      "Training loss  0.801 in Step 200\n",
      "Training loss  0.783 in Step 300\n",
      "Training loss  0.774 in Step 400\n",
      "Training loss  0.806 in Step 500\n",
      "Training loss  0.814 in Step 600\n",
      "Training loss  0.860 in Step 700\n",
      "Training loss  1.146 in Step 800\n",
      "Training loss  1.032 in Step 900\n",
      "Training loss  0.888 in Step 1000\n",
      "Training loss  0.823 in Step 1100\n",
      "Training loss  0.816 in Step 1200\n",
      "Training loss  0.786 in Step 1300\n",
      "Training loss  0.811 in Step 1400\n",
      "Training loss  0.795 in Step 1500\n",
      "Training loss  0.778 in Step 1600\n",
      "Training loss  0.807 in Step 1700\n",
      "※※※Training loss  0.845※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.780 in Step 0\n",
      "Valid loss  0.791 in Step 100\n",
      "Valid loss  0.785 in Step 200\n",
      "Valid loss  0.786 in Step 300\n",
      "Valid loss  0.792 in Step 400\n",
      "※※※Valid loss  0.795※※※\n",
      "Epoch 121\n",
      "Training loss  0.786 in Step 0\n",
      "Training loss  0.779 in Step 100\n",
      "Training loss  0.803 in Step 200\n",
      "Training loss  0.796 in Step 300\n",
      "Training loss  0.789 in Step 400\n",
      "Training loss  0.774 in Step 500\n",
      "Training loss  0.775 in Step 600\n",
      "Training loss  0.815 in Step 700\n",
      "Training loss  0.769 in Step 800\n",
      "Training loss  0.797 in Step 900\n",
      "Training loss  0.794 in Step 1000\n",
      "Training loss  0.806 in Step 1100\n",
      "Training loss  0.846 in Step 1200\n",
      "Training loss  0.813 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.797 in Step 1500\n",
      "Training loss  0.787 in Step 1600\n",
      "Training loss  0.779 in Step 1700\n",
      "※※※Training loss  0.801※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.780 in Step 0\n",
      "Valid loss  0.787 in Step 100\n",
      "Valid loss  0.786 in Step 200\n",
      "Valid loss  0.786 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.794※※※\n",
      "Epoch 122\n",
      "Training loss  0.778 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.778 in Step 200\n",
      "Training loss  0.800 in Step 300\n",
      "Training loss  0.799 in Step 400\n",
      "Training loss  0.778 in Step 500\n",
      "Training loss  0.778 in Step 600\n",
      "Training loss  0.790 in Step 700\n",
      "Training loss  0.775 in Step 800\n",
      "Training loss  0.785 in Step 900\n",
      "Training loss  0.802 in Step 1000\n",
      "Training loss  0.795 in Step 1100\n",
      "Training loss  0.785 in Step 1200\n",
      "Training loss  0.798 in Step 1300\n",
      "Training loss  0.802 in Step 1400\n",
      "Training loss  0.786 in Step 1500\n",
      "Training loss  0.776 in Step 1600\n",
      "Training loss  0.898 in Step 1700\n",
      "※※※Training loss  0.795※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.820 in Step 0\n",
      "Valid loss  0.831 in Step 100\n",
      "Valid loss  0.830 in Step 200\n",
      "Valid loss  0.832 in Step 300\n",
      "Valid loss  0.829 in Step 400\n",
      "※※※Valid loss  0.836※※※\n",
      "Epoch 123\n",
      "Training loss  0.839 in Step 0\n",
      "Training loss  0.803 in Step 100\n",
      "Training loss  0.812 in Step 200\n",
      "Training loss  0.792 in Step 300\n",
      "Training loss  0.795 in Step 400\n",
      "Training loss  0.796 in Step 500\n",
      "Training loss  0.811 in Step 600\n",
      "Training loss  0.790 in Step 700\n",
      "Training loss  0.799 in Step 800\n",
      "Training loss  0.803 in Step 900\n",
      "Training loss  0.779 in Step 1000\n",
      "Training loss  0.789 in Step 1100\n",
      "Training loss  0.802 in Step 1200\n",
      "Training loss  0.783 in Step 1300\n",
      "Training loss  0.802 in Step 1400\n",
      "Training loss  0.791 in Step 1500\n",
      "Training loss  0.799 in Step 1600\n",
      "Training loss  0.791 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.777 in Step 0\n",
      "Valid loss  0.783 in Step 100\n",
      "Valid loss  0.781 in Step 200\n",
      "Valid loss  0.784 in Step 300\n",
      "Valid loss  0.788 in Step 400\n",
      "※※※Valid loss  0.791※※※\n",
      "Epoch 124\n",
      "Training loss  0.783 in Step 0\n",
      "Training loss  0.805 in Step 100\n",
      "Training loss  0.787 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.789 in Step 400\n",
      "Training loss  0.793 in Step 500\n",
      "Training loss  0.814 in Step 600\n",
      "Training loss  0.796 in Step 700\n",
      "Training loss  0.783 in Step 800\n",
      "Training loss  0.804 in Step 900\n",
      "Training loss  0.796 in Step 1000\n",
      "Training loss  0.792 in Step 1100\n",
      "Training loss  0.788 in Step 1200\n",
      "Training loss  0.789 in Step 1300\n",
      "Training loss  0.783 in Step 1400\n",
      "Training loss  0.770 in Step 1500\n",
      "Training loss  0.791 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.791※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.779 in Step 0\n",
      "Valid loss  0.786 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.790 in Step 400\n",
      "※※※Valid loss  0.792※※※\n",
      "Epoch 125\n",
      "Training loss  0.803 in Step 0\n",
      "Training loss  0.817 in Step 100\n",
      "Training loss  0.792 in Step 200\n",
      "Training loss  0.784 in Step 300\n",
      "Training loss  0.798 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.800 in Step 700\n",
      "Training loss  0.781 in Step 800\n",
      "Training loss  0.802 in Step 900\n",
      "Training loss  0.797 in Step 1000\n",
      "Training loss  0.807 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.775 in Step 1300\n",
      "Training loss  0.790 in Step 1400\n",
      "Training loss  0.787 in Step 1500\n",
      "Training loss  0.769 in Step 1600\n",
      "Training loss  0.815 in Step 1700\n",
      "※※※Training loss  0.791※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.781 in Step 0\n",
      "Valid loss  0.784 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.784 in Step 300\n",
      "Valid loss  0.789 in Step 400\n",
      "※※※Valid loss  0.792※※※\n",
      "Epoch 126\n",
      "Training loss  0.775 in Step 0\n",
      "Training loss  0.793 in Step 100\n",
      "Training loss  0.816 in Step 200\n",
      "Training loss  0.797 in Step 300\n",
      "Training loss  0.808 in Step 400\n",
      "Training loss  0.790 in Step 500\n",
      "Training loss  0.782 in Step 600\n",
      "Training loss  0.788 in Step 700\n",
      "Training loss  0.787 in Step 800\n",
      "Training loss  0.786 in Step 900\n",
      "Training loss  0.786 in Step 1000\n",
      "Training loss  0.798 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.788 in Step 1400\n",
      "Training loss  0.783 in Step 1500\n",
      "Training loss  0.788 in Step 1600\n",
      "Training loss  0.797 in Step 1700\n",
      "※※※Training loss  0.794※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.780 in Step 0\n",
      "Valid loss  0.792 in Step 100\n",
      "Valid loss  0.784 in Step 200\n",
      "Valid loss  0.784 in Step 300\n",
      "Valid loss  0.792 in Step 400\n",
      "※※※Valid loss  0.793※※※\n",
      "Epoch 127\n",
      "Training loss  0.788 in Step 0\n",
      "Training loss  0.816 in Step 100\n",
      "Training loss  0.793 in Step 200\n",
      "Training loss  0.811 in Step 300\n",
      "Training loss  0.778 in Step 400\n",
      "Training loss  0.797 in Step 500\n",
      "Training loss  0.782 in Step 600\n",
      "Training loss  0.813 in Step 700\n",
      "Training loss  0.804 in Step 800\n",
      "Training loss  0.795 in Step 900\n",
      "Training loss  0.779 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.832 in Step 1300\n",
      "Training loss  0.806 in Step 1400\n",
      "Training loss  0.795 in Step 1500\n",
      "Training loss  0.787 in Step 1600\n",
      "Training loss  0.785 in Step 1700\n",
      "※※※Training loss  0.796※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.779 in Step 0\n",
      "Valid loss  0.785 in Step 100\n",
      "Valid loss  0.781 in Step 200\n",
      "Valid loss  0.784 in Step 300\n",
      "Valid loss  0.789 in Step 400\n",
      "※※※Valid loss  0.792※※※\n",
      "Epoch 128\n",
      "Training loss  0.781 in Step 0\n",
      "Training loss  0.791 in Step 100\n",
      "Training loss  0.821 in Step 200\n",
      "Training loss  0.792 in Step 300\n",
      "Training loss  0.818 in Step 400\n",
      "Training loss  0.803 in Step 500\n",
      "Training loss  0.785 in Step 600\n",
      "Training loss  0.788 in Step 700\n",
      "Training loss  0.802 in Step 800\n",
      "Training loss  0.791 in Step 900\n",
      "Training loss  0.780 in Step 1000\n",
      "Training loss  0.796 in Step 1100\n",
      "Training loss  0.788 in Step 1200\n",
      "Training loss  0.790 in Step 1300\n",
      "Training loss  0.791 in Step 1400\n",
      "Training loss  0.788 in Step 1500\n",
      "Training loss  0.777 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.793※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.776 in Step 0\n",
      "Valid loss  0.784 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.787 in Step 400\n",
      "※※※Valid loss  0.790※※※\n",
      "Epoch 129\n",
      "Training loss  0.787 in Step 0\n",
      "Training loss  0.802 in Step 100\n",
      "Training loss  0.786 in Step 200\n",
      "Training loss  0.771 in Step 300\n",
      "Training loss  0.785 in Step 400\n",
      "Training loss  0.784 in Step 500\n",
      "Training loss  0.792 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.791 in Step 800\n",
      "Training loss  0.790 in Step 900\n",
      "Training loss  0.789 in Step 1000\n",
      "Training loss  0.796 in Step 1100\n",
      "Training loss  0.787 in Step 1200\n",
      "Training loss  0.799 in Step 1300\n",
      "Training loss  0.812 in Step 1400\n",
      "Training loss  0.806 in Step 1500\n",
      "Training loss  0.786 in Step 1600\n",
      "Training loss  0.806 in Step 1700\n",
      "※※※Training loss  0.789※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.781 in Step 0\n",
      "Valid loss  0.792 in Step 100\n",
      "Valid loss  0.788 in Step 200\n",
      "Valid loss  0.793 in Step 300\n",
      "Valid loss  0.794 in Step 400\n",
      "※※※Valid loss  0.796※※※\n",
      "Epoch 130\n",
      "Training loss  0.792 in Step 0\n",
      "Training loss  0.788 in Step 100\n",
      "Training loss  0.780 in Step 200\n",
      "Training loss  0.781 in Step 300\n",
      "Training loss  0.794 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.793 in Step 600\n",
      "Training loss  0.773 in Step 700\n",
      "Training loss  0.790 in Step 800\n",
      "Training loss  0.801 in Step 900\n",
      "Training loss  0.796 in Step 1000\n",
      "Training loss  0.784 in Step 1100\n",
      "Training loss  0.797 in Step 1200\n",
      "Training loss  0.788 in Step 1300\n",
      "Training loss  0.774 in Step 1400\n",
      "Training loss  0.788 in Step 1500\n",
      "Training loss  0.795 in Step 1600\n",
      "Training loss  0.779 in Step 1700\n",
      "※※※Training loss  0.788※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.776 in Step 0\n",
      "Valid loss  0.785 in Step 100\n",
      "Valid loss  0.781 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.785 in Step 400\n",
      "※※※Valid loss  0.788※※※\n",
      "Epoch 131\n",
      "Training loss  0.785 in Step 0\n",
      "Training loss  0.861 in Step 100\n",
      "Training loss  0.799 in Step 200\n",
      "Training loss  0.774 in Step 300\n",
      "Training loss  0.824 in Step 400\n",
      "Training loss  0.792 in Step 500\n",
      "Training loss  0.792 in Step 600\n",
      "Training loss  0.786 in Step 700\n",
      "Training loss  0.784 in Step 800\n",
      "Training loss  0.791 in Step 900\n",
      "Training loss  0.796 in Step 1000\n",
      "Training loss  0.790 in Step 1100\n",
      "Training loss  0.776 in Step 1200\n",
      "Training loss  0.797 in Step 1300\n",
      "Training loss  0.786 in Step 1400\n",
      "Training loss  0.798 in Step 1500\n",
      "Training loss  0.786 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.790※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.776 in Step 0\n",
      "Valid loss  0.784 in Step 100\n",
      "Valid loss  0.780 in Step 200\n",
      "Valid loss  0.781 in Step 300\n",
      "Valid loss  0.784 in Step 400\n",
      "※※※Valid loss  0.789※※※\n",
      "Epoch 132\n",
      "Training loss  0.790 in Step 0\n",
      "Training loss  0.781 in Step 100\n",
      "Training loss  0.771 in Step 200\n",
      "Training loss  0.780 in Step 300\n",
      "Training loss  0.784 in Step 400\n",
      "Training loss  0.815 in Step 500\n",
      "Training loss  0.776 in Step 600\n",
      "Training loss  0.789 in Step 700\n",
      "Training loss  0.798 in Step 800\n",
      "Training loss  0.786 in Step 900\n",
      "Training loss  0.797 in Step 1000\n",
      "Training loss  0.783 in Step 1100\n",
      "Training loss  0.806 in Step 1200\n",
      "Training loss  0.792 in Step 1300\n",
      "Training loss  0.794 in Step 1400\n",
      "Training loss  0.795 in Step 1500\n",
      "Training loss  0.805 in Step 1600\n",
      "Training loss  0.791 in Step 1700\n",
      "※※※Training loss  0.788※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.779 in Step 0\n",
      "Valid loss  0.782 in Step 100\n",
      "Valid loss  0.778 in Step 200\n",
      "Valid loss  0.781 in Step 300\n",
      "Valid loss  0.785 in Step 400\n",
      "※※※Valid loss  0.788※※※\n",
      "Epoch 133\n",
      "Training loss  0.786 in Step 0\n",
      "Training loss  0.784 in Step 100\n",
      "Training loss  0.795 in Step 200\n",
      "Training loss  0.791 in Step 300\n",
      "Training loss  0.784 in Step 400\n",
      "Training loss  0.787 in Step 500\n",
      "Training loss  0.782 in Step 600\n",
      "Training loss  0.801 in Step 700\n",
      "Training loss  0.797 in Step 800\n",
      "Training loss  0.797 in Step 900\n",
      "Training loss  0.788 in Step 1000\n",
      "Training loss  0.788 in Step 1100\n",
      "Training loss  0.797 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.804 in Step 1400\n",
      "Training loss  0.764 in Step 1500\n",
      "Training loss  0.774 in Step 1600\n",
      "Training loss  0.794 in Step 1700\n",
      "※※※Training loss  0.788※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.784 in Step 100\n",
      "Valid loss  0.782 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.787 in Step 400\n",
      "※※※Valid loss  0.790※※※\n",
      "Epoch 134\n",
      "Training loss  0.795 in Step 0\n",
      "Training loss  0.784 in Step 100\n",
      "Training loss  0.776 in Step 200\n",
      "Training loss  0.808 in Step 300\n",
      "Training loss  0.785 in Step 400\n",
      "Training loss  0.790 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.794 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.789 in Step 900\n",
      "Training loss  0.798 in Step 1000\n",
      "Training loss  0.776 in Step 1100\n",
      "Training loss  0.793 in Step 1200\n",
      "Training loss  0.813 in Step 1300\n",
      "Training loss  0.765 in Step 1400\n",
      "Training loss  0.779 in Step 1500\n",
      "Training loss  0.777 in Step 1600\n",
      "Training loss  0.806 in Step 1700\n",
      "※※※Training loss  0.787※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.782 in Step 100\n",
      "Valid loss  0.779 in Step 200\n",
      "Valid loss  0.781 in Step 300\n",
      "Valid loss  0.782 in Step 400\n",
      "※※※Valid loss  0.788※※※\n",
      "Epoch 135\n",
      "Training loss  0.799 in Step 0\n",
      "Training loss  0.799 in Step 100\n",
      "Training loss  0.772 in Step 200\n",
      "Training loss  0.785 in Step 300\n",
      "Training loss  0.794 in Step 400\n",
      "Training loss  0.800 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.791 in Step 700\n",
      "Training loss  0.788 in Step 800\n",
      "Training loss  0.797 in Step 900\n",
      "Training loss  0.784 in Step 1000\n",
      "Training loss  0.796 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.796 in Step 1300\n",
      "Training loss  0.788 in Step 1400\n",
      "Training loss  0.792 in Step 1500\n",
      "Training loss  0.784 in Step 1600\n",
      "Training loss  0.770 in Step 1700\n",
      "※※※Training loss  0.787※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.778 in Step 0\n",
      "Valid loss  0.788 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.785 in Step 400\n",
      "※※※Valid loss  0.791※※※\n",
      "Epoch 136\n",
      "Training loss  0.771 in Step 0\n",
      "Training loss  0.782 in Step 100\n",
      "Training loss  0.792 in Step 200\n",
      "Training loss  0.782 in Step 300\n",
      "Training loss  0.787 in Step 400\n",
      "Training loss  0.789 in Step 500\n",
      "Training loss  0.788 in Step 600\n",
      "Training loss  0.780 in Step 700\n",
      "Training loss  0.798 in Step 800\n",
      "Training loss  0.781 in Step 900\n",
      "Training loss  0.786 in Step 1000\n",
      "Training loss  0.784 in Step 1100\n",
      "Training loss  0.785 in Step 1200\n",
      "Training loss  0.785 in Step 1300\n",
      "Training loss  0.793 in Step 1400\n",
      "Training loss  0.794 in Step 1500\n",
      "Training loss  0.790 in Step 1600\n",
      "Training loss  0.775 in Step 1700\n",
      "※※※Training loss  0.787※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.777 in Step 0\n",
      "Valid loss  0.784 in Step 100\n",
      "Valid loss  0.780 in Step 200\n",
      "Valid loss  0.780 in Step 300\n",
      "Valid loss  0.785 in Step 400\n",
      "※※※Valid loss  0.788※※※\n",
      "Epoch 137\n",
      "Training loss  0.777 in Step 0\n",
      "Training loss  0.780 in Step 100\n",
      "Training loss  0.794 in Step 200\n",
      "Training loss  0.784 in Step 300\n",
      "Training loss  0.791 in Step 400\n",
      "Training loss  0.785 in Step 500\n",
      "Training loss  0.781 in Step 600\n",
      "Training loss  0.794 in Step 700\n",
      "Training loss  0.786 in Step 800\n",
      "Training loss  0.782 in Step 900\n",
      "Training loss  0.793 in Step 1000\n",
      "Training loss  0.788 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.768 in Step 1300\n",
      "Training loss  0.787 in Step 1400\n",
      "Training loss  0.770 in Step 1500\n",
      "Training loss  0.769 in Step 1600\n",
      "Training loss  0.773 in Step 1700\n",
      "※※※Training loss  0.786※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.774 in Step 0\n",
      "Valid loss  0.782 in Step 100\n",
      "Valid loss  0.778 in Step 200\n",
      "Valid loss  0.780 in Step 300\n",
      "Valid loss  0.784 in Step 400\n",
      "※※※Valid loss  0.787※※※\n",
      "Epoch 138\n",
      "Training loss  0.792 in Step 0\n",
      "Training loss  0.780 in Step 100\n",
      "Training loss  0.779 in Step 200\n",
      "Training loss  0.770 in Step 300\n",
      "Training loss  0.783 in Step 400\n",
      "Training loss  0.792 in Step 500\n",
      "Training loss  0.790 in Step 600\n",
      "Training loss  0.784 in Step 700\n",
      "Training loss  0.794 in Step 800\n",
      "Training loss  0.784 in Step 900\n",
      "Training loss  0.790 in Step 1000\n",
      "Training loss  0.783 in Step 1100\n",
      "Training loss  0.797 in Step 1200\n",
      "Training loss  0.784 in Step 1300\n",
      "Training loss  0.791 in Step 1400\n",
      "Training loss  0.801 in Step 1500\n",
      "Training loss  0.789 in Step 1600\n",
      "Training loss  0.773 in Step 1700\n",
      "※※※Training loss  0.787※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.784 in Step 100\n",
      "Valid loss  0.778 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.781 in Step 400\n",
      "※※※Valid loss  0.788※※※\n",
      "Epoch 139\n",
      "Training loss  0.793 in Step 0\n",
      "Training loss  0.785 in Step 100\n",
      "Training loss  0.775 in Step 200\n",
      "Training loss  0.804 in Step 300\n",
      "Training loss  0.792 in Step 400\n",
      "Training loss  0.786 in Step 500\n",
      "Training loss  0.788 in Step 600\n",
      "Training loss  0.780 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.769 in Step 900\n",
      "Training loss  0.788 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.776 in Step 1200\n",
      "Training loss  0.790 in Step 1300\n",
      "Training loss  0.772 in Step 1400\n",
      "Training loss  0.775 in Step 1500\n",
      "Training loss  0.797 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.786※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.783 in Step 100\n",
      "Valid loss  0.780 in Step 200\n",
      "Valid loss  0.781 in Step 300\n",
      "Valid loss  0.784 in Step 400\n",
      "※※※Valid loss  0.788※※※\n",
      "Epoch 140\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.771 in Step 100\n",
      "Training loss  0.774 in Step 200\n",
      "Training loss  0.807 in Step 300\n",
      "Training loss  0.791 in Step 400\n",
      "Training loss  0.767 in Step 500\n",
      "Training loss  0.786 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.794 in Step 800\n",
      "Training loss  0.783 in Step 900\n",
      "Training loss  0.792 in Step 1000\n",
      "Training loss  0.792 in Step 1100\n",
      "Training loss  0.782 in Step 1200\n",
      "Training loss  0.797 in Step 1300\n",
      "Training loss  0.798 in Step 1400\n",
      "Training loss  0.774 in Step 1500\n",
      "Training loss  0.802 in Step 1600\n",
      "Training loss  0.801 in Step 1700\n",
      "※※※Training loss  0.788※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.774 in Step 0\n",
      "Valid loss  0.791 in Step 100\n",
      "Valid loss  0.779 in Step 200\n",
      "Valid loss  0.779 in Step 300\n",
      "Valid loss  0.782 in Step 400\n",
      "※※※Valid loss  0.787※※※\n",
      "Epoch 141\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.785 in Step 100\n",
      "Training loss  0.782 in Step 200\n",
      "Training loss  0.789 in Step 300\n",
      "Training loss  0.790 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.787 in Step 600\n",
      "Training loss  0.797 in Step 700\n",
      "Training loss  0.786 in Step 800\n",
      "Training loss  0.773 in Step 900\n",
      "Training loss  0.813 in Step 1000\n",
      "Training loss  0.790 in Step 1100\n",
      "Training loss  0.774 in Step 1200\n",
      "Training loss  0.778 in Step 1300\n",
      "Training loss  0.781 in Step 1400\n",
      "Training loss  0.788 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.766 in Step 1700\n",
      "※※※Training loss  0.787※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.785 in Step 100\n",
      "Valid loss  0.783 in Step 200\n",
      "Valid loss  0.782 in Step 300\n",
      "Valid loss  0.785 in Step 400\n",
      "※※※Valid loss  0.789※※※\n",
      "Epoch 142\n",
      "Training loss  0.774 in Step 0\n",
      "Training loss  0.765 in Step 100\n",
      "Training loss  0.799 in Step 200\n",
      "Training loss  0.785 in Step 300\n",
      "Training loss  0.770 in Step 400\n",
      "Training loss  0.781 in Step 500\n",
      "Training loss  0.778 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.799 in Step 800\n",
      "Training loss  0.794 in Step 900\n",
      "Training loss  0.800 in Step 1000\n",
      "Training loss  0.793 in Step 1100\n",
      "Training loss  0.780 in Step 1200\n",
      "Training loss  0.800 in Step 1300\n",
      "Training loss  0.784 in Step 1400\n",
      "Training loss  0.798 in Step 1500\n",
      "Training loss  0.773 in Step 1600\n",
      "Training loss  0.777 in Step 1700\n",
      "※※※Training loss  0.786※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.773 in Step 0\n",
      "Valid loss  0.783 in Step 100\n",
      "Valid loss  0.780 in Step 200\n",
      "Valid loss  0.777 in Step 300\n",
      "Valid loss  0.784 in Step 400\n",
      "※※※Valid loss  0.787※※※\n",
      "Epoch 143\n",
      "Training loss  0.776 in Step 0\n",
      "Training loss  0.785 in Step 100\n",
      "Training loss  0.787 in Step 200\n",
      "Training loss  0.779 in Step 300\n",
      "Training loss  0.777 in Step 400\n",
      "Training loss  0.794 in Step 500\n",
      "Training loss  0.784 in Step 600\n",
      "Training loss  0.772 in Step 700\n",
      "Training loss  0.773 in Step 800\n",
      "Training loss  0.793 in Step 900\n",
      "Training loss  0.786 in Step 1000\n",
      "Training loss  0.792 in Step 1100\n",
      "Training loss  0.761 in Step 1200\n",
      "Training loss  0.790 in Step 1300\n",
      "Training loss  0.784 in Step 1400\n",
      "Training loss  0.782 in Step 1500\n",
      "Training loss  0.784 in Step 1600\n",
      "Training loss  0.790 in Step 1700\n",
      "※※※Training loss  0.786※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.772 in Step 0\n",
      "Valid loss  0.781 in Step 100\n",
      "Valid loss  0.777 in Step 200\n",
      "Valid loss  0.779 in Step 300\n",
      "Valid loss  0.781 in Step 400\n",
      "※※※Valid loss  0.785※※※\n",
      "Epoch 144\n",
      "Training loss  0.786 in Step 0\n",
      "Training loss  0.782 in Step 100\n",
      "Training loss  0.808 in Step 200\n",
      "Training loss  0.786 in Step 300\n",
      "Training loss  0.788 in Step 400\n",
      "Training loss  0.781 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.805 in Step 700\n",
      "Training loss  0.756 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.806 in Step 1000\n",
      "Training loss  0.805 in Step 1100\n",
      "Training loss  0.782 in Step 1200\n",
      "Training loss  0.795 in Step 1300\n",
      "Training loss  0.797 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.795 in Step 1700\n",
      "※※※Training loss  0.785※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.781 in Step 100\n",
      "Valid loss  0.778 in Step 200\n",
      "Valid loss  0.779 in Step 300\n",
      "Valid loss  0.781 in Step 400\n",
      "※※※Valid loss  0.787※※※\n",
      "Epoch 145\n",
      "Training loss  0.781 in Step 0\n",
      "Training loss  0.785 in Step 100\n",
      "Training loss  0.775 in Step 200\n",
      "Training loss  0.808 in Step 300\n",
      "Training loss  0.766 in Step 400\n",
      "Training loss  0.785 in Step 500\n",
      "Training loss  0.774 in Step 600\n",
      "Training loss  0.799 in Step 700\n",
      "Training loss  0.799 in Step 800\n",
      "Training loss  0.794 in Step 900\n",
      "Training loss  0.783 in Step 1000\n",
      "Training loss  0.788 in Step 1100\n",
      "Training loss  0.782 in Step 1200\n",
      "Training loss  0.796 in Step 1300\n",
      "Training loss  0.792 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.794 in Step 1600\n",
      "Training loss  0.791 in Step 1700\n",
      "※※※Training loss  0.785※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.772 in Step 0\n",
      "Valid loss  0.779 in Step 100\n",
      "Valid loss  0.778 in Step 200\n",
      "Valid loss  0.779 in Step 300\n",
      "Valid loss  0.780 in Step 400\n",
      "※※※Valid loss  0.785※※※\n",
      "Epoch 146\n",
      "Training loss  0.763 in Step 0\n",
      "Training loss  0.796 in Step 100\n",
      "Training loss  0.789 in Step 200\n",
      "Training loss  0.783 in Step 300\n",
      "Training loss  0.787 in Step 400\n",
      "Training loss  0.798 in Step 500\n",
      "Training loss  0.762 in Step 600\n",
      "Training loss  0.771 in Step 700\n",
      "Training loss  0.779 in Step 800\n",
      "Training loss  0.785 in Step 900\n",
      "Training loss  0.782 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.796 in Step 1200\n",
      "Training loss  0.787 in Step 1300\n",
      "Training loss  0.779 in Step 1400\n",
      "Training loss  0.790 in Step 1500\n",
      "Training loss  0.795 in Step 1600\n",
      "Training loss  0.795 in Step 1700\n",
      "※※※Training loss  0.785※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.772 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.778 in Step 200\n",
      "Valid loss  0.779 in Step 300\n",
      "Valid loss  0.780 in Step 400\n",
      "※※※Valid loss  0.785※※※\n",
      "Epoch 147\n",
      "Training loss  0.754 in Step 0\n",
      "Training loss  0.765 in Step 100\n",
      "Training loss  0.798 in Step 200\n",
      "Training loss  0.780 in Step 300\n",
      "Training loss  0.791 in Step 400\n",
      "Training loss  0.796 in Step 500\n",
      "Training loss  0.805 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.799 in Step 800\n",
      "Training loss  0.781 in Step 900\n",
      "Training loss  0.775 in Step 1000\n",
      "Training loss  0.788 in Step 1100\n",
      "Training loss  0.785 in Step 1200\n",
      "Training loss  0.792 in Step 1300\n",
      "Training loss  0.774 in Step 1400\n",
      "Training loss  0.787 in Step 1500\n",
      "Training loss  0.776 in Step 1600\n",
      "Training loss  0.786 in Step 1700\n",
      "※※※Training loss  0.786※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.779 in Step 100\n",
      "Valid loss  0.777 in Step 200\n",
      "Valid loss  0.778 in Step 300\n",
      "Valid loss  0.779 in Step 400\n",
      "※※※Valid loss  0.785※※※\n",
      "Epoch 148\n",
      "Training loss  0.768 in Step 0\n",
      "Training loss  0.800 in Step 100\n",
      "Training loss  0.784 in Step 200\n",
      "Training loss  0.793 in Step 300\n",
      "Training loss  0.787 in Step 400\n",
      "Training loss  0.786 in Step 500\n",
      "Training loss  0.782 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.797 in Step 800\n",
      "Training loss  0.785 in Step 900\n",
      "Training loss  0.789 in Step 1000\n",
      "Training loss  0.777 in Step 1100\n",
      "Training loss  0.786 in Step 1200\n",
      "Training loss  0.777 in Step 1300\n",
      "Training loss  0.784 in Step 1400\n",
      "Training loss  0.786 in Step 1500\n",
      "Training loss  0.787 in Step 1600\n",
      "Training loss  0.778 in Step 1700\n",
      "※※※Training loss  0.785※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.780 in Step 0\n",
      "Valid loss  0.782 in Step 100\n",
      "Valid loss  0.777 in Step 200\n",
      "Valid loss  0.780 in Step 300\n",
      "Valid loss  0.781 in Step 400\n",
      "※※※Valid loss  0.786※※※\n",
      "Epoch 149\n",
      "Training loss  0.796 in Step 0\n",
      "Training loss  0.783 in Step 100\n",
      "Training loss  0.776 in Step 200\n",
      "Training loss  0.794 in Step 300\n",
      "Training loss  0.788 in Step 400\n",
      "Training loss  0.792 in Step 500\n",
      "Training loss  0.784 in Step 600\n",
      "Training loss  0.772 in Step 700\n",
      "Training loss  1.095 in Step 800\n",
      "Training loss  1.305 in Step 900\n",
      "Training loss  1.067 in Step 1000\n",
      "Training loss  0.982 in Step 1100\n",
      "Training loss  0.911 in Step 1200\n",
      "Training loss  0.874 in Step 1300\n",
      "Training loss  0.828 in Step 1400\n",
      "Training loss  0.846 in Step 1500\n",
      "Training loss  0.822 in Step 1600\n",
      "Training loss  0.803 in Step 1700\n",
      "※※※Training loss  0.877※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.805 in Step 0\n",
      "Valid loss  0.803 in Step 100\n",
      "Valid loss  0.798 in Step 200\n",
      "Valid loss  0.800 in Step 300\n",
      "Valid loss  0.807 in Step 400\n",
      "※※※Valid loss  0.810※※※\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "train_losses.save()\n",
    "\n",
    "valid_losses.save()\n",
    "\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f98e3c14910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0908015948'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(filename, my_list):\n",
    "    try:\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(my_list, file)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the list: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_list_to_file(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)), valid_ds.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            my_list = pickle.load(file)\n",
    "        return my_list\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the list: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = read_list_from_file(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56808"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryset1 = torch.utils.data.Subset(ds, vds)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
