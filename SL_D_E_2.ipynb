{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Direct - English\n",
    "Version 1: In this version we make the model \"simple\": make the encoder RNN into normal RNN first and try to see the result.  \n",
    "Version 2: Learning is not very much. Following Dr Coupe's advice we try simpler model structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import pickle\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from recorder import *\n",
    "from loss import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PhxLearner, SimplerPhxLearner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规范用语；规定两种方式：全加载；按rec加载（舍弃了按chunk加载，处理起来更简单）\n",
    "# RandomPhoneDataset; AnnoPhoneDataset; AnnoSeqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset that loads cutted wave files from disk and returns input-output pairs for\n",
    "    training autoencoder. \n",
    "    \n",
    "    Version 3: wav -> mel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load_dir, load_control_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the class by reading a CSV file and merging the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        The function reads the CSV file from the provided control path, extracts the \"rec\" and \"idx\" columns,\n",
    "        and concatenates the values from these columns using an underscore. It then appends the \".wav\" extension\n",
    "        to each of the merged strings and converts the merged pandas Series to a list, which is assigned to\n",
    "        the 'dataset' attribute of the class.\n",
    "\n",
    "        Args:\n",
    "        load_dir (str): The directory containing the files to load.\n",
    "        load_control_path (str): The path to the CSV file containing the \"rec\" and \"idx\" columns.\n",
    "\n",
    "        Attributes:\n",
    "        dataset (list): A list of merged strings from the \"rec\" and \"idx\" columns, with the \".wav\" extension.\n",
    "        \"\"\"\n",
    "        control_file = pd.read_csv(load_control_path)\n",
    "        control_file = control_file[control_file['n_frames'] > 400]\n",
    "        control_file = control_file[control_file['duration'] <= 2.0]\n",
    "        \n",
    "        # Extract the \"rec\" and \"idx\" columns\n",
    "        rec_col = control_file['rec'].astype(str)\n",
    "        idx_col = control_file['idx'].astype(str).str.zfill(8)\n",
    "        \n",
    "        # Merge the two columns by concatenating the strings with '_' and append extension name\n",
    "        merged_col = rec_col + '_' + idx_col + \".wav\"\n",
    "        \n",
    "        self.dataset = merged_col.tolist()\n",
    "        self.load_dir = load_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of input-output pairs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (input_data, output_data) for the given index.\n",
    "\n",
    "        The function first checks if the provided index is a tensor, and if so, converts it to a list.\n",
    "        It then constructs the file path for the .wav file using the dataset attribute and the provided index.\n",
    "        The .wav file is loaded using torchaudio, and its data is normalized. If a transform is provided,\n",
    "        the data is transformed using the specified transform. Finally, the input_data and output_data are\n",
    "        set to the same data (creating a tuple), and the tuple is returned.\n",
    "\n",
    "        Args:\n",
    "        idx (int or torch.Tensor): The index of the desired data.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing input_data and output_data, both of which are the audio data\n",
    "               from the .wav file at the specified index.\n",
    "\n",
    "        Note: \n",
    "        This function assumes that the class has the following attributes:\n",
    "        - self.load_dir (str): The directory containing the .wav files.\n",
    "        - self.dataset (list): A list of .wav file names.\n",
    "        - self.transform (callable, optional): An optional transform to apply to the audio data.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        wav_name = os.path.join(self.load_dir,\n",
    "                                self.dataset[idx])\n",
    "        \n",
    "        data, sample_rate = torchaudio.load(wav_name, normalize=True)\n",
    "        if self.transform:\n",
    "            data = self.transform(data, sr=sample_rate)\n",
    "        \n",
    "        # # Prepare for possible in-out discrepencies in the future\n",
    "        # input_data = data\n",
    "        # output_data = data\n",
    "        \n",
    "        return data\n",
    "\n",
    "def collate_fn(xx):\n",
    "    # only working for one data at the moment\n",
    "    batch_first = True\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=batch_first, padding_value=0)\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "\n",
    "class MyTransform(nn.Module): \n",
    "    def __init__(self, sample_rate, n_fft): \n",
    "        super().__init__()\n",
    "        # self.transform = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft, n_mels=64)\n",
    "        # self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        # self.transform = torchaudio.transforms.MFCC(n_mfcc=13)\n",
    "    \n",
    "    def forward(self, waveform, sr=16000): \n",
    "        # extract mfcc\n",
    "        feature = torchaudio.compliance.kaldi.mfcc(waveform, sample_frequency=sr)\n",
    "\n",
    "        # add deltas\n",
    "        d1 = torchaudio.functional.compute_deltas(feature)\n",
    "        d2 = torchaudio.functional.compute_deltas(d1)\n",
    "        feature = torch.cat([feature, d1, d2], dim=-1)\n",
    "\n",
    "        # Apply normalization (CMVN)\n",
    "        eps = 1e-9\n",
    "        mean = feature.mean(0, keepdim=True)\n",
    "        std = feature.std(0, keepdim=True, unbiased=False)\n",
    "        # print(feature.shape)\n",
    "        # print(mean, std)\n",
    "        feature = (feature - mean) / (std + eps)\n",
    "\n",
    "        # mel_spec = self.transform(waveform)\n",
    "        # # mel_spec = self.to_db(mel_spec)\n",
    "        # mel_spec = mel_spec.squeeze()\n",
    "        # mel_spec = mel_spec.permute(1, 0) # (F, L) -> (L, F)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIM = 39\n",
    "OUTPUT_DIM = 13\n",
    "\n",
    "INTER_DIM_0 = 16\n",
    "INTER_DIM_1 = 8\n",
    "INTER_DIM_2 = 3\n",
    "# INTER_DIM_3 = 3\n",
    "\n",
    "ENC_SIZE_LIST = [INPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "DEC_SIZE_LIST = [OUTPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "\n",
    "LOADER_WORKER = 16\n",
    "# LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "masked_recon_loss = MaskedLoss(recon_loss)\n",
    "model_loss = masked_recon_loss\n",
    "\n",
    "# model = PhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=1)\n",
    "model = SimplerPhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=1)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplerPhxLearner(\n",
       "  (encoder): OnlyRNNEncoder(\n",
       "    (rnn): LSTM(39, 3, batch_first=True)\n",
       "    (act): Tanh()\n",
       "  )\n",
       "  (decoder): SimperRNNDecoder(\n",
       "    (lin_1): LinearPack(\n",
       "      (linear): Linear(in_features=13, out_features=16, bias=True)\n",
       "    )\n",
       "    (rnn): LSTM(16, 3, batch_first=True)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (lin_3): LinearPack(\n",
       "      (linear): Linear(in_features=3, out_features=13, bias=True)\n",
       "    )\n",
       "    (act): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "ts = str(get_timestamp())\n",
    "# ts = \"0623152604\"\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "# save_train1hist_name = \"train_hist_recon{}.hst\".format(ts)\n",
    "# save_train2hist_name = \"train_hist_reg{}.hst\".format(ts)\n",
    "\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)\n",
    "# save_val1hist_name = \"val_hist_recon{}.hst\".format(ts)\n",
    "# save_val2hist_name = \"val_hist_reg{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "# train_recon_losses = LossRecorder(model_save_dir + save_train1hist_name)\n",
    "# train_reg_losses = LossRecorder(model_save_dir + save_train2hist_name)\n",
    "\n",
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "# valid_recon_losses = LossRecorder(model_save_dir + save_val1hist_name)\n",
    "# valid_reg_losses = LossRecorder(model_save_dir + save_val2hist_name)\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "READ = False\n",
    "# READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    valid_losses.read()\n",
    "    train_losses.read()\n",
    "\n",
    "    # model_name = last_model_namec\n",
    "    model_name = \"PT_0623152604_29_full.pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "    model = PhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=1)\n",
    "    \n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [],
   "source": [
    "mytrans = MyTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT)\n",
    "ds = PhoneDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "\n",
    "# this is to reduce the size of the dataset when the training power is not sufficient\n",
    "small_len = int(0.1 * len(ds))\n",
    "other_len = len(ds) - small_len\n",
    "\n",
    "# # Randomly split the dataset into train and validation sets\n",
    "ds, other_ds = random_split(ds, [small_len, other_len])\n",
    "\n",
    "train_len = int(0.8 * len(ds))\n",
    "valid_len = len(ds) - train_len\n",
    "\n",
    "# Randomly split the dataset into train and validation sets\n",
    "train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "valid_num = len(valid_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BASE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y = x[:, :, :13]    # extract MFCC-only data\n",
    "            \n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            y = x[:, :, :13]    # extract MFCC-only data\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30\n",
      "Training loss  0.957 in Step 0\n",
      "Training loss  0.936 in Step 10\n",
      "Training loss  0.953 in Step 20\n",
      "Training loss  0.965 in Step 30\n",
      "Training loss  0.957 in Step 40\n",
      "Training loss  0.963 in Step 50\n",
      "Training loss  0.949 in Step 60\n",
      "Training loss  0.961 in Step 70\n",
      "Training loss  0.949 in Step 80\n",
      "Training loss  0.952 in Step 90\n",
      "Training loss  0.959 in Step 100\n",
      "Training loss  0.950 in Step 110\n",
      "Training loss  0.948 in Step 120\n",
      "Training loss  0.945 in Step 130\n",
      "Training loss  0.942 in Step 140\n",
      "Training loss  0.952 in Step 150\n",
      "Training loss  0.956 in Step 160\n",
      "Training loss  0.951 in Step 170\n",
      "※※※Training loss  0.952※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.955 in Step 0\n",
      "※※※Valid loss  0.954※※※\n",
      "Epoch 31\n",
      "Training loss  0.952 in Step 0\n",
      "Training loss  0.949 in Step 10\n",
      "Training loss  0.949 in Step 20\n",
      "Training loss  0.948 in Step 30\n",
      "Training loss  0.959 in Step 40\n",
      "Training loss  0.946 in Step 50\n",
      "Training loss  0.961 in Step 60\n",
      "Training loss  0.955 in Step 70\n",
      "Training loss  0.963 in Step 80\n",
      "Training loss  0.957 in Step 90\n",
      "Training loss  0.956 in Step 100\n",
      "Training loss  0.959 in Step 110\n",
      "Training loss  0.944 in Step 120\n",
      "Training loss  0.948 in Step 130\n",
      "Training loss  0.956 in Step 140\n",
      "Training loss  0.951 in Step 150\n",
      "Training loss  0.950 in Step 160\n",
      "Training loss  0.958 in Step 170\n",
      "※※※Training loss  0.952※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.955 in Step 0\n",
      "※※※Valid loss  0.953※※※\n",
      "Epoch 32\n",
      "Training loss  0.953 in Step 0\n",
      "Training loss  0.952 in Step 10\n",
      "Training loss  0.945 in Step 20\n",
      "Training loss  0.942 in Step 30\n",
      "Training loss  0.951 in Step 40\n",
      "Training loss  0.945 in Step 50\n",
      "Training loss  0.957 in Step 60\n",
      "Training loss  0.964 in Step 70\n",
      "Training loss  0.952 in Step 80\n",
      "Training loss  0.951 in Step 90\n",
      "Training loss  0.941 in Step 100\n",
      "Training loss  0.950 in Step 110\n",
      "Training loss  0.957 in Step 120\n",
      "Training loss  0.956 in Step 130\n",
      "Training loss  0.956 in Step 140\n",
      "Training loss  0.945 in Step 150\n",
      "Training loss  0.957 in Step 160\n",
      "Training loss  0.952 in Step 170\n",
      "※※※Training loss  0.951※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.954 in Step 0\n",
      "※※※Valid loss  0.952※※※\n",
      "Epoch 33\n",
      "Training loss  0.955 in Step 0\n",
      "Training loss  0.949 in Step 10\n",
      "Training loss  0.952 in Step 20\n",
      "Training loss  0.946 in Step 30\n",
      "Training loss  0.954 in Step 40\n",
      "Training loss  0.956 in Step 50\n",
      "Training loss  0.956 in Step 60\n",
      "Training loss  0.954 in Step 70\n",
      "Training loss  0.959 in Step 80\n",
      "Training loss  0.950 in Step 90\n",
      "Training loss  0.949 in Step 100\n",
      "Training loss  0.947 in Step 110\n",
      "Training loss  0.956 in Step 120\n",
      "Training loss  0.958 in Step 130\n",
      "Training loss  0.936 in Step 140\n",
      "Training loss  0.959 in Step 150\n",
      "Training loss  0.959 in Step 160\n",
      "Training loss  0.940 in Step 170\n",
      "※※※Training loss  0.951※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.954 in Step 0\n",
      "※※※Valid loss  0.952※※※\n",
      "Epoch 34\n",
      "Training loss  0.947 in Step 0\n",
      "Training loss  0.954 in Step 10\n",
      "Training loss  0.956 in Step 20\n",
      "Training loss  0.961 in Step 30\n",
      "Training loss  0.947 in Step 40\n",
      "Training loss  0.941 in Step 50\n",
      "Training loss  0.962 in Step 60\n",
      "Training loss  0.946 in Step 70\n",
      "Training loss  0.944 in Step 80\n",
      "Training loss  0.950 in Step 90\n",
      "Training loss  0.952 in Step 100\n",
      "Training loss  0.958 in Step 110\n",
      "Training loss  0.956 in Step 120\n",
      "Training loss  0.959 in Step 130\n",
      "Training loss  0.949 in Step 140\n",
      "Training loss  0.940 in Step 150\n",
      "Training loss  0.947 in Step 160\n",
      "Training loss  0.950 in Step 170\n",
      "※※※Training loss  0.951※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.954 in Step 0\n",
      "※※※Valid loss  0.953※※※\n",
      "Epoch 35\n",
      "Training loss  0.944 in Step 0\n",
      "Training loss  0.953 in Step 10\n",
      "Training loss  0.940 in Step 20\n",
      "Training loss  0.950 in Step 30\n",
      "Training loss  0.944 in Step 40\n",
      "Training loss  0.955 in Step 50\n",
      "Training loss  0.961 in Step 60\n",
      "Training loss  0.949 in Step 70\n",
      "Training loss  0.958 in Step 80\n",
      "Training loss  0.945 in Step 90\n",
      "Training loss  0.957 in Step 100\n",
      "Training loss  0.953 in Step 110\n",
      "Training loss  0.954 in Step 120\n",
      "Training loss  0.943 in Step 130\n",
      "Training loss  0.962 in Step 140\n",
      "Training loss  0.947 in Step 150\n",
      "Training loss  0.946 in Step 160\n",
      "Training loss  0.952 in Step 170\n",
      "※※※Training loss  0.950※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.955 in Step 0\n",
      "※※※Valid loss  0.953※※※\n",
      "Epoch 36\n",
      "Training loss  0.952 in Step 0\n",
      "Training loss  0.951 in Step 10\n",
      "Training loss  0.953 in Step 20\n",
      "Training loss  0.962 in Step 30\n",
      "Training loss  0.957 in Step 40\n",
      "Training loss  0.959 in Step 50\n",
      "Training loss  0.949 in Step 60\n",
      "Training loss  0.955 in Step 70\n",
      "Training loss  0.952 in Step 80\n",
      "Training loss  0.946 in Step 90\n",
      "Training loss  0.947 in Step 100\n",
      "Training loss  0.937 in Step 110\n",
      "Training loss  0.948 in Step 120\n",
      "Training loss  0.957 in Step 130\n",
      "Training loss  0.964 in Step 140\n",
      "Training loss  0.939 in Step 150\n",
      "Training loss  0.958 in Step 160\n",
      "Training loss  0.952 in Step 170\n",
      "※※※Training loss  0.950※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.954 in Step 0\n",
      "※※※Valid loss  0.951※※※\n",
      "Epoch 37\n",
      "Training loss  0.947 in Step 0\n",
      "Training loss  0.950 in Step 10\n",
      "Training loss  0.956 in Step 20\n",
      "Training loss  0.958 in Step 30\n",
      "Training loss  0.949 in Step 40\n",
      "Training loss  0.956 in Step 50\n",
      "Training loss  0.954 in Step 60\n",
      "Training loss  0.955 in Step 70\n",
      "Training loss  0.950 in Step 80\n",
      "Training loss  0.947 in Step 90\n",
      "Training loss  0.946 in Step 100\n",
      "Training loss  0.945 in Step 110\n",
      "Training loss  0.953 in Step 120\n",
      "Training loss  0.952 in Step 130\n",
      "Training loss  0.940 in Step 140\n",
      "Training loss  0.944 in Step 150\n",
      "Training loss  0.950 in Step 160\n",
      "Training loss  0.952 in Step 170\n",
      "※※※Training loss  0.950※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.953 in Step 0\n",
      "※※※Valid loss  0.951※※※\n",
      "Epoch 38\n",
      "Training loss  0.950 in Step 0\n",
      "Training loss  0.948 in Step 10\n",
      "Training loss  0.955 in Step 20\n",
      "Training loss  0.949 in Step 30\n",
      "Training loss  0.951 in Step 40\n",
      "Training loss  0.954 in Step 50\n",
      "Training loss  0.945 in Step 60\n",
      "Training loss  0.946 in Step 70\n",
      "Training loss  0.943 in Step 80\n",
      "Training loss  0.939 in Step 90\n",
      "Training loss  0.950 in Step 100\n",
      "Training loss  0.940 in Step 110\n",
      "Training loss  0.952 in Step 120\n",
      "Training loss  0.945 in Step 130\n",
      "Training loss  0.936 in Step 140\n",
      "Training loss  0.952 in Step 150\n",
      "Training loss  0.965 in Step 160\n",
      "Training loss  0.951 in Step 170\n",
      "※※※Training loss  0.950※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.953 in Step 0\n",
      "※※※Valid loss  0.950※※※\n",
      "Epoch 39\n",
      "Training loss  0.942 in Step 0\n",
      "Training loss  0.953 in Step 10\n",
      "Training loss  0.956 in Step 20\n",
      "Training loss  0.951 in Step 30\n",
      "Training loss  0.941 in Step 40\n",
      "Training loss  0.951 in Step 50\n",
      "Training loss  0.960 in Step 60\n",
      "Training loss  0.950 in Step 70\n",
      "Training loss  0.942 in Step 80\n",
      "Training loss  0.960 in Step 90\n",
      "Training loss  0.956 in Step 100\n",
      "Training loss  0.944 in Step 110\n",
      "Training loss  0.961 in Step 120\n",
      "Training loss  0.948 in Step 130\n",
      "Training loss  0.958 in Step 140\n",
      "Training loss  0.951 in Step 150\n",
      "Training loss  0.947 in Step 160\n",
      "Training loss  0.952 in Step 170\n",
      "※※※Training loss  0.949※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.952 in Step 0\n",
      "※※※Valid loss  0.951※※※\n",
      "Epoch 40\n",
      "Training loss  0.945 in Step 0\n",
      "Training loss  0.956 in Step 10\n",
      "Training loss  0.947 in Step 20\n",
      "Training loss  0.955 in Step 30\n",
      "Training loss  0.952 in Step 40\n",
      "Training loss  0.949 in Step 50\n",
      "Training loss  0.947 in Step 60\n",
      "Training loss  0.965 in Step 70\n",
      "Training loss  0.958 in Step 80\n",
      "Training loss  0.955 in Step 90\n",
      "Training loss  0.965 in Step 100\n",
      "Training loss  0.954 in Step 110\n",
      "Training loss  0.947 in Step 120\n",
      "Training loss  0.963 in Step 130\n",
      "Training loss  0.962 in Step 140\n",
      "Training loss  0.944 in Step 150\n",
      "Training loss  0.947 in Step 160\n",
      "Training loss  0.947 in Step 170\n",
      "※※※Training loss  0.949※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.952 in Step 0\n",
      "※※※Valid loss  0.950※※※\n",
      "Epoch 41\n",
      "Training loss  0.952 in Step 0\n",
      "Training loss  0.947 in Step 10\n",
      "Training loss  0.950 in Step 20\n",
      "Training loss  0.950 in Step 30\n",
      "Training loss  0.955 in Step 40\n",
      "Training loss  0.953 in Step 50\n",
      "Training loss  0.938 in Step 60\n",
      "Training loss  0.946 in Step 70\n",
      "Training loss  0.949 in Step 80\n",
      "Training loss  0.954 in Step 90\n",
      "Training loss  0.945 in Step 100\n",
      "Training loss  0.950 in Step 110\n",
      "Training loss  0.956 in Step 120\n",
      "Training loss  0.961 in Step 130\n",
      "Training loss  0.961 in Step 140\n",
      "Training loss  0.953 in Step 150\n",
      "Training loss  0.948 in Step 160\n",
      "Training loss  0.952 in Step 170\n",
      "※※※Training loss  0.949※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.951 in Step 0\n",
      "※※※Valid loss  0.950※※※\n",
      "Epoch 42\n",
      "Training loss  0.951 in Step 0\n",
      "Training loss  0.950 in Step 10\n",
      "Training loss  0.957 in Step 20\n",
      "Training loss  0.946 in Step 30\n",
      "Training loss  0.947 in Step 40\n",
      "Training loss  0.952 in Step 50\n",
      "Training loss  0.956 in Step 60\n",
      "Training loss  0.951 in Step 70\n",
      "Training loss  0.949 in Step 80\n",
      "Training loss  0.946 in Step 90\n",
      "Training loss  0.940 in Step 100\n",
      "Training loss  0.964 in Step 110\n",
      "Training loss  0.953 in Step 120\n",
      "Training loss  0.950 in Step 130\n",
      "Training loss  0.955 in Step 140\n",
      "Training loss  0.943 in Step 150\n",
      "Training loss  0.958 in Step 160\n",
      "Training loss  0.945 in Step 170\n",
      "※※※Training loss  0.949※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.951 in Step 0\n",
      "※※※Valid loss  0.949※※※\n",
      "Epoch 43\n",
      "Training loss  0.941 in Step 0\n",
      "Training loss  0.956 in Step 10\n",
      "Training loss  0.943 in Step 20\n",
      "Training loss  0.953 in Step 30\n",
      "Training loss  0.945 in Step 40\n",
      "Training loss  0.948 in Step 50\n",
      "Training loss  0.939 in Step 60\n",
      "Training loss  0.945 in Step 70\n",
      "Training loss  0.958 in Step 80\n",
      "Training loss  0.937 in Step 90\n",
      "Training loss  0.959 in Step 100\n",
      "Training loss  0.944 in Step 110\n",
      "Training loss  0.948 in Step 120\n",
      "Training loss  0.950 in Step 130\n",
      "Training loss  0.945 in Step 140\n",
      "Training loss  0.954 in Step 150\n",
      "Training loss  0.950 in Step 160\n",
      "Training loss  0.951 in Step 170\n",
      "※※※Training loss  0.948※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.952 in Step 0\n",
      "※※※Valid loss  0.949※※※\n",
      "Epoch 44\n",
      "Training loss  0.943 in Step 0\n",
      "Training loss  0.948 in Step 10\n",
      "Training loss  0.955 in Step 20\n",
      "Training loss  0.946 in Step 30\n",
      "Training loss  0.951 in Step 40\n",
      "Training loss  0.938 in Step 50\n",
      "Training loss  0.939 in Step 60\n",
      "Training loss  0.956 in Step 70\n",
      "Training loss  0.957 in Step 80\n",
      "Training loss  0.949 in Step 90\n",
      "Training loss  0.935 in Step 100\n",
      "Training loss  0.956 in Step 110\n",
      "Training loss  0.950 in Step 120\n",
      "Training loss  0.945 in Step 130\n",
      "Training loss  0.948 in Step 140\n",
      "Training loss  0.942 in Step 150\n",
      "Training loss  0.946 in Step 160\n",
      "Training loss  0.941 in Step 170\n",
      "※※※Training loss  0.948※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.952 in Step 0\n",
      "※※※Valid loss  0.949※※※\n",
      "Epoch 45\n",
      "Training loss  0.952 in Step 0\n",
      "Training loss  0.959 in Step 10\n",
      "Training loss  0.940 in Step 20\n",
      "Training loss  0.948 in Step 30\n",
      "Training loss  0.946 in Step 40\n",
      "Training loss  0.946 in Step 50\n",
      "Training loss  0.944 in Step 60\n",
      "Training loss  0.954 in Step 70\n",
      "Training loss  0.955 in Step 80\n",
      "Training loss  0.940 in Step 90\n",
      "Training loss  0.952 in Step 100\n",
      "Training loss  0.932 in Step 110\n",
      "Training loss  0.964 in Step 120\n",
      "Training loss  0.950 in Step 130\n",
      "Training loss  0.951 in Step 140\n",
      "Training loss  0.955 in Step 150\n",
      "Training loss  0.949 in Step 160\n",
      "Training loss  0.942 in Step 170\n",
      "※※※Training loss  0.948※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.950 in Step 0\n",
      "※※※Valid loss  0.948※※※\n",
      "Epoch 46\n",
      "Training loss  0.932 in Step 0\n",
      "Training loss  0.950 in Step 10\n",
      "Training loss  0.952 in Step 20\n",
      "Training loss  0.950 in Step 30\n",
      "Training loss  0.951 in Step 40\n",
      "Training loss  0.960 in Step 50\n",
      "Training loss  0.948 in Step 60\n",
      "Training loss  0.942 in Step 70\n",
      "Training loss  0.949 in Step 80\n",
      "Training loss  0.938 in Step 90\n",
      "Training loss  0.951 in Step 100\n",
      "Training loss  0.947 in Step 110\n",
      "Training loss  0.950 in Step 120\n",
      "Training loss  0.952 in Step 130\n",
      "Training loss  0.951 in Step 140\n",
      "Training loss  0.949 in Step 150\n",
      "Training loss  0.958 in Step 160\n",
      "Training loss  0.951 in Step 170\n",
      "※※※Training loss  0.948※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.951 in Step 0\n",
      "※※※Valid loss  0.948※※※\n",
      "Epoch 47\n",
      "Training loss  0.930 in Step 0\n",
      "Training loss  0.942 in Step 10\n",
      "Training loss  0.952 in Step 20\n",
      "Training loss  0.949 in Step 30\n",
      "Training loss  0.949 in Step 40\n",
      "Training loss  0.944 in Step 50\n",
      "Training loss  0.949 in Step 60\n",
      "Training loss  0.948 in Step 70\n",
      "Training loss  0.945 in Step 80\n",
      "Training loss  0.947 in Step 90\n",
      "Training loss  0.941 in Step 100\n",
      "Training loss  0.947 in Step 110\n",
      "Training loss  0.950 in Step 120\n",
      "Training loss  0.951 in Step 130\n",
      "Training loss  0.935 in Step 140\n",
      "Training loss  0.948 in Step 150\n",
      "Training loss  0.942 in Step 160\n",
      "Training loss  0.945 in Step 170\n",
      "※※※Training loss  0.947※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.950 in Step 0\n",
      "※※※Valid loss  0.948※※※\n",
      "Epoch 48\n",
      "Training loss  0.943 in Step 0\n",
      "Training loss  0.952 in Step 10\n",
      "Training loss  0.946 in Step 20\n",
      "Training loss  0.960 in Step 30\n",
      "Training loss  0.954 in Step 40\n",
      "Training loss  0.952 in Step 50\n",
      "Training loss  0.954 in Step 60\n",
      "Training loss  0.952 in Step 70\n",
      "Training loss  0.945 in Step 80\n",
      "Training loss  0.948 in Step 90\n",
      "Training loss  0.959 in Step 100\n",
      "Training loss  0.954 in Step 110\n",
      "Training loss  0.946 in Step 120\n",
      "Training loss  0.932 in Step 130\n",
      "Training loss  0.950 in Step 140\n",
      "Training loss  0.943 in Step 150\n",
      "Training loss  0.966 in Step 160\n",
      "Training loss  0.947 in Step 170\n",
      "※※※Training loss  0.948※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.951 in Step 0\n",
      "※※※Valid loss  0.948※※※\n",
      "Epoch 49\n",
      "Training loss  0.949 in Step 0\n",
      "Training loss  0.945 in Step 10\n",
      "Training loss  0.948 in Step 20\n",
      "Training loss  0.956 in Step 30\n",
      "Training loss  0.946 in Step 40\n",
      "Training loss  0.947 in Step 50\n",
      "Training loss  0.946 in Step 60\n",
      "Training loss  0.935 in Step 70\n",
      "Training loss  0.949 in Step 80\n",
      "Training loss  0.955 in Step 90\n",
      "Training loss  0.944 in Step 100\n",
      "Training loss  0.958 in Step 110\n",
      "Training loss  0.959 in Step 120\n",
      "Training loss  0.940 in Step 130\n",
      "Training loss  0.945 in Step 140\n",
      "Training loss  0.947 in Step 150\n",
      "Training loss  0.955 in Step 160\n",
      "Training loss  0.940 in Step 170\n",
      "※※※Training loss  0.947※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.951 in Step 0\n",
      "※※※Valid loss  0.948※※※\n",
      "Epoch 50\n",
      "Training loss  0.956 in Step 0\n",
      "Training loss  0.941 in Step 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[0;32m----> 2\u001b[0m     train()\n",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m      7\u001b[0m train_num \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)    \u001b[39m# train_loader\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m idx, (x, x_lens) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m     y \u001b[39m=\u001b[39m x[:, :, :\u001b[39m13\u001b[39m]    \u001b[39m# extract MFCC-only data\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[39m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(res)\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/site-packages/torch/multiprocessing/reductions.py:307\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrebuild_storage_fd\u001b[39m(\u001b[39mcls\u001b[39m, df, size):\n\u001b[0;32m--> 307\u001b[0m     fd \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mdetach()\n\u001b[1;32m    308\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         storage \u001b[39m=\u001b[39m storage_from_cache(\u001b[39mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetach\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mwith\u001b[39;00m _resource_sharer\u001b[39m.\u001b[39;49mget_connection(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id) \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[39mreturn\u001b[39;00m reduction\u001b[39m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[39m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[39m=\u001b[39m Client(address, authkey\u001b[39m=\u001b[39;49mprocess\u001b[39m.\u001b[39;49mcurrent_process()\u001b[39m.\u001b[39;49mauthkey)\n\u001b[1;32m     87\u001b[0m c\u001b[39m.\u001b[39msend((key, os\u001b[39m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/connection.py:507\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mauthkey should be a byte string\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m authkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m     answer_challenge(c, authkey)\n\u001b[1;32m    508\u001b[0m     deliver_challenge(c, authkey)\n\u001b[1;32m    510\u001b[0m \u001b[39mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/connection.py:756\u001b[0m, in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    754\u001b[0m digest \u001b[39m=\u001b[39m hmac\u001b[39m.\u001b[39mnew(authkey, message, \u001b[39m'\u001b[39m\u001b[39mmd5\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdigest()\n\u001b[1;32m    755\u001b[0m connection\u001b[39m.\u001b[39msend_bytes(digest)\n\u001b[0;32m--> 756\u001b[0m response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mrecv_bytes(\u001b[39m256\u001b[39;49m)        \u001b[39m# reject large message\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39m!=\u001b[39m WELCOME:\n\u001b[1;32m    758\u001b[0m     \u001b[39mraise\u001b[39;00m AuthenticationError(\u001b[39m'\u001b[39m\u001b[39mdigest sent was rejected\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/connection.py:215\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m maxlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m maxlength \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnegative maxlength\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes(maxlength)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m buf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/connection.py:413\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 413\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    414\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wavln/lib/python3.11/multiprocessing/connection.py:378\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    376\u001b[0m remaining \u001b[39m=\u001b[39m size\n\u001b[1;32m    377\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 378\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    379\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "train_losses.save()\n",
    "\n",
    "valid_losses.save()\n",
    "\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fabc02c0f10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjPWgjpid7PR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
