{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Direct - English\n",
    "Version 1: In this version we make the model \"simple\": make the encoder RNN into normal RNN first and try to see the result.  \n",
    "Version 2: Learning is not very much. Following Dr Coupe's advice we try simpler model structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PhxLearner, SimplerPhxLearner\n",
    "from my_dataset import MyTransform, DS_Tools\n",
    "from phone_dataset import PhoneDataset, collate_fn\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from recorder import *\n",
    "from loss import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIM = 39\n",
    "OUTPUT_DIM = 13\n",
    "\n",
    "INTER_DIM_0 = 64\n",
    "INTER_DIM_1 = 16\n",
    "INTER_DIM_2 = 3\n",
    "\n",
    "ENC_SIZE_LIST = [INPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "DEC_SIZE_LIST = [OUTPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "\n",
    "LOADER_WORKER = 16\n",
    "# LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "masked_recon_loss = MaskedLoss(recon_loss)\n",
    "model_loss = masked_recon_loss\n",
    "\n",
    "model = SimplerPhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=2)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplerPhxLearner(\n",
       "  (encoder): RLEncoder(\n",
       "    (rnn): LSTM(39, 16, num_layers=2, batch_first=True)\n",
       "    (lin_2): LinearPack(\n",
       "      (linear): Linear(in_features=16, out_features=3, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): RALDecoder(\n",
       "    (rnn): LSTM(13, 3, num_layers=2, batch_first=True)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (lin_3): LinearPack(\n",
       "      (linear): Linear(in_features=3, out_features=13, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6275"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "ts = \"0908015948\"\n",
    "stop_epoch = \"149\"\n",
    "# ts = str(get_timestamp())\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "\n",
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "# READ = False\n",
    "READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    valid_losses.read()\n",
    "    train_losses.read()\n",
    "\n",
    "    model_raw_name = \"PT_{}_{}_full\".format(ts, stop_epoch)\n",
    "    model_name = model_raw_name + \".pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [],
   "source": [
    "mytrans = MyTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT)\n",
    "ds = PhoneDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "\n",
    "\n",
    "if READ: \n",
    "    valid_ds_indices = DS_Tools.read_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)))\n",
    "    all_indices = list(range(len(ds)))\n",
    "    train_ds_indices = list(set(all_indices).difference(set(valid_ds_indices)))\n",
    "\n",
    "    train_ds = torch.utils.data.Subset(ds, train_ds_indices)\n",
    "    valid_ds = torch.utils.data.Subset(ds, valid_ds_indices)\n",
    "else: \n",
    "    train_len = int(0.8 * len(ds))\n",
    "    valid_len = len(ds) - train_len\n",
    "\n",
    "    # Randomly split the dataset into train and validation sets\n",
    "    train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "    DS_Tools.save_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)), valid_ds.indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=LOADER_WORKER, collate_fn=collate_fn)\n",
    "valid_num = len(valid_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BASE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y = x[:, :, :13]    # extract MFCC-only data\n",
    "            \n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            # 这个函数计算的是全局梯度范数\n",
    "            # torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            # parameters: an iterable of Variables that will have gradients normalized\n",
    "            # max_norm: max norm of the gradients(阈值设定)\n",
    "            # norm_type: type of the used p-norm. Can be'inf'for infinity norm(定义范数类型)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            y = x[:, :, :13]    # extract MFCC-only data\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss  0.769 in Step 0\n",
      "Training loss  0.788 in Step 100\n",
      "Training loss  0.775 in Step 200\n",
      "Training loss  0.774 in Step 300\n",
      "Training loss  0.779 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.789 in Step 600\n",
      "Training loss  0.765 in Step 700\n",
      "Training loss  0.770 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.796 in Step 1000\n",
      "Training loss  0.770 in Step 1100\n",
      "Training loss  0.777 in Step 1200\n",
      "Training loss  0.769 in Step 1300\n",
      "Training loss  0.758 in Step 1400\n",
      "Training loss  0.793 in Step 1500\n",
      "Training loss  0.781 in Step 1600\n",
      "Training loss  0.783 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.773 in Step 0\n",
      "Valid loss  0.780 in Step 100\n",
      "Valid loss  0.777 in Step 200\n",
      "Valid loss  0.779 in Step 300\n",
      "Valid loss  0.779 in Step 400\n",
      "※※※Valid loss  0.785※※※\n",
      "Epoch 201\n",
      "Training loss  0.785 in Step 0\n",
      "Training loss  0.776 in Step 100\n",
      "Training loss  0.764 in Step 200\n",
      "Training loss  0.788 in Step 300\n",
      "Training loss  0.769 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.787 in Step 600\n",
      "Training loss  0.795 in Step 700\n",
      "Training loss  0.771 in Step 800\n",
      "Training loss  0.794 in Step 900\n",
      "Training loss  0.783 in Step 1000\n",
      "Training loss  0.765 in Step 1100\n",
      "Training loss  0.778 in Step 1200\n",
      "Training loss  0.776 in Step 1300\n",
      "Training loss  0.773 in Step 1400\n",
      "Training loss  0.789 in Step 1500\n",
      "Training loss  0.783 in Step 1600\n",
      "Training loss  0.781 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.772 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.774 in Step 200\n",
      "Valid loss  0.775 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 202\n",
      "Training loss  0.789 in Step 0\n",
      "Training loss  0.776 in Step 100\n",
      "Training loss  0.762 in Step 200\n",
      "Training loss  0.777 in Step 300\n",
      "Training loss  0.778 in Step 400\n",
      "Training loss  0.791 in Step 500\n",
      "Training loss  0.778 in Step 600\n",
      "Training loss  0.771 in Step 700\n",
      "Training loss  0.798 in Step 800\n",
      "Training loss  0.778 in Step 900\n",
      "Training loss  0.783 in Step 1000\n",
      "Training loss  0.773 in Step 1100\n",
      "Training loss  0.776 in Step 1200\n",
      "Training loss  0.775 in Step 1300\n",
      "Training loss  0.784 in Step 1400\n",
      "Training loss  0.805 in Step 1500\n",
      "Training loss  0.771 in Step 1600\n",
      "Training loss  0.769 in Step 1700\n",
      "※※※Training loss  0.780※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.773 in Step 200\n",
      "Valid loss  0.775 in Step 300\n",
      "Valid loss  0.776 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 203\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.771 in Step 100\n",
      "Training loss  0.765 in Step 200\n",
      "Training loss  0.769 in Step 300\n",
      "Training loss  0.784 in Step 400\n",
      "Training loss  0.771 in Step 500\n",
      "Training loss  0.790 in Step 600\n",
      "Training loss  0.780 in Step 700\n",
      "Training loss  0.793 in Step 800\n",
      "Training loss  0.786 in Step 900\n",
      "Training loss  0.764 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.745 in Step 1200\n",
      "Training loss  0.786 in Step 1300\n",
      "Training loss  0.787 in Step 1400\n",
      "Training loss  0.782 in Step 1500\n",
      "Training loss  0.800 in Step 1600\n",
      "Training loss  0.779 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.769 in Step 0\n",
      "Valid loss  0.774 in Step 100\n",
      "Valid loss  0.773 in Step 200\n",
      "Valid loss  0.775 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 204\n",
      "Training loss  0.763 in Step 0\n",
      "Training loss  0.781 in Step 100\n",
      "Training loss  0.797 in Step 200\n",
      "Training loss  0.773 in Step 300\n",
      "Training loss  0.760 in Step 400\n",
      "Training loss  0.779 in Step 500\n",
      "Training loss  0.786 in Step 600\n",
      "Training loss  0.779 in Step 700\n",
      "Training loss  0.787 in Step 800\n",
      "Training loss  0.776 in Step 900\n",
      "Training loss  0.785 in Step 1000\n",
      "Training loss  0.777 in Step 1100\n",
      "Training loss  0.789 in Step 1200\n",
      "Training loss  0.779 in Step 1300\n",
      "Training loss  0.781 in Step 1400\n",
      "Training loss  0.767 in Step 1500\n",
      "Training loss  0.771 in Step 1600\n",
      "Training loss  0.775 in Step 1700\n",
      "※※※Training loss  0.780※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.775 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 205\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.793 in Step 100\n",
      "Training loss  0.770 in Step 200\n",
      "Training loss  0.780 in Step 300\n",
      "Training loss  0.776 in Step 400\n",
      "Training loss  0.766 in Step 500\n",
      "Training loss  0.793 in Step 600\n",
      "Training loss  0.797 in Step 700\n",
      "Training loss  0.793 in Step 800\n",
      "Training loss  0.775 in Step 900\n",
      "Training loss  0.787 in Step 1000\n",
      "Training loss  0.778 in Step 1100\n",
      "Training loss  0.792 in Step 1200\n",
      "Training loss  0.795 in Step 1300\n",
      "Training loss  0.778 in Step 1400\n",
      "Training loss  0.778 in Step 1500\n",
      "Training loss  0.776 in Step 1600\n",
      "Training loss  0.776 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.775 in Step 0\n",
      "Valid loss  0.779 in Step 100\n",
      "Valid loss  0.777 in Step 200\n",
      "Valid loss  0.778 in Step 300\n",
      "Valid loss  0.775 in Step 400\n",
      "※※※Valid loss  0.783※※※\n",
      "Epoch 206\n",
      "Training loss  0.767 in Step 0\n",
      "Training loss  0.788 in Step 100\n",
      "Training loss  0.766 in Step 200\n",
      "Training loss  0.787 in Step 300\n",
      "Training loss  0.757 in Step 400\n",
      "Training loss  0.785 in Step 500\n",
      "Training loss  0.780 in Step 600\n",
      "Training loss  0.783 in Step 700\n",
      "Training loss  0.774 in Step 800\n",
      "Training loss  0.774 in Step 900\n",
      "Training loss  0.768 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.798 in Step 1200\n",
      "Training loss  0.773 in Step 1300\n",
      "Training loss  0.789 in Step 1400\n",
      "Training loss  0.802 in Step 1500\n",
      "Training loss  0.792 in Step 1600\n",
      "Training loss  0.782 in Step 1700\n",
      "※※※Training loss  0.780※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.779 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 207\n",
      "Training loss  0.782 in Step 0\n",
      "Training loss  0.782 in Step 100\n",
      "Training loss  0.781 in Step 200\n",
      "Training loss  0.784 in Step 300\n",
      "Training loss  0.782 in Step 400\n",
      "Training loss  0.778 in Step 500\n",
      "Training loss  0.773 in Step 600\n",
      "Training loss  0.774 in Step 700\n",
      "Training loss  0.774 in Step 800\n",
      "Training loss  0.765 in Step 900\n",
      "Training loss  0.793 in Step 1000\n",
      "Training loss  0.764 in Step 1100\n",
      "Training loss  0.777 in Step 1200\n",
      "Training loss  0.780 in Step 1300\n",
      "Training loss  0.788 in Step 1400\n",
      "Training loss  0.763 in Step 1500\n",
      "Training loss  0.803 in Step 1600\n",
      "Training loss  0.763 in Step 1700\n",
      "※※※Training loss  0.780※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.773 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.774 in Step 200\n",
      "Valid loss  0.778 in Step 300\n",
      "Valid loss  0.775 in Step 400\n",
      "※※※Valid loss  0.783※※※\n",
      "Epoch 208\n",
      "Training loss  0.790 in Step 0\n",
      "Training loss  0.781 in Step 100\n",
      "Training loss  0.793 in Step 200\n",
      "Training loss  0.790 in Step 300\n",
      "Training loss  0.787 in Step 400\n",
      "Training loss  0.784 in Step 500\n",
      "Training loss  0.792 in Step 600\n",
      "Training loss  0.770 in Step 700\n",
      "Training loss  0.766 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.758 in Step 1000\n",
      "Training loss  0.779 in Step 1100\n",
      "Training loss  0.789 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.770 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.791 in Step 1600\n",
      "Training loss  0.774 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.773 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 209\n",
      "Training loss  0.779 in Step 0\n",
      "Training loss  0.789 in Step 100\n",
      "Training loss  0.783 in Step 200\n",
      "Training loss  0.792 in Step 300\n",
      "Training loss  0.788 in Step 400\n",
      "Training loss  0.775 in Step 500\n",
      "Training loss  0.759 in Step 600\n",
      "Training loss  0.772 in Step 700\n",
      "Training loss  0.783 in Step 800\n",
      "Training loss  0.791 in Step 900\n",
      "Training loss  0.779 in Step 1000\n",
      "Training loss  0.774 in Step 1100\n",
      "Training loss  0.778 in Step 1200\n",
      "Training loss  0.792 in Step 1300\n",
      "Training loss  0.770 in Step 1400\n",
      "Training loss  0.793 in Step 1500\n",
      "Training loss  0.782 in Step 1600\n",
      "Training loss  0.784 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 210\n",
      "Training loss  0.783 in Step 0\n",
      "Training loss  0.757 in Step 100\n",
      "Training loss  0.782 in Step 200\n",
      "Training loss  0.790 in Step 300\n",
      "Training loss  0.778 in Step 400\n",
      "Training loss  0.782 in Step 500\n",
      "Training loss  0.775 in Step 600\n",
      "Training loss  0.789 in Step 700\n",
      "Training loss  0.783 in Step 800\n",
      "Training loss  0.781 in Step 900\n",
      "Training loss  0.776 in Step 1000\n",
      "Training loss  0.795 in Step 1100\n",
      "Training loss  0.784 in Step 1200\n",
      "Training loss  0.772 in Step 1300\n",
      "Training loss  0.768 in Step 1400\n",
      "Training loss  0.785 in Step 1500\n",
      "Training loss  0.774 in Step 1600\n",
      "Training loss  0.761 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.773 in Step 0\n",
      "Valid loss  0.781 in Step 100\n",
      "Valid loss  0.774 in Step 200\n",
      "Valid loss  0.775 in Step 300\n",
      "Valid loss  0.775 in Step 400\n",
      "※※※Valid loss  0.782※※※\n",
      "Epoch 211\n",
      "Training loss  0.779 in Step 0\n",
      "Training loss  0.772 in Step 100\n",
      "Training loss  0.783 in Step 200\n",
      "Training loss  0.789 in Step 300\n",
      "Training loss  0.789 in Step 400\n",
      "Training loss  0.775 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.769 in Step 700\n",
      "Training loss  0.795 in Step 800\n",
      "Training loss  0.779 in Step 900\n",
      "Training loss  0.793 in Step 1000\n",
      "Training loss  0.782 in Step 1100\n",
      "Training loss  0.782 in Step 1200\n",
      "Training loss  0.792 in Step 1300\n",
      "Training loss  0.775 in Step 1400\n",
      "Training loss  0.765 in Step 1500\n",
      "Training loss  0.761 in Step 1600\n",
      "Training loss  0.794 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.773 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 212\n",
      "Training loss  0.774 in Step 0\n",
      "Training loss  0.771 in Step 100\n",
      "Training loss  0.776 in Step 200\n",
      "Training loss  0.775 in Step 300\n",
      "Training loss  0.772 in Step 400\n",
      "Training loss  0.790 in Step 500\n",
      "Training loss  0.794 in Step 600\n",
      "Training loss  0.779 in Step 700\n",
      "Training loss  0.788 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.783 in Step 1000\n",
      "Training loss  0.795 in Step 1100\n",
      "Training loss  0.764 in Step 1200\n",
      "Training loss  0.762 in Step 1300\n",
      "Training loss  0.786 in Step 1400\n",
      "Training loss  0.781 in Step 1500\n",
      "Training loss  0.788 in Step 1600\n",
      "Training loss  0.764 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.773 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 213\n",
      "Training loss  0.778 in Step 0\n",
      "Training loss  0.779 in Step 100\n",
      "Training loss  0.780 in Step 200\n",
      "Training loss  0.792 in Step 300\n",
      "Training loss  0.772 in Step 400\n",
      "Training loss  0.776 in Step 500\n",
      "Training loss  0.766 in Step 600\n",
      "Training loss  0.786 in Step 700\n",
      "Training loss  0.791 in Step 800\n",
      "Training loss  0.785 in Step 900\n",
      "Training loss  0.795 in Step 1000\n",
      "Training loss  0.787 in Step 1100\n",
      "Training loss  0.785 in Step 1200\n",
      "Training loss  0.775 in Step 1300\n",
      "Training loss  0.762 in Step 1400\n",
      "Training loss  0.774 in Step 1500\n",
      "Training loss  0.788 in Step 1600\n",
      "Training loss  0.765 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.769 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 214\n",
      "Training loss  0.775 in Step 0\n",
      "Training loss  0.757 in Step 100\n",
      "Training loss  0.768 in Step 200\n",
      "Training loss  0.783 in Step 300\n",
      "Training loss  0.771 in Step 400\n",
      "Training loss  0.793 in Step 500\n",
      "Training loss  0.761 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.803 in Step 800\n",
      "Training loss  0.790 in Step 900\n",
      "Training loss  0.773 in Step 1000\n",
      "Training loss  0.777 in Step 1100\n",
      "Training loss  0.767 in Step 1200\n",
      "Training loss  0.780 in Step 1300\n",
      "Training loss  0.780 in Step 1400\n",
      "Training loss  0.795 in Step 1500\n",
      "Training loss  0.788 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.769 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.775 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.778 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 215\n",
      "Training loss  0.762 in Step 0\n",
      "Training loss  0.784 in Step 100\n",
      "Training loss  0.770 in Step 200\n",
      "Training loss  0.759 in Step 300\n",
      "Training loss  0.774 in Step 400\n",
      "Training loss  0.773 in Step 500\n",
      "Training loss  0.783 in Step 600\n",
      "Training loss  0.764 in Step 700\n",
      "Training loss  0.774 in Step 800\n",
      "Training loss  0.778 in Step 900\n",
      "Training loss  0.773 in Step 1000\n",
      "Training loss  0.782 in Step 1100\n",
      "Training loss  0.763 in Step 1200\n",
      "Training loss  0.759 in Step 1300\n",
      "Training loss  0.811 in Step 1400\n",
      "Training loss  0.772 in Step 1500\n",
      "Training loss  0.789 in Step 1600\n",
      "Training loss  0.788 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.767 in Step 0\n",
      "Valid loss  0.774 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 216\n",
      "Training loss  0.792 in Step 0\n",
      "Training loss  0.770 in Step 100\n",
      "Training loss  0.779 in Step 200\n",
      "Training loss  0.769 in Step 300\n",
      "Training loss  0.773 in Step 400\n",
      "Training loss  0.769 in Step 500\n",
      "Training loss  0.787 in Step 600\n",
      "Training loss  0.778 in Step 700\n",
      "Training loss  0.769 in Step 800\n",
      "Training loss  0.782 in Step 900\n",
      "Training loss  0.775 in Step 1000\n",
      "Training loss  0.782 in Step 1100\n",
      "Training loss  0.772 in Step 1200\n",
      "Training loss  0.764 in Step 1300\n",
      "Training loss  0.767 in Step 1400\n",
      "Training loss  0.794 in Step 1500\n",
      "Training loss  0.784 in Step 1600\n",
      "Training loss  0.771 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.767 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.773 in Step 200\n",
      "Valid loss  0.771 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 217\n",
      "Training loss  0.766 in Step 0\n",
      "Training loss  0.792 in Step 100\n",
      "Training loss  0.773 in Step 200\n",
      "Training loss  0.790 in Step 300\n",
      "Training loss  0.777 in Step 400\n",
      "Training loss  0.772 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.787 in Step 700\n",
      "Training loss  0.772 in Step 800\n",
      "Training loss  0.779 in Step 900\n",
      "Training loss  0.786 in Step 1000\n",
      "Training loss  0.776 in Step 1100\n",
      "Training loss  0.788 in Step 1200\n",
      "Training loss  0.773 in Step 1300\n",
      "Training loss  0.770 in Step 1400\n",
      "Training loss  0.764 in Step 1500\n",
      "Training loss  0.788 in Step 1600\n",
      "Training loss  0.786 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.769 in Step 0\n",
      "Valid loss  0.773 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 218\n",
      "Training loss  0.759 in Step 0\n",
      "Training loss  0.768 in Step 100\n",
      "Training loss  0.782 in Step 200\n",
      "Training loss  0.800 in Step 300\n",
      "Training loss  0.779 in Step 400\n",
      "Training loss  0.755 in Step 500\n",
      "Training loss  0.790 in Step 600\n",
      "Training loss  0.785 in Step 700\n",
      "Training loss  0.777 in Step 800\n",
      "Training loss  0.776 in Step 900\n",
      "Training loss  0.777 in Step 1000\n",
      "Training loss  0.771 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.785 in Step 1300\n",
      "Training loss  0.784 in Step 1400\n",
      "Training loss  0.788 in Step 1500\n",
      "Training loss  0.784 in Step 1600\n",
      "Training loss  0.790 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.773 in Step 0\n",
      "Valid loss  0.774 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.775 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 219\n",
      "Training loss  0.781 in Step 0\n",
      "Training loss  0.783 in Step 100\n",
      "Training loss  0.778 in Step 200\n",
      "Training loss  0.791 in Step 300\n",
      "Training loss  0.782 in Step 400\n",
      "Training loss  0.774 in Step 500\n",
      "Training loss  0.783 in Step 600\n",
      "Training loss  0.783 in Step 700\n",
      "Training loss  0.769 in Step 800\n",
      "Training loss  0.793 in Step 900\n",
      "Training loss  0.780 in Step 1000\n",
      "Training loss  0.791 in Step 1100\n",
      "Training loss  0.784 in Step 1200\n",
      "Training loss  0.781 in Step 1300\n",
      "Training loss  0.769 in Step 1400\n",
      "Training loss  0.761 in Step 1500\n",
      "Training loss  0.785 in Step 1600\n",
      "Training loss  0.772 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.767 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 220\n",
      "Training loss  0.794 in Step 0\n",
      "Training loss  0.775 in Step 100\n",
      "Training loss  0.786 in Step 200\n",
      "Training loss  0.771 in Step 300\n",
      "Training loss  0.783 in Step 400\n",
      "Training loss  0.773 in Step 500\n",
      "Training loss  0.768 in Step 600\n",
      "Training loss  0.781 in Step 700\n",
      "Training loss  0.792 in Step 800\n",
      "Training loss  0.783 in Step 900\n",
      "Training loss  0.783 in Step 1000\n",
      "Training loss  0.773 in Step 1100\n",
      "Training loss  0.798 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.765 in Step 1400\n",
      "Training loss  0.792 in Step 1500\n",
      "Training loss  0.777 in Step 1600\n",
      "Training loss  0.779 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.768 in Step 0\n",
      "Valid loss  0.776 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 221\n",
      "Training loss  0.780 in Step 0\n",
      "Training loss  0.772 in Step 100\n",
      "Training loss  0.765 in Step 200\n",
      "Training loss  0.775 in Step 300\n",
      "Training loss  0.770 in Step 400\n",
      "Training loss  0.773 in Step 500\n",
      "Training loss  0.798 in Step 600\n",
      "Training loss  0.764 in Step 700\n",
      "Training loss  0.787 in Step 800\n",
      "Training loss  0.767 in Step 900\n",
      "Training loss  0.785 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.783 in Step 1200\n",
      "Training loss  0.769 in Step 1300\n",
      "Training loss  0.765 in Step 1400\n",
      "Training loss  0.779 in Step 1500\n",
      "Training loss  0.764 in Step 1600\n",
      "Training loss  0.782 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.770 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 222\n",
      "Training loss  0.774 in Step 0\n",
      "Training loss  0.788 in Step 100\n",
      "Training loss  0.774 in Step 200\n",
      "Training loss  0.780 in Step 300\n",
      "Training loss  0.781 in Step 400\n",
      "Training loss  0.777 in Step 500\n",
      "Training loss  0.776 in Step 600\n",
      "Training loss  0.770 in Step 700\n",
      "Training loss  0.780 in Step 800\n",
      "Training loss  0.788 in Step 900\n",
      "Training loss  0.773 in Step 1000\n",
      "Training loss  0.787 in Step 1100\n",
      "Training loss  0.783 in Step 1200\n",
      "Training loss  0.771 in Step 1300\n",
      "Training loss  0.782 in Step 1400\n",
      "Training loss  0.785 in Step 1500\n",
      "Training loss  0.776 in Step 1600\n",
      "Training loss  0.783 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.767 in Step 0\n",
      "Valid loss  0.773 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.773 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 223\n",
      "Training loss  0.784 in Step 0\n",
      "Training loss  0.775 in Step 100\n",
      "Training loss  0.761 in Step 200\n",
      "Training loss  0.780 in Step 300\n",
      "Training loss  0.778 in Step 400\n",
      "Training loss  0.767 in Step 500\n",
      "Training loss  0.775 in Step 600\n",
      "Training loss  0.809 in Step 700\n",
      "Training loss  0.778 in Step 800\n",
      "Training loss  0.777 in Step 900\n",
      "Training loss  0.769 in Step 1000\n",
      "Training loss  0.778 in Step 1100\n",
      "Training loss  0.790 in Step 1200\n",
      "Training loss  0.799 in Step 1300\n",
      "Training loss  0.759 in Step 1400\n",
      "Training loss  0.756 in Step 1500\n",
      "Training loss  0.784 in Step 1600\n",
      "Training loss  0.788 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.774 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 224\n",
      "Training loss  0.795 in Step 0\n",
      "Training loss  0.779 in Step 100\n",
      "Training loss  0.778 in Step 200\n",
      "Training loss  0.779 in Step 300\n",
      "Training loss  0.777 in Step 400\n",
      "Training loss  0.761 in Step 500\n",
      "Training loss  0.766 in Step 600\n",
      "Training loss  0.789 in Step 700\n",
      "Training loss  0.771 in Step 800\n",
      "Training loss  0.765 in Step 900\n",
      "Training loss  0.798 in Step 1000\n",
      "Training loss  0.772 in Step 1100\n",
      "Training loss  0.759 in Step 1200\n",
      "Training loss  0.772 in Step 1300\n",
      "Training loss  0.783 in Step 1400\n",
      "Training loss  0.789 in Step 1500\n",
      "Training loss  0.800 in Step 1600\n",
      "Training loss  0.780 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.770 in Step 0\n",
      "Valid loss  0.773 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.773 in Step 300\n",
      "Valid loss  0.775 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 225\n",
      "Training loss  0.765 in Step 0\n",
      "Training loss  0.762 in Step 100\n",
      "Training loss  0.764 in Step 200\n",
      "Training loss  0.778 in Step 300\n",
      "Training loss  0.777 in Step 400\n",
      "Training loss  0.769 in Step 500\n",
      "Training loss  0.787 in Step 600\n",
      "Training loss  0.773 in Step 700\n",
      "Training loss  0.770 in Step 800\n",
      "Training loss  0.781 in Step 900\n",
      "Training loss  0.770 in Step 1000\n",
      "Training loss  0.765 in Step 1100\n",
      "Training loss  0.774 in Step 1200\n",
      "Training loss  0.784 in Step 1300\n",
      "Training loss  0.778 in Step 1400\n",
      "Training loss  0.787 in Step 1500\n",
      "Training loss  0.775 in Step 1600\n",
      "Training loss  0.777 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.767 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.777 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 226\n",
      "Training loss  0.790 in Step 0\n",
      "Training loss  0.788 in Step 100\n",
      "Training loss  0.774 in Step 200\n",
      "Training loss  0.785 in Step 300\n",
      "Training loss  0.787 in Step 400\n",
      "Training loss  0.780 in Step 500\n",
      "Training loss  0.782 in Step 600\n",
      "Training loss  0.779 in Step 700\n",
      "Training loss  0.786 in Step 800\n",
      "Training loss  0.754 in Step 900\n",
      "Training loss  0.779 in Step 1000\n",
      "Training loss  0.774 in Step 1100\n",
      "Training loss  0.778 in Step 1200\n",
      "Training loss  0.780 in Step 1300\n",
      "Training loss  0.783 in Step 1400\n",
      "Training loss  0.786 in Step 1500\n",
      "Training loss  0.774 in Step 1600\n",
      "Training loss  0.778 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.776 in Step 100\n",
      "Valid loss  0.773 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 227\n",
      "Training loss  0.775 in Step 0\n",
      "Training loss  0.768 in Step 100\n",
      "Training loss  0.781 in Step 200\n",
      "Training loss  0.789 in Step 300\n",
      "Training loss  0.789 in Step 400\n",
      "Training loss  0.784 in Step 500\n",
      "Training loss  0.764 in Step 600\n",
      "Training loss  0.773 in Step 700\n",
      "Training loss  0.775 in Step 800\n",
      "Training loss  0.779 in Step 900\n",
      "Training loss  0.781 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.752 in Step 1200\n",
      "Training loss  0.780 in Step 1300\n",
      "Training loss  0.770 in Step 1400\n",
      "Training loss  0.791 in Step 1500\n",
      "Training loss  0.775 in Step 1600\n",
      "Training loss  0.788 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.766 in Step 0\n",
      "Valid loss  0.774 in Step 100\n",
      "Valid loss  0.774 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 228\n",
      "Training loss  0.781 in Step 0\n",
      "Training loss  0.785 in Step 100\n",
      "Training loss  0.790 in Step 200\n",
      "Training loss  0.775 in Step 300\n",
      "Training loss  0.787 in Step 400\n",
      "Training loss  0.775 in Step 500\n",
      "Training loss  0.785 in Step 600\n",
      "Training loss  0.787 in Step 700\n",
      "Training loss  0.778 in Step 800\n",
      "Training loss  0.775 in Step 900\n",
      "Training loss  0.789 in Step 1000\n",
      "Training loss  0.796 in Step 1100\n",
      "Training loss  0.774 in Step 1200\n",
      "Training loss  0.773 in Step 1300\n",
      "Training loss  0.804 in Step 1400\n",
      "Training loss  0.774 in Step 1500\n",
      "Training loss  0.776 in Step 1600\n",
      "Training loss  0.792 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.768 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 229\n",
      "Training loss  0.784 in Step 0\n",
      "Training loss  0.758 in Step 100\n",
      "Training loss  0.772 in Step 200\n",
      "Training loss  0.779 in Step 300\n",
      "Training loss  0.771 in Step 400\n",
      "Training loss  0.760 in Step 500\n",
      "Training loss  0.769 in Step 600\n",
      "Training loss  0.768 in Step 700\n",
      "Training loss  0.794 in Step 800\n",
      "Training loss  0.774 in Step 900\n",
      "Training loss  0.778 in Step 1000\n",
      "Training loss  0.766 in Step 1100\n",
      "Training loss  0.779 in Step 1200\n",
      "Training loss  0.767 in Step 1300\n",
      "Training loss  0.771 in Step 1400\n",
      "Training loss  0.790 in Step 1500\n",
      "Training loss  0.759 in Step 1600\n",
      "Training loss  0.769 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.768 in Step 0\n",
      "Valid loss  0.772 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.771 in Step 400\n",
      "※※※Valid loss  0.778※※※\n",
      "Epoch 230\n",
      "Training loss  0.754 in Step 0\n",
      "Training loss  0.774 in Step 100\n",
      "Training loss  0.774 in Step 200\n",
      "Training loss  0.764 in Step 300\n",
      "Training loss  0.790 in Step 400\n",
      "Training loss  0.781 in Step 500\n",
      "Training loss  0.767 in Step 600\n",
      "Training loss  0.781 in Step 700\n",
      "Training loss  0.787 in Step 800\n",
      "Training loss  0.797 in Step 900\n",
      "Training loss  0.765 in Step 1000\n",
      "Training loss  0.783 in Step 1100\n",
      "Training loss  0.775 in Step 1200\n",
      "Training loss  0.776 in Step 1300\n",
      "Training loss  0.761 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.777 in Step 1600\n",
      "Training loss  0.769 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.769 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 231\n",
      "Training loss  0.787 in Step 0\n",
      "Training loss  0.763 in Step 100\n",
      "Training loss  0.770 in Step 200\n",
      "Training loss  0.762 in Step 300\n",
      "Training loss  0.772 in Step 400\n",
      "Training loss  0.784 in Step 500\n",
      "Training loss  0.781 in Step 600\n",
      "Training loss  0.782 in Step 700\n",
      "Training loss  0.768 in Step 800\n",
      "Training loss  0.786 in Step 900\n",
      "Training loss  0.771 in Step 1000\n",
      "Training loss  0.754 in Step 1100\n",
      "Training loss  0.763 in Step 1200\n",
      "Training loss  0.766 in Step 1300\n",
      "Training loss  0.765 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.799 in Step 1600\n",
      "Training loss  0.800 in Step 1700\n",
      "※※※Training loss  0.779※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.779 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.775 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 232\n",
      "Training loss  0.795 in Step 0\n",
      "Training loss  0.770 in Step 100\n",
      "Training loss  0.806 in Step 200\n",
      "Training loss  0.766 in Step 300\n",
      "Training loss  0.780 in Step 400\n",
      "Training loss  0.784 in Step 500\n",
      "Training loss  0.773 in Step 600\n",
      "Training loss  0.780 in Step 700\n",
      "Training loss  0.778 in Step 800\n",
      "Training loss  0.779 in Step 900\n",
      "Training loss  0.792 in Step 1000\n",
      "Training loss  0.778 in Step 1100\n",
      "Training loss  0.788 in Step 1200\n",
      "Training loss  0.763 in Step 1300\n",
      "Training loss  0.773 in Step 1400\n",
      "Training loss  0.776 in Step 1500\n",
      "Training loss  0.773 in Step 1600\n",
      "Training loss  0.768 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.772 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.773 in Step 300\n",
      "Valid loss  0.776 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 233\n",
      "Training loss  0.785 in Step 0\n",
      "Training loss  0.776 in Step 100\n",
      "Training loss  0.762 in Step 200\n",
      "Training loss  0.769 in Step 300\n",
      "Training loss  0.765 in Step 400\n",
      "Training loss  0.762 in Step 500\n",
      "Training loss  0.796 in Step 600\n",
      "Training loss  0.782 in Step 700\n",
      "Training loss  0.779 in Step 800\n",
      "Training loss  0.763 in Step 900\n",
      "Training loss  0.771 in Step 1000\n",
      "Training loss  0.783 in Step 1100\n",
      "Training loss  0.787 in Step 1200\n",
      "Training loss  0.779 in Step 1300\n",
      "Training loss  0.784 in Step 1400\n",
      "Training loss  0.764 in Step 1500\n",
      "Training loss  0.772 in Step 1600\n",
      "Training loss  0.773 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.770 in Step 0\n",
      "Valid loss  0.779 in Step 100\n",
      "Valid loss  0.776 in Step 200\n",
      "Valid loss  0.776 in Step 300\n",
      "Valid loss  0.776 in Step 400\n",
      "※※※Valid loss  0.783※※※\n",
      "Epoch 234\n",
      "Training loss  0.778 in Step 0\n",
      "Training loss  0.790 in Step 100\n",
      "Training loss  0.766 in Step 200\n",
      "Training loss  0.765 in Step 300\n",
      "Training loss  0.775 in Step 400\n",
      "Training loss  0.773 in Step 500\n",
      "Training loss  0.784 in Step 600\n",
      "Training loss  0.764 in Step 700\n",
      "Training loss  0.774 in Step 800\n",
      "Training loss  0.771 in Step 900\n",
      "Training loss  0.772 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.784 in Step 1200\n",
      "Training loss  0.785 in Step 1300\n",
      "Training loss  0.774 in Step 1400\n",
      "Training loss  0.777 in Step 1500\n",
      "Training loss  0.780 in Step 1600\n",
      "Training loss  0.782 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.770 in Step 0\n",
      "Valid loss  0.776 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 235\n",
      "Training loss  0.778 in Step 0\n",
      "Training loss  0.761 in Step 100\n",
      "Training loss  0.772 in Step 200\n",
      "Training loss  0.766 in Step 300\n",
      "Training loss  0.783 in Step 400\n",
      "Training loss  0.778 in Step 500\n",
      "Training loss  0.772 in Step 600\n",
      "Training loss  0.772 in Step 700\n",
      "Training loss  0.795 in Step 800\n",
      "Training loss  0.772 in Step 900\n",
      "Training loss  0.767 in Step 1000\n",
      "Training loss  0.771 in Step 1100\n",
      "Training loss  0.791 in Step 1200\n",
      "Training loss  0.775 in Step 1300\n",
      "Training loss  0.762 in Step 1400\n",
      "Training loss  0.777 in Step 1500\n",
      "Training loss  0.784 in Step 1600\n",
      "Training loss  0.783 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.773 in Step 0\n",
      "Valid loss  0.776 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.773 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 236\n",
      "Training loss  0.793 in Step 0\n",
      "Training loss  0.791 in Step 100\n",
      "Training loss  0.786 in Step 200\n",
      "Training loss  0.764 in Step 300\n",
      "Training loss  0.774 in Step 400\n",
      "Training loss  0.765 in Step 500\n",
      "Training loss  0.779 in Step 600\n",
      "Training loss  0.790 in Step 700\n",
      "Training loss  0.775 in Step 800\n",
      "Training loss  0.782 in Step 900\n",
      "Training loss  0.773 in Step 1000\n",
      "Training loss  0.769 in Step 1100\n",
      "Training loss  0.775 in Step 1200\n",
      "Training loss  0.784 in Step 1300\n",
      "Training loss  0.784 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.796 in Step 1600\n",
      "Training loss  0.783 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.766 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.775 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 237\n",
      "Training loss  0.790 in Step 0\n",
      "Training loss  0.767 in Step 100\n",
      "Training loss  0.775 in Step 200\n",
      "Training loss  0.774 in Step 300\n",
      "Training loss  0.775 in Step 400\n",
      "Training loss  0.761 in Step 500\n",
      "Training loss  0.778 in Step 600\n",
      "Training loss  0.783 in Step 700\n",
      "Training loss  0.779 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.765 in Step 1000\n",
      "Training loss  0.789 in Step 1100\n",
      "Training loss  0.779 in Step 1200\n",
      "Training loss  0.779 in Step 1300\n",
      "Training loss  0.765 in Step 1400\n",
      "Training loss  0.773 in Step 1500\n",
      "Training loss  0.782 in Step 1600\n",
      "Training loss  0.760 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.766 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.770 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 238\n",
      "Training loss  0.783 in Step 0\n",
      "Training loss  0.779 in Step 100\n",
      "Training loss  0.781 in Step 200\n",
      "Training loss  0.783 in Step 300\n",
      "Training loss  0.775 in Step 400\n",
      "Training loss  0.791 in Step 500\n",
      "Training loss  0.776 in Step 600\n",
      "Training loss  0.776 in Step 700\n",
      "Training loss  0.782 in Step 800\n",
      "Training loss  0.767 in Step 900\n",
      "Training loss  0.792 in Step 1000\n",
      "Training loss  0.783 in Step 1100\n",
      "Training loss  0.782 in Step 1200\n",
      "Training loss  0.795 in Step 1300\n",
      "Training loss  0.772 in Step 1400\n",
      "Training loss  0.768 in Step 1500\n",
      "Training loss  0.770 in Step 1600\n",
      "Training loss  0.793 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.771 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 239\n",
      "Training loss  0.805 in Step 0\n",
      "Training loss  0.799 in Step 100\n",
      "Training loss  0.776 in Step 200\n",
      "Training loss  0.771 in Step 300\n",
      "Training loss  0.769 in Step 400\n",
      "Training loss  0.768 in Step 500\n",
      "Training loss  0.809 in Step 600\n",
      "Training loss  0.781 in Step 700\n",
      "Training loss  0.758 in Step 800\n",
      "Training loss  0.769 in Step 900\n",
      "Training loss  0.785 in Step 1000\n",
      "Training loss  0.780 in Step 1100\n",
      "Training loss  0.748 in Step 1200\n",
      "Training loss  0.771 in Step 1300\n",
      "Training loss  0.766 in Step 1400\n",
      "Training loss  0.781 in Step 1500\n",
      "Training loss  0.772 in Step 1600\n",
      "Training loss  0.778 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.776 in Step 100\n",
      "Valid loss  0.772 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 240\n",
      "Training loss  0.758 in Step 0\n",
      "Training loss  0.789 in Step 100\n",
      "Training loss  0.809 in Step 200\n",
      "Training loss  0.794 in Step 300\n",
      "Training loss  0.774 in Step 400\n",
      "Training loss  0.779 in Step 500\n",
      "Training loss  0.770 in Step 600\n",
      "Training loss  0.780 in Step 700\n",
      "Training loss  0.781 in Step 800\n",
      "Training loss  0.778 in Step 900\n",
      "Training loss  0.776 in Step 1000\n",
      "Training loss  0.771 in Step 1100\n",
      "Training loss  0.777 in Step 1200\n",
      "Training loss  0.772 in Step 1300\n",
      "Training loss  0.789 in Step 1400\n",
      "Training loss  0.780 in Step 1500\n",
      "Training loss  0.766 in Step 1600\n",
      "Training loss  0.777 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.769 in Step 0\n",
      "Valid loss  0.776 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 241\n",
      "Training loss  0.776 in Step 0\n",
      "Training loss  0.784 in Step 100\n",
      "Training loss  0.770 in Step 200\n",
      "Training loss  0.776 in Step 300\n",
      "Training loss  0.769 in Step 400\n",
      "Training loss  0.776 in Step 500\n",
      "Training loss  0.765 in Step 600\n",
      "Training loss  0.760 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.780 in Step 900\n",
      "Training loss  0.778 in Step 1000\n",
      "Training loss  0.767 in Step 1100\n",
      "Training loss  0.763 in Step 1200\n",
      "Training loss  0.765 in Step 1300\n",
      "Training loss  0.776 in Step 1400\n",
      "Training loss  0.755 in Step 1500\n",
      "Training loss  0.789 in Step 1600\n",
      "Training loss  0.788 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.771 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.771 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.780※※※\n",
      "Epoch 242\n",
      "Training loss  0.779 in Step 0\n",
      "Training loss  0.780 in Step 100\n",
      "Training loss  0.787 in Step 200\n",
      "Training loss  0.782 in Step 300\n",
      "Training loss  0.769 in Step 400\n",
      "Training loss  0.797 in Step 500\n",
      "Training loss  0.787 in Step 600\n",
      "Training loss  0.789 in Step 700\n",
      "Training loss  0.787 in Step 800\n",
      "Training loss  0.788 in Step 900\n",
      "Training loss  0.790 in Step 1000\n",
      "Training loss  0.778 in Step 1100\n",
      "Training loss  0.782 in Step 1200\n",
      "Training loss  0.762 in Step 1300\n",
      "Training loss  0.776 in Step 1400\n",
      "Training loss  0.775 in Step 1500\n",
      "Training loss  0.778 in Step 1600\n",
      "Training loss  0.754 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.766 in Step 0\n",
      "Valid loss  0.772 in Step 100\n",
      "Valid loss  0.770 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.771 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 243\n",
      "Training loss  0.777 in Step 0\n",
      "Training loss  0.776 in Step 100\n",
      "Training loss  0.774 in Step 200\n",
      "Training loss  0.779 in Step 300\n",
      "Training loss  0.798 in Step 400\n",
      "Training loss  0.770 in Step 500\n",
      "Training loss  0.785 in Step 600\n",
      "Training loss  0.777 in Step 700\n",
      "Training loss  0.772 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.775 in Step 1000\n",
      "Training loss  0.784 in Step 1100\n",
      "Training loss  0.781 in Step 1200\n",
      "Training loss  0.777 in Step 1300\n",
      "Training loss  0.756 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.768 in Step 1600\n",
      "Training loss  0.771 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.765 in Step 0\n",
      "Valid loss  0.774 in Step 100\n",
      "Valid loss  0.769 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.778※※※\n",
      "Epoch 244\n",
      "Training loss  0.784 in Step 0\n",
      "Training loss  0.775 in Step 100\n",
      "Training loss  0.787 in Step 200\n",
      "Training loss  0.781 in Step 300\n",
      "Training loss  0.769 in Step 400\n",
      "Training loss  0.785 in Step 500\n",
      "Training loss  0.778 in Step 600\n",
      "Training loss  0.774 in Step 700\n",
      "Training loss  0.769 in Step 800\n",
      "Training loss  0.779 in Step 900\n",
      "Training loss  0.774 in Step 1000\n",
      "Training loss  0.764 in Step 1100\n",
      "Training loss  0.786 in Step 1200\n",
      "Training loss  0.782 in Step 1300\n",
      "Training loss  0.771 in Step 1400\n",
      "Training loss  0.775 in Step 1500\n",
      "Training loss  0.781 in Step 1600\n",
      "Training loss  0.775 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.770 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.774 in Step 200\n",
      "Valid loss  0.776 in Step 300\n",
      "Valid loss  0.774 in Step 400\n",
      "※※※Valid loss  0.782※※※\n",
      "Epoch 245\n",
      "Training loss  0.797 in Step 0\n",
      "Training loss  0.795 in Step 100\n",
      "Training loss  0.771 in Step 200\n",
      "Training loss  0.769 in Step 300\n",
      "Training loss  0.776 in Step 400\n",
      "Training loss  0.768 in Step 500\n",
      "Training loss  0.771 in Step 600\n",
      "Training loss  0.773 in Step 700\n",
      "Training loss  0.747 in Step 800\n",
      "Training loss  0.787 in Step 900\n",
      "Training loss  0.774 in Step 1000\n",
      "Training loss  0.782 in Step 1100\n",
      "Training loss  0.783 in Step 1200\n",
      "Training loss  0.746 in Step 1300\n",
      "Training loss  0.779 in Step 1400\n",
      "Training loss  0.783 in Step 1500\n",
      "Training loss  0.796 in Step 1600\n",
      "Training loss  0.768 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.764 in Step 0\n",
      "Valid loss  0.773 in Step 100\n",
      "Valid loss  0.769 in Step 200\n",
      "Valid loss  0.771 in Step 300\n",
      "Valid loss  0.771 in Step 400\n",
      "※※※Valid loss  0.778※※※\n",
      "Epoch 246\n",
      "Training loss  0.779 in Step 0\n",
      "Training loss  0.779 in Step 100\n",
      "Training loss  0.759 in Step 200\n",
      "Training loss  0.756 in Step 300\n",
      "Training loss  0.771 in Step 400\n",
      "Training loss  0.790 in Step 500\n",
      "Training loss  0.797 in Step 600\n",
      "Training loss  0.778 in Step 700\n",
      "Training loss  0.784 in Step 800\n",
      "Training loss  0.777 in Step 900\n",
      "Training loss  0.781 in Step 1000\n",
      "Training loss  0.771 in Step 1100\n",
      "Training loss  0.750 in Step 1200\n",
      "Training loss  0.779 in Step 1300\n",
      "Training loss  0.781 in Step 1400\n",
      "Training loss  0.781 in Step 1500\n",
      "Training loss  0.768 in Step 1600\n",
      "Training loss  0.773 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.769 in Step 0\n",
      "Valid loss  0.777 in Step 100\n",
      "Valid loss  0.773 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.773 in Step 400\n",
      "※※※Valid loss  0.781※※※\n",
      "Epoch 247\n",
      "Training loss  0.776 in Step 0\n",
      "Training loss  0.756 in Step 100\n",
      "Training loss  0.773 in Step 200\n",
      "Training loss  0.766 in Step 300\n",
      "Training loss  0.772 in Step 400\n",
      "Training loss  0.780 in Step 500\n",
      "Training loss  0.783 in Step 600\n",
      "Training loss  0.770 in Step 700\n",
      "Training loss  0.789 in Step 800\n",
      "Training loss  0.780 in Step 900\n",
      "Training loss  0.791 in Step 1000\n",
      "Training loss  0.774 in Step 1100\n",
      "Training loss  0.791 in Step 1200\n",
      "Training loss  0.769 in Step 1300\n",
      "Training loss  0.773 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.774 in Step 1600\n",
      "Training loss  0.772 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.767 in Step 0\n",
      "Valid loss  0.775 in Step 100\n",
      "Valid loss  0.769 in Step 200\n",
      "Valid loss  0.772 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.779※※※\n",
      "Epoch 248\n",
      "Training loss  0.766 in Step 0\n",
      "Training loss  0.786 in Step 100\n",
      "Training loss  0.762 in Step 200\n",
      "Training loss  0.775 in Step 300\n",
      "Training loss  0.761 in Step 400\n",
      "Training loss  0.784 in Step 500\n",
      "Training loss  0.783 in Step 600\n",
      "Training loss  0.767 in Step 700\n",
      "Training loss  0.786 in Step 800\n",
      "Training loss  0.784 in Step 900\n",
      "Training loss  0.777 in Step 1000\n",
      "Training loss  0.777 in Step 1100\n",
      "Training loss  0.757 in Step 1200\n",
      "Training loss  0.791 in Step 1300\n",
      "Training loss  0.796 in Step 1400\n",
      "Training loss  0.784 in Step 1500\n",
      "Training loss  0.759 in Step 1600\n",
      "Training loss  0.773 in Step 1700\n",
      "※※※Training loss  0.778※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.766 in Step 0\n",
      "Valid loss  0.773 in Step 100\n",
      "Valid loss  0.770 in Step 200\n",
      "Valid loss  0.771 in Step 300\n",
      "Valid loss  0.772 in Step 400\n",
      "※※※Valid loss  0.778※※※\n",
      "Epoch 249\n",
      "Training loss  0.771 in Step 0\n",
      "Training loss  0.781 in Step 100\n",
      "Training loss  0.786 in Step 200\n",
      "Training loss  0.773 in Step 300\n",
      "Training loss  0.784 in Step 400\n",
      "Training loss  0.774 in Step 500\n",
      "Training loss  0.780 in Step 600\n",
      "Training loss  0.787 in Step 700\n",
      "Training loss  0.772 in Step 800\n",
      "Training loss  0.767 in Step 900\n",
      "Training loss  0.804 in Step 1000\n",
      "Training loss  0.788 in Step 1100\n",
      "Training loss  0.772 in Step 1200\n",
      "Training loss  0.761 in Step 1300\n",
      "Training loss  0.791 in Step 1400\n",
      "Training loss  0.779 in Step 1500\n",
      "Training loss  0.774 in Step 1600\n",
      "Training loss  0.768 in Step 1700\n",
      "※※※Training loss  0.777※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.773 in Step 0\n",
      "Valid loss  0.778 in Step 100\n",
      "Valid loss  0.777 in Step 200\n",
      "Valid loss  0.774 in Step 300\n",
      "Valid loss  0.781 in Step 400\n",
      "※※※Valid loss  0.785※※※\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "train_losses.save()\n",
    "\n",
    "valid_losses.save()\n",
    "\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f28e0258690>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This model should converge to loss around 0.78. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
