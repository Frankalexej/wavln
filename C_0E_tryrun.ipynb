{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from C_0E_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = \"0526172101-1\"\n",
    "train_name = \"C_0E\"\n",
    "model_save_dir = os.path.join(model_save_, f\"{train_name}-{ts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dir = model_save_dir\n",
    "model_type = \"mtl\"\n",
    "condition = \"u\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.path.join(hyper_dir, model_type, condition)\n",
    "mk(model_save_dir)\n",
    "\n",
    "# Loss Recording\n",
    "train_losses = ListRecorder(os.path.join(model_save_dir, \"train.loss\"))\n",
    "train_recon_losses = ListRecorder(os.path.join(model_save_dir, \"train.recon.loss\"))\n",
    "train_embedding_losses = ListRecorder(os.path.join(model_save_dir, \"train.embedding.loss\"))\n",
    "train_commitment_losses = ListRecorder(os.path.join(model_save_dir, \"train.commitment.loss\"))\n",
    "\n",
    "valid_losses = ListRecorder(os.path.join(model_save_dir, \"valid.loss\"))\n",
    "valid_recon_losses = ListRecorder(os.path.join(model_save_dir, \"valid.recon.loss\"))\n",
    "valid_embedding_losses = ListRecorder(os.path.join(model_save_dir, \"valid.embedding.loss\"))\n",
    "valid_commitment_losses = ListRecorder(os.path.join(model_save_dir, \"valid.commitment.loss\"))\n",
    "\n",
    "# In C we take onlyST to record the phenomenon-target dataset\n",
    "onlyST_valid_losses = ListRecorder(os.path.join(model_save_dir, \"valid_onlyST.loss\"))\n",
    "onlyST_valid_recon_losses = ListRecorder(os.path.join(model_save_dir, \"valid_onlyST.recon.loss\"))\n",
    "onlyST_valid_embedding_losses = ListRecorder(os.path.join(model_save_dir, \"valid_onlyST.embedding.loss\"))\n",
    "onlyST_valid_commitment_losses = ListRecorder(os.path.join(model_save_dir, \"valid_onlyST.commitment.loss\"))\n",
    "\n",
    "text_hist = HistRecorder(os.path.join(model_save_dir, \"trainhist.txt\"))\n",
    "\n",
    "# Recording Directory\n",
    "phone_rec_dir = train_cut_phone_\n",
    "word_rec_dir = train_cut_word_\n",
    "train_guide_path = os.path.join(src_, \"guide_train.csv\")\n",
    "valid_guide_path = os.path.join(src_, \"guide_validation.csv\")\n",
    "\n",
    "# Load TokenMap to map the phoneme to the index\n",
    "with open(os.path.join(src_, \"no-stress-seg.dict\"), \"rb\") as file:\n",
    "    # Load the object from the file\n",
    "    mylist = pickle.load(file)\n",
    "    mylist = [\"BLANK\"] + mylist\n",
    "    mylist = mylist + [\"SIL\"]   # this is to fit STV vs #TV\n",
    "\n",
    "# Now you can use the loaded object\n",
    "mymap = TokenMap(mylist)\n",
    "class_dim = mymap.token_num()\n",
    "ctc_size_list = {'hid': INTER_DIM_2, 'class': class_dim}\n",
    "\n",
    "# Initialize Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "masked_loss = MaskedLoss(loss_fn=nn.MSELoss(reduction=\"none\"))\n",
    "ctc_loss = nn.CTCLoss(blank=mymap.encode(\"BLANK\"))\n",
    "model_loss = AlphaCombineLoss(masked_loss, ctc_loss, alpha=0.2)\n",
    "if model_type == \"mtl\":\n",
    "    model = AEPPV1(enc_size_list=ENC_SIZE_LIST, \n",
    "                dec_size_list=DEC_SIZE_LIST, \n",
    "                ctc_decoder_size_list=ctc_size_list,\n",
    "                num_layers=NUM_LAYERS, dropout=DROPOUT)\n",
    "else: \n",
    "    raise Exception(\"Model type not supported! \")\n",
    "\n",
    "model.to(device)\n",
    "initialize_model(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model_str = str(model)\n",
    "model_txt_path = os.path.join(model_save_dir, \"model.txt\")\n",
    "with open(model_txt_path, \"w\") as f:\n",
    "    f.write(model_str)\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "guide_path = os.path.join(hyper_dir, \"guides\")\n",
    "# train_loader = load_data_general(TrainDataset, \n",
    "#                                     word_rec_dir, train_guide_path, load=\"train\", select=0.3, sampled=False)\n",
    "valid_loader = load_data_general(TrainDataset, \n",
    "                                    word_rec_dir, valid_guide_path, load=\"valid\", select=0.3, sampled=False)\n",
    "onlyST_valid_loader = load_data_phenomenon(TestDataset, \n",
    "                                            phone_rec_dir, guide_path, load=\"valid\", select=\"both\", sampled=True, word_guide_=valid_guide_path)\n",
    "\n",
    "num_epochs = 100\n",
    "l_w_embedding = 1\n",
    "l_w_commitment = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaCombineLoss:\n",
    "    def __init__(self, recon_loss, pred_loss, alpha=0.1):\n",
    "        self.recon_loss = recon_loss\n",
    "        self.pred_loss = pred_loss\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def get_loss(self, y_hat_recon, y_recon, y_hat_pred, y_pred, x_lens, y_pred_lens, mask): \n",
    "        reconstruction_loss = self.recon_loss.get_loss(y_hat_recon, y_recon, mask)\n",
    "        prediction_loss = self.pred_loss(y_hat_pred, y_pred, x_lens, y_pred_lens)\n",
    "\n",
    "        # Compute the regularization term based on the number of boundaries\n",
    "        prediction_loss_term = self.alpha * prediction_loss\n",
    "\n",
    "        return reconstruction_loss + prediction_loss_term, (reconstruction_loss, prediction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     text_hist.print(\"Epoch {}\".format(epoch))\n",
    "#     model.train()\n",
    "#     train_loss = 0.\n",
    "#     train_cumulative_l_reconstruct = 0.\n",
    "#     train_cumulative_l_embedding = 0.\n",
    "#     train_cumulative_l_commitment = 0.\n",
    "#     train_num = len(train_loader.dataset)    # train_loader\n",
    "#     for idx, (x, x_lens, y_preds, y_preds_lens) in enumerate(train_loader):\n",
    "#         current_batch_size = x.shape[0]\n",
    "#         # y_lens should be the same as x_lens\n",
    "#         optimizer.zero_grad()\n",
    "#         x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "#         y_recon = x\n",
    "#         x = x.to(device)\n",
    "#         y_recon = y_recon.to(device)\n",
    "#         y_preds = y_preds.to(device)\n",
    "#         y_preds = y_preds.long()\n",
    "\n",
    "#         (x_hat_recon, y_hat_preds), (attn_w_recon, attn_w_preds), (ze, zq) = model(x, x_lens, x_mask)\n",
    "#         y_hat_preds = y_hat_preds.permute(1, 0, 2)\n",
    "\n",
    "        \n",
    "\n",
    "#         l_alpha, (l_reconstruct, l_prediction) = model_loss.get_loss(x_hat_recon, y_recon, \n",
    "#                                                                         y_hat_preds, y_preds, \n",
    "#                                                                         x_lens, y_preds_lens, \n",
    "#                                                                         x_mask)\n",
    "#         if model_type == \"vqvae\":\n",
    "#             l_embedding = model_loss.get_loss(ze.detach(), zq, x_mask)\n",
    "#             l_commitment = model_loss.get_loss(ze, zq.detach(), x_mask)\n",
    "#             loss = l_alpha + \\\n",
    "#                 l_w_embedding * l_embedding + l_w_commitment * l_commitment\n",
    "#         elif model_type == \"mtl\":\n",
    "#             l_embedding = l_prediction\n",
    "#             l_commitment = l_prediction\n",
    "#             loss = l_alpha\n",
    "#         else: \n",
    "#             l_embedding = torch.tensor(0)\n",
    "#             l_commitment = torch.tensor(0)\n",
    "#             loss = l_alpha\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item() * current_batch_size\n",
    "#         train_cumulative_l_reconstruct += l_reconstruct.item() * current_batch_size\n",
    "#         train_cumulative_l_embedding += l_embedding.item() * current_batch_size\n",
    "#         train_cumulative_l_commitment += l_commitment.item() * current_batch_size\n",
    "\n",
    "#         if idx % 100 == 0:\n",
    "#             text_hist.print(f\"\"\"Training step {idx} loss {loss: .3f} \\t recon {l_reconstruct: .3f} \\t embed {l_embedding: .3f} \\t commit {l_commitment: .3f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Valid (ST + T)\n",
    "# model.eval()\n",
    "# valid_loss = 0.\n",
    "# valid_cumulative_l_reconstruct = 0.\n",
    "# valid_cumulative_l_embedding = 0.\n",
    "# valid_cumulative_l_commitment = 0.\n",
    "# valid_num = len(valid_loader.dataset)\n",
    "# for idx, (x, x_lens, y_preds, y_preds_lens) in enumerate(valid_loader):\n",
    "#     current_batch_size = x.shape[0]\n",
    "#     x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "\n",
    "#     y_recon = x\n",
    "#     x = x.to(device)\n",
    "#     y_recon = y_recon.to(device)\n",
    "#     y_preds = y_preds.to(device)\n",
    "#     y_preds = y_preds.long()\n",
    "\n",
    "#     (x_hat_recon, y_hat_preds), (attn_w_recon, attn_w_preds), (ze, zq) = model(x, x_lens, x_mask)\n",
    "#     y_hat_preds = y_hat_preds.permute(1, 0, 2)\n",
    "\n",
    "#     l_alpha, (l_reconstruct, l_prediction) = model_loss.get_loss(x_hat_recon, y_recon, \n",
    "#                                                                     y_hat_preds, y_preds, \n",
    "#                                                                     x_lens, y_preds_lens, \n",
    "#                                                                     x_mask)\n",
    "#     if model_type == \"vqvae\":\n",
    "#         l_embedding = model_loss.get_loss(ze.detach(), zq, x_mask)\n",
    "#         l_commitment = model_loss.get_loss(ze, zq.detach(), x_mask)\n",
    "#         loss = l_alpha + \\\n",
    "#             l_w_embedding * l_embedding + l_w_commitment * l_commitment\n",
    "#     elif model_type == \"mtl\":\n",
    "#         l_embedding = l_prediction\n",
    "#         l_commitment = l_prediction\n",
    "#         loss = l_alpha\n",
    "#     else: \n",
    "#         l_embedding = torch.tensor(0)\n",
    "#         l_commitment = torch.tensor(0)\n",
    "#         loss = l_alpha\n",
    "\n",
    "#     valid_loss += loss.item() * current_batch_size\n",
    "#     valid_cumulative_l_reconstruct += l_reconstruct.item() * current_batch_size\n",
    "#     valid_cumulative_l_embedding += l_embedding.item() * current_batch_size\n",
    "#     valid_cumulative_l_commitment += l_commitment.item() * current_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "onlyST_valid_loss = 0.\n",
    "onlyST_valid_cumulative_l_reconstruct = 0.\n",
    "onlyST_valid_cumulative_l_embedding = 0.\n",
    "onlyST_valid_cumulative_l_commitment = 0.\n",
    "onlyST_valid_num = len(onlyST_valid_loader.dataset)\n",
    "for idx, ((x, y_preds), (x_lens, y_preds_lens), pt, sn) in enumerate(onlyST_valid_loader):\n",
    "    current_batch_size = x.shape[0]\n",
    "    x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "\n",
    "    y_recon = x\n",
    "    x = x.to(device)\n",
    "    y_recon = y_recon.to(device)\n",
    "    y_preds = y_preds.to(device)\n",
    "    y_preds = y_preds.long()\n",
    "\n",
    "    (x_hat_recon, y_hat_preds), (attn_w_recon, attn_w_preds), (ze, zq) = model(x, x_lens, x_mask)\n",
    "    y_hat_preds = y_hat_preds.permute(1, 0, 2)\n",
    "\n",
    "    l_alpha, (l_reconstruct, l_prediction) = model_loss.get_loss(x_hat_recon, y_recon, \n",
    "                                                                    y_hat_preds, y_preds, \n",
    "                                                                    x_lens, y_preds_lens, \n",
    "                                                                    x_mask)\n",
    "    if model_type == \"vqvae\":\n",
    "        l_embedding = model_loss.get_loss(ze.detach(), zq, x_mask)\n",
    "        l_commitment = model_loss.get_loss(ze, zq.detach(), x_mask)\n",
    "        loss = l_alpha + \\\n",
    "            l_w_embedding * l_embedding + l_w_commitment * l_commitment\n",
    "    elif model_type == \"mtl\":\n",
    "        l_embedding = l_prediction\n",
    "        l_commitment = l_prediction\n",
    "        loss = l_alpha\n",
    "    else: \n",
    "        l_embedding = torch.tensor(0)\n",
    "        l_commitment = torch.tensor(0)\n",
    "        loss = l_alpha\n",
    "\n",
    "    onlyST_valid_loss += loss.item() * current_batch_size\n",
    "    onlyST_valid_cumulative_l_reconstruct += l_reconstruct.item() * current_batch_size\n",
    "    onlyST_valid_cumulative_l_embedding += l_embedding.item() * current_batch_size\n",
    "    onlyST_valid_cumulative_l_commitment += l_commitment.item() * current_batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
