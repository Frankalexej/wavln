{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Direct - English\n",
    "Version 1: In this version we make the model \"simple\": make the encoder RNN into normal RNN first and try to see the result.  \n",
    "Version 2: Learning is not very much. Following Dr Coupe's advice we try simpler model structure.   \n",
    "Version 3: A simple trial training with Mel spectrogram instead of MFCC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PhxLearner, SimplerPhxLearner\n",
    "from my_dataset import DS_Tools\n",
    "from dataset import SeqDataset, MelSpecTransform\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from recorder import *\n",
    "from loss import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIM = 64\n",
    "OUTPUT_DIM = 64\n",
    "\n",
    "INTER_DIM_0 = 32\n",
    "INTER_DIM_1 = 16\n",
    "INTER_DIM_2 = 3\n",
    "\n",
    "ENC_SIZE_LIST = [INPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "DEC_SIZE_LIST = [OUTPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 64\n",
    "\n",
    "LOADER_WORKER = 16\n",
    "# LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "masked_recon_loss = MaskedLoss(recon_loss)\n",
    "model_loss = masked_recon_loss\n",
    "\n",
    "model = SimplerPhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=2)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplerPhxLearner(\n",
       "  (encoder): RLEncoder(\n",
       "    (rnn): LSTM(64, 16, num_layers=2, batch_first=True)\n",
       "    (lin_2): LinearPack(\n",
       "      (linear): Linear(in_features=16, out_features=3, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): RALDecoder(\n",
       "    (rnn): LSTM(64, 3, num_layers=2, batch_first=True)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (lin_3): LinearPack(\n",
       "      (linear): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8691"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "ts = \"0918192113\"\n",
    "stop_epoch = \"74\"\n",
    "# ts = str(get_timestamp())\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "\n",
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "# READ = False\n",
    "READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    valid_losses.read()\n",
    "    train_losses.read()\n",
    "\n",
    "    model_raw_name = \"PT_{}_{}_full\".format(ts, stop_epoch)\n",
    "    model_name = model_raw_name + \".pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [],
   "source": [
    "mytrans = MelSpecTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT, n_mels=N_MELS)\n",
    "ds = SeqDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "\n",
    "\n",
    "if READ: \n",
    "    valid_ds_indices = DS_Tools.read_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)))\n",
    "    all_indices = list(range(len(ds)))\n",
    "    train_ds_indices = list(set(all_indices).difference(set(valid_ds_indices)))\n",
    "\n",
    "    train_ds = torch.utils.data.Subset(ds, train_ds_indices)\n",
    "    valid_ds = torch.utils.data.Subset(ds, valid_ds_indices)\n",
    "else: \n",
    "    train_len = int(0.8 * len(ds))\n",
    "    valid_len = len(ds) - train_len\n",
    "\n",
    "    # Randomly split the dataset into train and validation sets\n",
    "    train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "    DS_Tools.save_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)), valid_ds.indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=SeqDataset.collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=LOADER_WORKER, collate_fn=SeqDataset.collate_fn)\n",
    "valid_num = len(valid_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "BASE = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y = x \n",
    "            \n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            # 这个函数计算的是全局梯度范数\n",
    "            # torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            # parameters: an iterable of Variables that will have gradients normalized\n",
    "            # max_norm: max norm of the gradients(阈值设定)\n",
    "            # norm_type: type of the used p-norm. Can be'inf'for infinity norm(定义范数类型)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            y = x    # extract MFCC-only data\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75\n",
      "Training loss  0.685 in Step 0\n",
      "Training loss  0.686 in Step 100\n",
      "Training loss  0.708 in Step 200\n",
      "Training loss  0.710 in Step 300\n",
      "Training loss  0.685 in Step 400\n",
      "Training loss  0.678 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.698 in Step 700\n",
      "Training loss  0.680 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.691 in Step 1100\n",
      "Training loss  0.684 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.678 in Step 1400\n",
      "Training loss  0.685 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.707 in Step 1700\n",
      "※※※Training loss  0.693※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.679 in Step 0\n",
      "Valid loss  0.689 in Step 100\n",
      "Valid loss  0.686 in Step 200\n",
      "Valid loss  0.711 in Step 300\n",
      "Valid loss  0.714 in Step 400\n",
      "※※※Valid loss  0.694※※※\n",
      "Epoch 76\n",
      "Training loss  0.713 in Step 0\n",
      "Training loss  0.698 in Step 100\n",
      "Training loss  0.678 in Step 200\n",
      "Training loss  0.702 in Step 300\n",
      "Training loss  0.685 in Step 400\n",
      "Training loss  0.677 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.712 in Step 700\n",
      "Training loss  0.678 in Step 800\n",
      "Training loss  0.702 in Step 900\n",
      "Training loss  0.687 in Step 1000\n",
      "Training loss  0.712 in Step 1100\n",
      "Training loss  0.695 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.676 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.692※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.679 in Step 0\n",
      "Valid loss  0.686 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.706 in Step 300\n",
      "Valid loss  0.713 in Step 400\n",
      "※※※Valid loss  0.692※※※\n",
      "Epoch 77\n",
      "Training loss  0.685 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.682 in Step 300\n",
      "Training loss  0.697 in Step 400\n",
      "Training loss  0.718 in Step 500\n",
      "Training loss  0.668 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.683 in Step 800\n",
      "Training loss  0.683 in Step 900\n",
      "Training loss  0.699 in Step 1000\n",
      "Training loss  0.688 in Step 1100\n",
      "Training loss  0.691 in Step 1200\n",
      "Training loss  0.679 in Step 1300\n",
      "Training loss  0.694 in Step 1400\n",
      "Training loss  0.708 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.696 in Step 1700\n",
      "※※※Training loss  0.692※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.673 in Step 0\n",
      "Valid loss  0.685 in Step 100\n",
      "Valid loss  0.684 in Step 200\n",
      "Valid loss  0.706 in Step 300\n",
      "Valid loss  0.707 in Step 400\n",
      "※※※Valid loss  0.691※※※\n",
      "Epoch 78\n",
      "Training loss  0.697 in Step 0\n",
      "Training loss  0.707 in Step 100\n",
      "Training loss  0.698 in Step 200\n",
      "Training loss  0.690 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.709 in Step 600\n",
      "Training loss  0.716 in Step 700\n",
      "Training loss  0.697 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.708 in Step 1000\n",
      "Training loss  0.682 in Step 1100\n",
      "Training loss  0.675 in Step 1200\n",
      "Training loss  0.693 in Step 1300\n",
      "Training loss  0.684 in Step 1400\n",
      "Training loss  0.697 in Step 1500\n",
      "Training loss  0.686 in Step 1600\n",
      "Training loss  0.699 in Step 1700\n",
      "※※※Training loss  0.692※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.680 in Step 0\n",
      "Valid loss  0.688 in Step 100\n",
      "Valid loss  0.684 in Step 200\n",
      "Valid loss  0.717 in Step 300\n",
      "Valid loss  0.716 in Step 400\n",
      "※※※Valid loss  0.694※※※\n",
      "Epoch 79\n",
      "Training loss  0.683 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.717 in Step 300\n",
      "Training loss  0.701 in Step 400\n",
      "Training loss  0.698 in Step 500\n",
      "Training loss  0.699 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.670 in Step 800\n",
      "Training loss  0.699 in Step 900\n",
      "Training loss  0.699 in Step 1000\n",
      "Training loss  0.679 in Step 1100\n",
      "Training loss  0.689 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.708 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.679 in Step 0\n",
      "Valid loss  0.686 in Step 100\n",
      "Valid loss  0.682 in Step 200\n",
      "Valid loss  0.711 in Step 300\n",
      "Valid loss  0.713 in Step 400\n",
      "※※※Valid loss  0.693※※※\n",
      "Epoch 80\n",
      "Training loss  0.671 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.701 in Step 200\n",
      "Training loss  0.681 in Step 300\n",
      "Training loss  0.699 in Step 400\n",
      "Training loss  0.683 in Step 500\n",
      "Training loss  0.694 in Step 600\n",
      "Training loss  0.691 in Step 700\n",
      "Training loss  0.665 in Step 800\n",
      "Training loss  0.687 in Step 900\n",
      "Training loss  0.681 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.676 in Step 1400\n",
      "Training loss  0.697 in Step 1500\n",
      "Training loss  0.717 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.673 in Step 0\n",
      "Valid loss  0.688 in Step 100\n",
      "Valid loss  0.683 in Step 200\n",
      "Valid loss  0.714 in Step 300\n",
      "Valid loss  0.712 in Step 400\n",
      "※※※Valid loss  0.694※※※\n",
      "Epoch 81\n",
      "Training loss  0.680 in Step 0\n",
      "Training loss  0.660 in Step 100\n",
      "Training loss  0.686 in Step 200\n",
      "Training loss  0.694 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.694 in Step 500\n",
      "Training loss  0.697 in Step 600\n",
      "Training loss  0.703 in Step 700\n",
      "Training loss  0.677 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.708 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.684 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.685 in Step 1500\n",
      "Training loss  0.687 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.673 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.685 in Step 200\n",
      "Valid loss  0.702 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.691※※※\n",
      "Epoch 82\n",
      "Training loss  0.696 in Step 0\n",
      "Training loss  0.697 in Step 100\n",
      "Training loss  0.696 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.695 in Step 400\n",
      "Training loss  0.683 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.667 in Step 700\n",
      "Training loss  0.706 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.679 in Step 1000\n",
      "Training loss  0.706 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.708 in Step 1300\n",
      "Training loss  0.690 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.694 in Step 1600\n",
      "Training loss  0.676 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.680 in Step 200\n",
      "Valid loss  0.704 in Step 300\n",
      "Valid loss  0.708 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 83\n",
      "Training loss  0.672 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.700 in Step 200\n",
      "Training loss  0.675 in Step 300\n",
      "Training loss  0.713 in Step 400\n",
      "Training loss  0.692 in Step 500\n",
      "Training loss  0.692 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.697 in Step 800\n",
      "Training loss  0.701 in Step 900\n",
      "Training loss  0.698 in Step 1000\n",
      "Training loss  0.702 in Step 1100\n",
      "Training loss  0.665 in Step 1200\n",
      "Training loss  0.655 in Step 1300\n",
      "Training loss  0.675 in Step 1400\n",
      "Training loss  0.662 in Step 1500\n",
      "Training loss  0.687 in Step 1600\n",
      "Training loss  0.703 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.708 in Step 300\n",
      "Valid loss  0.708 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 84\n",
      "Training loss  0.695 in Step 0\n",
      "Training loss  0.682 in Step 100\n",
      "Training loss  0.699 in Step 200\n",
      "Training loss  0.710 in Step 300\n",
      "Training loss  0.661 in Step 400\n",
      "Training loss  0.702 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.715 in Step 700\n",
      "Training loss  0.732 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.675 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.689 in Step 1300\n",
      "Training loss  0.678 in Step 1400\n",
      "Training loss  0.676 in Step 1500\n",
      "Training loss  0.698 in Step 1600\n",
      "Training loss  0.674 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.680 in Step 0\n",
      "Valid loss  0.689 in Step 100\n",
      "Valid loss  0.684 in Step 200\n",
      "Valid loss  0.706 in Step 300\n",
      "Valid loss  0.714 in Step 400\n",
      "※※※Valid loss  0.694※※※\n",
      "Epoch 85\n",
      "Training loss  0.686 in Step 0\n",
      "Training loss  0.701 in Step 100\n",
      "Training loss  0.696 in Step 200\n",
      "Training loss  0.690 in Step 300\n",
      "Training loss  0.701 in Step 400\n",
      "Training loss  0.673 in Step 500\n",
      "Training loss  0.686 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.698 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.698 in Step 1100\n",
      "Training loss  0.669 in Step 1200\n",
      "Training loss  0.689 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.691 in Step 1500\n",
      "Training loss  0.696 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.674 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.680 in Step 200\n",
      "Valid loss  0.705 in Step 300\n",
      "Valid loss  0.711 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 86\n",
      "Training loss  0.705 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.672 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.701 in Step 400\n",
      "Training loss  0.694 in Step 500\n",
      "Training loss  0.669 in Step 600\n",
      "Training loss  0.706 in Step 700\n",
      "Training loss  0.696 in Step 800\n",
      "Training loss  0.681 in Step 900\n",
      "Training loss  0.655 in Step 1000\n",
      "Training loss  0.705 in Step 1100\n",
      "Training loss  0.698 in Step 1200\n",
      "Training loss  0.695 in Step 1300\n",
      "Training loss  0.666 in Step 1400\n",
      "Training loss  0.687 in Step 1500\n",
      "Training loss  0.701 in Step 1600\n",
      "Training loss  0.696 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.674 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 87\n",
      "Training loss  0.679 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.691 in Step 200\n",
      "Training loss  0.701 in Step 300\n",
      "Training loss  0.693 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.709 in Step 600\n",
      "Training loss  0.697 in Step 700\n",
      "Training loss  0.709 in Step 800\n",
      "Training loss  0.682 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.662 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.698 in Step 1300\n",
      "Training loss  0.693 in Step 1400\n",
      "Training loss  0.678 in Step 1500\n",
      "Training loss  0.692 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.680 in Step 0\n",
      "Valid loss  0.694 in Step 100\n",
      "Valid loss  0.696 in Step 200\n",
      "Valid loss  0.705 in Step 300\n",
      "Valid loss  0.718 in Step 400\n",
      "※※※Valid loss  0.699※※※\n",
      "Epoch 88\n",
      "Training loss  0.705 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.695 in Step 200\n",
      "Training loss  0.702 in Step 300\n",
      "Training loss  0.676 in Step 400\n",
      "Training loss  0.699 in Step 500\n",
      "Training loss  0.687 in Step 600\n",
      "Training loss  0.710 in Step 700\n",
      "Training loss  0.692 in Step 800\n",
      "Training loss  0.685 in Step 900\n",
      "Training loss  0.700 in Step 1000\n",
      "Training loss  0.676 in Step 1100\n",
      "Training loss  0.710 in Step 1200\n",
      "Training loss  0.655 in Step 1300\n",
      "Training loss  0.671 in Step 1400\n",
      "Training loss  0.692 in Step 1500\n",
      "Training loss  0.683 in Step 1600\n",
      "Training loss  0.713 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 89\n",
      "Training loss  0.670 in Step 0\n",
      "Training loss  0.707 in Step 100\n",
      "Training loss  0.717 in Step 200\n",
      "Training loss  0.675 in Step 300\n",
      "Training loss  0.695 in Step 400\n",
      "Training loss  0.686 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.712 in Step 900\n",
      "Training loss  0.719 in Step 1000\n",
      "Training loss  0.701 in Step 1100\n",
      "Training loss  0.693 in Step 1200\n",
      "Training loss  0.708 in Step 1300\n",
      "Training loss  0.702 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.692 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.689※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.677 in Step 0\n",
      "Valid loss  0.689 in Step 100\n",
      "Valid loss  0.686 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.710 in Step 400\n",
      "※※※Valid loss  0.692※※※\n",
      "Epoch 90\n",
      "Training loss  0.688 in Step 0\n",
      "Training loss  0.681 in Step 100\n",
      "Training loss  0.679 in Step 200\n",
      "Training loss  0.704 in Step 300\n",
      "Training loss  0.702 in Step 400\n",
      "Training loss  0.680 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.682 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.685 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.687 in Step 1100\n",
      "Training loss  0.668 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.709 in Step 1600\n",
      "Training loss  0.685 in Step 1700\n",
      "※※※Training loss  0.689※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.685 in Step 100\n",
      "Valid loss  0.683 in Step 200\n",
      "Valid loss  0.702 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 91\n",
      "Training loss  0.695 in Step 0\n",
      "Training loss  0.679 in Step 100\n",
      "Training loss  0.684 in Step 200\n",
      "Training loss  0.677 in Step 300\n",
      "Training loss  0.703 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.694 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.653 in Step 800\n",
      "Training loss  0.698 in Step 900\n",
      "Training loss  0.688 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.692 in Step 1200\n",
      "Training loss  0.714 in Step 1300\n",
      "Training loss  0.710 in Step 1400\n",
      "Training loss  0.716 in Step 1500\n",
      "Training loss  0.687 in Step 1600\n",
      "Training loss  0.716 in Step 1700\n",
      "※※※Training loss  0.693※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.679 in Step 0\n",
      "Valid loss  0.697 in Step 100\n",
      "Valid loss  0.695 in Step 200\n",
      "Valid loss  0.706 in Step 300\n",
      "Valid loss  0.706 in Step 400\n",
      "※※※Valid loss  0.697※※※\n",
      "Epoch 92\n",
      "Training loss  0.712 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.678 in Step 200\n",
      "Training loss  0.726 in Step 300\n",
      "Training loss  0.700 in Step 400\n",
      "Training loss  0.670 in Step 500\n",
      "Training loss  0.710 in Step 600\n",
      "Training loss  0.696 in Step 700\n",
      "Training loss  0.677 in Step 800\n",
      "Training loss  0.731 in Step 900\n",
      "Training loss  0.710 in Step 1000\n",
      "Training loss  0.698 in Step 1100\n",
      "Training loss  0.688 in Step 1200\n",
      "Training loss  0.696 in Step 1300\n",
      "Training loss  0.714 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.701 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.693※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.680 in Step 0\n",
      "Valid loss  0.692 in Step 100\n",
      "Valid loss  0.695 in Step 200\n",
      "Valid loss  0.718 in Step 300\n",
      "Valid loss  0.718 in Step 400\n",
      "※※※Valid loss  0.702※※※\n",
      "Epoch 93\n",
      "Training loss  0.713 in Step 0\n",
      "Training loss  0.701 in Step 100\n",
      "Training loss  0.702 in Step 200\n",
      "Training loss  0.698 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.694 in Step 500\n",
      "Training loss  0.712 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.688 in Step 800\n",
      "Training loss  0.666 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.690 in Step 1100\n",
      "Training loss  0.697 in Step 1200\n",
      "Training loss  0.704 in Step 1300\n",
      "Training loss  0.682 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.695※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.682 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.710 in Step 300\n",
      "Valid loss  0.708 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 94\n",
      "Training loss  0.675 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.689 in Step 200\n",
      "Training loss  0.707 in Step 300\n",
      "Training loss  0.698 in Step 400\n",
      "Training loss  0.718 in Step 500\n",
      "Training loss  0.678 in Step 600\n",
      "Training loss  0.677 in Step 700\n",
      "Training loss  0.699 in Step 800\n",
      "Training loss  0.672 in Step 900\n",
      "Training loss  0.703 in Step 1000\n",
      "Training loss  0.674 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.681 in Step 1500\n",
      "Training loss  0.690 in Step 1600\n",
      "Training loss  0.677 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.674 in Step 0\n",
      "Valid loss  0.685 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.705 in Step 300\n",
      "Valid loss  0.712 in Step 400\n",
      "※※※Valid loss  0.692※※※\n",
      "Epoch 95\n",
      "Training loss  0.682 in Step 0\n",
      "Training loss  0.707 in Step 100\n",
      "Training loss  0.709 in Step 200\n",
      "Training loss  0.699 in Step 300\n",
      "Training loss  0.692 in Step 400\n",
      "Training loss  0.715 in Step 500\n",
      "Training loss  0.692 in Step 600\n",
      "Training loss  0.698 in Step 700\n",
      "Training loss  0.694 in Step 800\n",
      "Training loss  0.686 in Step 900\n",
      "Training loss  0.702 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.714 in Step 1200\n",
      "Training loss  0.685 in Step 1300\n",
      "Training loss  0.711 in Step 1400\n",
      "Training loss  0.690 in Step 1500\n",
      "Training loss  0.708 in Step 1600\n",
      "Training loss  0.709 in Step 1700\n",
      "※※※Training loss  0.694※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.682 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 96\n",
      "Training loss  0.689 in Step 0\n",
      "Training loss  0.698 in Step 100\n",
      "Training loss  0.713 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.680 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.672 in Step 700\n",
      "Training loss  0.671 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.719 in Step 1000\n",
      "Training loss  0.668 in Step 1100\n",
      "Training loss  0.730 in Step 1200\n",
      "Training loss  0.691 in Step 1300\n",
      "Training loss  0.700 in Step 1400\n",
      "Training loss  0.707 in Step 1500\n",
      "Training loss  0.701 in Step 1600\n",
      "Training loss  0.702 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.674 in Step 0\n",
      "Valid loss  0.686 in Step 100\n",
      "Valid loss  0.685 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.709 in Step 400\n",
      "※※※Valid loss  0.692※※※\n",
      "Epoch 97\n",
      "Training loss  0.675 in Step 0\n",
      "Training loss  0.704 in Step 100\n",
      "Training loss  0.692 in Step 200\n",
      "Training loss  0.692 in Step 300\n",
      "Training loss  0.671 in Step 400\n",
      "Training loss  0.678 in Step 500\n",
      "Training loss  0.713 in Step 600\n",
      "Training loss  0.703 in Step 700\n",
      "Training loss  0.701 in Step 800\n",
      "Training loss  0.682 in Step 900\n",
      "Training loss  0.686 in Step 1000\n",
      "Training loss  0.699 in Step 1100\n",
      "Training loss  0.688 in Step 1200\n",
      "Training loss  0.697 in Step 1300\n",
      "Training loss  0.677 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.712 in Step 1600\n",
      "Training loss  0.715 in Step 1700\n",
      "※※※Training loss  0.689※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.680 in Step 0\n",
      "Valid loss  0.690 in Step 100\n",
      "Valid loss  0.690 in Step 200\n",
      "Valid loss  0.707 in Step 300\n",
      "Valid loss  0.714 in Step 400\n",
      "※※※Valid loss  0.696※※※\n",
      "Epoch 98\n",
      "Training loss  0.681 in Step 0\n",
      "Training loss  0.686 in Step 100\n",
      "Training loss  0.679 in Step 200\n",
      "Training loss  0.697 in Step 300\n",
      "Training loss  0.688 in Step 400\n",
      "Training loss  0.682 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.684 in Step 700\n",
      "Training loss  0.699 in Step 800\n",
      "Training loss  0.707 in Step 900\n",
      "Training loss  0.672 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.707 in Step 1200\n",
      "Training loss  0.699 in Step 1300\n",
      "Training loss  0.687 in Step 1400\n",
      "Training loss  0.702 in Step 1500\n",
      "Training loss  0.682 in Step 1600\n",
      "Training loss  0.678 in Step 1700\n",
      "※※※Training loss  0.689※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.677 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.704 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 99\n",
      "Training loss  0.679 in Step 0\n",
      "Training loss  0.662 in Step 100\n",
      "Training loss  0.705 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.666 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.659 in Step 600\n",
      "Training loss  0.696 in Step 700\n",
      "Training loss  0.716 in Step 800\n",
      "Training loss  0.682 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.720 in Step 1100\n",
      "Training loss  0.681 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.696 in Step 1400\n",
      "Training loss  0.691 in Step 1500\n",
      "Training loss  0.707 in Step 1600\n",
      "Training loss  0.672 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.680 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 100\n",
      "Training loss  0.694 in Step 0\n",
      "Training loss  0.676 in Step 100\n",
      "Training loss  0.664 in Step 200\n",
      "Training loss  0.708 in Step 300\n",
      "Training loss  0.701 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.674 in Step 600\n",
      "Training loss  0.694 in Step 700\n",
      "Training loss  0.701 in Step 800\n",
      "Training loss  0.699 in Step 900\n",
      "Training loss  0.694 in Step 1000\n",
      "Training loss  0.709 in Step 1100\n",
      "Training loss  0.671 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.705 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.705 in Step 1600\n",
      "Training loss  0.708 in Step 1700\n",
      "※※※Training loss  0.693※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.672 in Step 0\n",
      "Valid loss  0.684 in Step 100\n",
      "Valid loss  0.684 in Step 200\n",
      "Valid loss  0.702 in Step 300\n",
      "Valid loss  0.703 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 101\n",
      "Training loss  0.701 in Step 0\n",
      "Training loss  0.683 in Step 100\n",
      "Training loss  0.679 in Step 200\n",
      "Training loss  0.664 in Step 300\n",
      "Training loss  0.680 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.709 in Step 600\n",
      "Training loss  0.708 in Step 700\n",
      "Training loss  0.683 in Step 800\n",
      "Training loss  0.709 in Step 900\n",
      "Training loss  0.721 in Step 1000\n",
      "Training loss  0.735 in Step 1100\n",
      "Training loss  0.697 in Step 1200\n",
      "Training loss  0.670 in Step 1300\n",
      "Training loss  0.693 in Step 1400\n",
      "Training loss  0.675 in Step 1500\n",
      "Training loss  0.685 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 102\n",
      "Training loss  0.688 in Step 0\n",
      "Training loss  0.670 in Step 100\n",
      "Training loss  0.708 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.697 in Step 400\n",
      "Training loss  0.680 in Step 500\n",
      "Training loss  0.700 in Step 600\n",
      "Training loss  0.709 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.706 in Step 900\n",
      "Training loss  0.729 in Step 1000\n",
      "Training loss  0.709 in Step 1100\n",
      "Training loss  0.775 in Step 1200\n",
      "Training loss  0.784 in Step 1300\n",
      "Training loss  0.720 in Step 1400\n",
      "Training loss  0.724 in Step 1500\n",
      "Training loss  0.718 in Step 1600\n",
      "Training loss  0.717 in Step 1700\n",
      "※※※Training loss  0.712※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.689 in Step 0\n",
      "Valid loss  0.719 in Step 100\n",
      "Valid loss  0.704 in Step 200\n",
      "Valid loss  0.714 in Step 300\n",
      "Valid loss  0.740 in Step 400\n",
      "※※※Valid loss  0.714※※※\n",
      "Epoch 103\n",
      "Training loss  0.706 in Step 0\n",
      "Training loss  0.726 in Step 100\n",
      "Training loss  0.685 in Step 200\n",
      "Training loss  0.706 in Step 300\n",
      "Training loss  0.721 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.720 in Step 700\n",
      "Training loss  0.728 in Step 800\n",
      "Training loss  0.722 in Step 900\n",
      "Training loss  0.695 in Step 1000\n",
      "Training loss  0.720 in Step 1100\n",
      "Training loss  0.714 in Step 1200\n",
      "Training loss  0.720 in Step 1300\n",
      "Training loss  0.704 in Step 1400\n",
      "Training loss  0.703 in Step 1500\n",
      "Training loss  0.683 in Step 1600\n",
      "Training loss  0.712 in Step 1700\n",
      "※※※Training loss  0.709※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.673 in Step 0\n",
      "Valid loss  0.693 in Step 100\n",
      "Valid loss  0.687 in Step 200\n",
      "Valid loss  0.712 in Step 300\n",
      "Valid loss  0.714 in Step 400\n",
      "※※※Valid loss  0.694※※※\n",
      "Epoch 104\n",
      "Training loss  0.660 in Step 0\n",
      "Training loss  0.676 in Step 100\n",
      "Training loss  0.700 in Step 200\n",
      "Training loss  0.682 in Step 300\n",
      "Training loss  0.694 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.712 in Step 600\n",
      "Training loss  0.694 in Step 700\n",
      "Training loss  0.685 in Step 800\n",
      "Training loss  0.681 in Step 900\n",
      "Training loss  0.710 in Step 1000\n",
      "Training loss  0.702 in Step 1100\n",
      "Training loss  0.704 in Step 1200\n",
      "Training loss  0.693 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.687 in Step 1500\n",
      "Training loss  0.671 in Step 1600\n",
      "Training loss  0.688 in Step 1700\n",
      "※※※Training loss  0.693※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.693 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.709 in Step 300\n",
      "Valid loss  0.709 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 105\n",
      "Training loss  0.686 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.675 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.679 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.707 in Step 700\n",
      "Training loss  0.702 in Step 800\n",
      "Training loss  0.683 in Step 900\n",
      "Training loss  0.663 in Step 1000\n",
      "Training loss  0.696 in Step 1100\n",
      "Training loss  0.689 in Step 1200\n",
      "Training loss  0.693 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.695 in Step 1500\n",
      "Training loss  0.674 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.693※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.676 in Step 0\n",
      "Valid loss  0.694 in Step 100\n",
      "Valid loss  0.689 in Step 200\n",
      "Valid loss  0.714 in Step 300\n",
      "Valid loss  0.718 in Step 400\n",
      "※※※Valid loss  0.700※※※\n",
      "Epoch 106\n",
      "Training loss  0.700 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.708 in Step 200\n",
      "Training loss  0.686 in Step 300\n",
      "Training loss  0.675 in Step 400\n",
      "Training loss  0.693 in Step 500\n",
      "Training loss  0.693 in Step 600\n",
      "Training loss  0.701 in Step 700\n",
      "Training loss  0.677 in Step 800\n",
      "Training loss  0.703 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.718 in Step 1100\n",
      "Training loss  0.677 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.676 in Step 1500\n",
      "Training loss  0.676 in Step 1600\n",
      "Training loss  0.676 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.702 in Step 300\n",
      "Valid loss  0.707 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 107\n",
      "Training loss  0.701 in Step 0\n",
      "Training loss  0.739 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.685 in Step 300\n",
      "Training loss  0.669 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.696 in Step 600\n",
      "Training loss  0.695 in Step 700\n",
      "Training loss  0.678 in Step 800\n",
      "Training loss  0.709 in Step 900\n",
      "Training loss  0.692 in Step 1000\n",
      "Training loss  0.702 in Step 1100\n",
      "Training loss  0.686 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.679 in Step 1700\n",
      "※※※Training loss  0.688※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.682 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.708 in Step 300\n",
      "Valid loss  0.707 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 108\n",
      "Training loss  0.685 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.705 in Step 200\n",
      "Training loss  0.709 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.702 in Step 500\n",
      "Training loss  0.709 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.701 in Step 800\n",
      "Training loss  0.718 in Step 900\n",
      "Training loss  0.694 in Step 1000\n",
      "Training loss  0.672 in Step 1100\n",
      "Training loss  0.702 in Step 1200\n",
      "Training loss  0.726 in Step 1300\n",
      "Training loss  0.725 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.713 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.680 in Step 100\n",
      "Valid loss  0.674 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.706 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 109\n",
      "Training loss  0.674 in Step 0\n",
      "Training loss  0.664 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.684 in Step 300\n",
      "Training loss  0.663 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.683 in Step 800\n",
      "Training loss  0.657 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.686 in Step 1200\n",
      "Training loss  0.716 in Step 1300\n",
      "Training loss  0.670 in Step 1400\n",
      "Training loss  0.703 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.677 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.703 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 110\n",
      "Training loss  0.689 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.697 in Step 200\n",
      "Training loss  0.680 in Step 300\n",
      "Training loss  0.658 in Step 400\n",
      "Training loss  0.699 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.686 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.695 in Step 1000\n",
      "Training loss  0.694 in Step 1100\n",
      "Training loss  0.666 in Step 1200\n",
      "Training loss  0.701 in Step 1300\n",
      "Training loss  0.668 in Step 1400\n",
      "Training loss  0.694 in Step 1500\n",
      "Training loss  0.685 in Step 1600\n",
      "Training loss  0.685 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.702 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 111\n",
      "Training loss  0.688 in Step 0\n",
      "Training loss  0.659 in Step 100\n",
      "Training loss  0.692 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.686 in Step 400\n",
      "Training loss  0.697 in Step 500\n",
      "Training loss  0.715 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.659 in Step 800\n",
      "Training loss  0.669 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.678 in Step 1100\n",
      "Training loss  0.677 in Step 1200\n",
      "Training loss  0.696 in Step 1300\n",
      "Training loss  0.703 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.699 in Step 1600\n",
      "Training loss  0.684 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.707 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 112\n",
      "Training loss  0.706 in Step 0\n",
      "Training loss  0.697 in Step 100\n",
      "Training loss  0.692 in Step 200\n",
      "Training loss  0.699 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.677 in Step 500\n",
      "Training loss  0.691 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.703 in Step 900\n",
      "Training loss  0.693 in Step 1000\n",
      "Training loss  0.662 in Step 1100\n",
      "Training loss  0.651 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.671 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.709 in Step 1600\n",
      "Training loss  0.706 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.680 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 113\n",
      "Training loss  0.706 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.704 in Step 200\n",
      "Training loss  0.694 in Step 300\n",
      "Training loss  0.683 in Step 400\n",
      "Training loss  0.692 in Step 500\n",
      "Training loss  0.717 in Step 600\n",
      "Training loss  0.664 in Step 700\n",
      "Training loss  0.676 in Step 800\n",
      "Training loss  0.704 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.690 in Step 1100\n",
      "Training loss  0.688 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.678 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 114\n",
      "Training loss  0.715 in Step 0\n",
      "Training loss  0.702 in Step 100\n",
      "Training loss  0.671 in Step 200\n",
      "Training loss  0.683 in Step 300\n",
      "Training loss  0.663 in Step 400\n",
      "Training loss  0.696 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.717 in Step 700\n",
      "Training loss  0.703 in Step 800\n",
      "Training loss  0.690 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.682 in Step 1100\n",
      "Training loss  0.701 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.665 in Step 1400\n",
      "Training loss  0.695 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.706 in Step 1700\n",
      "※※※Training loss  0.688※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.706 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 115\n",
      "Training loss  0.689 in Step 0\n",
      "Training loss  0.696 in Step 100\n",
      "Training loss  0.711 in Step 200\n",
      "Training loss  0.719 in Step 300\n",
      "Training loss  0.702 in Step 400\n",
      "Training loss  0.697 in Step 500\n",
      "Training loss  0.699 in Step 600\n",
      "Training loss  0.672 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.693 in Step 900\n",
      "Training loss  0.694 in Step 1000\n",
      "Training loss  0.682 in Step 1100\n",
      "Training loss  0.685 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.702 in Step 1400\n",
      "Training loss  0.699 in Step 1500\n",
      "Training loss  0.666 in Step 1600\n",
      "Training loss  0.679 in Step 1700\n",
      "※※※Training loss  0.688※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.703 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 116\n",
      "Training loss  0.673 in Step 0\n",
      "Training loss  0.681 in Step 100\n",
      "Training loss  0.711 in Step 200\n",
      "Training loss  0.675 in Step 300\n",
      "Training loss  0.673 in Step 400\n",
      "Training loss  0.681 in Step 500\n",
      "Training loss  0.687 in Step 600\n",
      "Training loss  0.665 in Step 700\n",
      "Training loss  0.698 in Step 800\n",
      "Training loss  0.710 in Step 900\n",
      "Training loss  0.693 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.701 in Step 1200\n",
      "Training loss  0.693 in Step 1300\n",
      "Training loss  0.699 in Step 1400\n",
      "Training loss  0.681 in Step 1500\n",
      "Training loss  0.709 in Step 1600\n",
      "Training loss  0.691 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.698 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 117\n",
      "Training loss  0.711 in Step 0\n",
      "Training loss  0.690 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.696 in Step 300\n",
      "Training loss  0.658 in Step 400\n",
      "Training loss  0.706 in Step 500\n",
      "Training loss  0.671 in Step 600\n",
      "Training loss  0.670 in Step 700\n",
      "Training loss  0.704 in Step 800\n",
      "Training loss  0.667 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.695 in Step 1100\n",
      "Training loss  0.699 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.710 in Step 1400\n",
      "Training loss  0.664 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 118\n",
      "Training loss  0.676 in Step 0\n",
      "Training loss  0.679 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.677 in Step 400\n",
      "Training loss  0.680 in Step 500\n",
      "Training loss  0.661 in Step 600\n",
      "Training loss  0.689 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.681 in Step 900\n",
      "Training loss  0.681 in Step 1000\n",
      "Training loss  0.688 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.694 in Step 1300\n",
      "Training loss  0.706 in Step 1400\n",
      "Training loss  0.693 in Step 1500\n",
      "Training loss  0.663 in Step 1600\n",
      "Training loss  0.692 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.673 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.678 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 119\n",
      "Training loss  0.698 in Step 0\n",
      "Training loss  0.695 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.679 in Step 300\n",
      "Training loss  0.678 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.703 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.704 in Step 900\n",
      "Training loss  0.705 in Step 1000\n",
      "Training loss  0.686 in Step 1100\n",
      "Training loss  0.707 in Step 1200\n",
      "Training loss  0.682 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.698 in Step 1600\n",
      "Training loss  0.665 in Step 1700\n",
      "※※※Training loss  0.688※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.678 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.704 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 120\n",
      "Training loss  0.676 in Step 0\n",
      "Training loss  0.690 in Step 100\n",
      "Training loss  0.711 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.678 in Step 400\n",
      "Training loss  0.699 in Step 500\n",
      "Training loss  0.718 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.702 in Step 800\n",
      "Training loss  0.690 in Step 900\n",
      "Training loss  0.691 in Step 1000\n",
      "Training loss  0.701 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.704 in Step 1300\n",
      "Training loss  0.711 in Step 1400\n",
      "Training loss  0.698 in Step 1500\n",
      "Training loss  0.675 in Step 1600\n",
      "Training loss  0.688 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.677 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 121\n",
      "Training loss  0.672 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.673 in Step 200\n",
      "Training loss  0.697 in Step 300\n",
      "Training loss  0.700 in Step 400\n",
      "Training loss  0.668 in Step 500\n",
      "Training loss  0.669 in Step 600\n",
      "Training loss  0.673 in Step 700\n",
      "Training loss  0.684 in Step 800\n",
      "Training loss  0.685 in Step 900\n",
      "Training loss  0.692 in Step 1000\n",
      "Training loss  0.703 in Step 1100\n",
      "Training loss  0.693 in Step 1200\n",
      "Training loss  0.661 in Step 1300\n",
      "Training loss  0.712 in Step 1400\n",
      "Training loss  0.665 in Step 1500\n",
      "Training loss  0.695 in Step 1600\n",
      "Training loss  0.679 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.704 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 122\n",
      "Training loss  0.698 in Step 0\n",
      "Training loss  0.709 in Step 100\n",
      "Training loss  0.700 in Step 200\n",
      "Training loss  0.711 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.668 in Step 800\n",
      "Training loss  0.700 in Step 900\n",
      "Training loss  0.673 in Step 1000\n",
      "Training loss  0.665 in Step 1100\n",
      "Training loss  0.680 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.694 in Step 1400\n",
      "Training loss  0.674 in Step 1500\n",
      "Training loss  0.678 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.703 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 123\n",
      "Training loss  0.719 in Step 0\n",
      "Training loss  0.671 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.680 in Step 300\n",
      "Training loss  0.688 in Step 400\n",
      "Training loss  0.690 in Step 500\n",
      "Training loss  0.690 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.667 in Step 800\n",
      "Training loss  0.692 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.687 in Step 1100\n",
      "Training loss  0.683 in Step 1200\n",
      "Training loss  0.686 in Step 1300\n",
      "Training loss  0.716 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.700 in Step 1600\n",
      "Training loss  0.691 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.700 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 124\n",
      "Training loss  0.696 in Step 0\n",
      "Training loss  0.680 in Step 100\n",
      "Training loss  0.682 in Step 200\n",
      "Training loss  0.704 in Step 300\n",
      "Training loss  0.684 in Step 400\n",
      "Training loss  0.718 in Step 500\n",
      "Training loss  0.675 in Step 600\n",
      "Training loss  0.694 in Step 700\n",
      "Training loss  0.676 in Step 800\n",
      "Training loss  0.704 in Step 900\n",
      "Training loss  0.687 in Step 1000\n",
      "Training loss  0.682 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.672 in Step 1300\n",
      "Training loss  0.681 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.677 in Step 1700\n",
      "※※※Training loss  0.687※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 125\n",
      "Training loss  0.674 in Step 0\n",
      "Training loss  0.714 in Step 100\n",
      "Training loss  0.706 in Step 200\n",
      "Training loss  0.669 in Step 300\n",
      "Training loss  0.670 in Step 400\n",
      "Training loss  0.678 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.696 in Step 700\n",
      "Training loss  0.692 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.698 in Step 1000\n",
      "Training loss  0.672 in Step 1100\n",
      "Training loss  0.699 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.698 in Step 1400\n",
      "Training loss  0.671 in Step 1500\n",
      "Training loss  0.706 in Step 1600\n",
      "Training loss  0.691 in Step 1700\n",
      "※※※Training loss  0.685※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.671 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 126\n",
      "Training loss  0.675 in Step 0\n",
      "Training loss  0.667 in Step 100\n",
      "Training loss  0.657 in Step 200\n",
      "Training loss  0.691 in Step 300\n",
      "Training loss  0.692 in Step 400\n",
      "Training loss  0.680 in Step 500\n",
      "Training loss  0.708 in Step 600\n",
      "Training loss  0.699 in Step 700\n",
      "Training loss  0.697 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.671 in Step 1000\n",
      "Training loss  0.668 in Step 1100\n",
      "Training loss  0.705 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.673 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.687 in Step 1600\n",
      "Training loss  0.700 in Step 1700\n",
      "※※※Training loss  0.685※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.704 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 127\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.663 in Step 200\n",
      "Training loss  0.686 in Step 300\n",
      "Training loss  0.687 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.692 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.671 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.663 in Step 1000\n",
      "Training loss  0.675 in Step 1100\n",
      "Training loss  0.683 in Step 1200\n",
      "Training loss  0.677 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.692 in Step 1500\n",
      "Training loss  0.681 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.704 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 128\n",
      "Training loss  0.673 in Step 0\n",
      "Training loss  0.661 in Step 100\n",
      "Training loss  0.683 in Step 200\n",
      "Training loss  0.649 in Step 300\n",
      "Training loss  0.693 in Step 400\n",
      "Training loss  0.677 in Step 500\n",
      "Training loss  0.669 in Step 600\n",
      "Training loss  0.682 in Step 700\n",
      "Training loss  0.660 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.670 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.681 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.692 in Step 1400\n",
      "Training loss  0.705 in Step 1500\n",
      "Training loss  0.696 in Step 1600\n",
      "Training loss  0.715 in Step 1700\n",
      "※※※Training loss  0.685※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.677 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.710 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 129\n",
      "Training loss  0.701 in Step 0\n",
      "Training loss  0.690 in Step 100\n",
      "Training loss  0.690 in Step 200\n",
      "Training loss  0.685 in Step 300\n",
      "Training loss  0.685 in Step 400\n",
      "Training loss  0.701 in Step 500\n",
      "Training loss  0.710 in Step 600\n",
      "Training loss  0.678 in Step 700\n",
      "Training loss  0.709 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.673 in Step 1100\n",
      "Training loss  0.696 in Step 1200\n",
      "Training loss  0.666 in Step 1300\n",
      "Training loss  0.677 in Step 1400\n",
      "Training loss  0.669 in Step 1500\n",
      "Training loss  0.690 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.700 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 130\n",
      "Training loss  0.663 in Step 0\n",
      "Training loss  0.659 in Step 100\n",
      "Training loss  0.655 in Step 200\n",
      "Training loss  0.706 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.670 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.685 in Step 700\n",
      "Training loss  0.691 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.717 in Step 1000\n",
      "Training loss  0.682 in Step 1100\n",
      "Training loss  0.686 in Step 1200\n",
      "Training loss  0.706 in Step 1300\n",
      "Training loss  0.691 in Step 1400\n",
      "Training loss  0.692 in Step 1500\n",
      "Training loss  0.704 in Step 1600\n",
      "Training loss  0.677 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.694 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 131\n",
      "Training loss  0.682 in Step 0\n",
      "Training loss  0.709 in Step 100\n",
      "Training loss  0.686 in Step 200\n",
      "Training loss  0.693 in Step 300\n",
      "Training loss  0.682 in Step 400\n",
      "Training loss  0.682 in Step 500\n",
      "Training loss  0.690 in Step 600\n",
      "Training loss  0.699 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.675 in Step 900\n",
      "Training loss  0.699 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.700 in Step 1200\n",
      "Training loss  0.694 in Step 1300\n",
      "Training loss  0.701 in Step 1400\n",
      "Training loss  0.678 in Step 1500\n",
      "Training loss  0.704 in Step 1600\n",
      "Training loss  0.706 in Step 1700\n",
      "※※※Training loss  0.688※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.688 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 132\n",
      "Training loss  0.691 in Step 0\n",
      "Training loss  0.705 in Step 100\n",
      "Training loss  0.697 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.696 in Step 400\n",
      "Training loss  0.703 in Step 500\n",
      "Training loss  0.683 in Step 600\n",
      "Training loss  0.672 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.694 in Step 1000\n",
      "Training loss  0.717 in Step 1100\n",
      "Training loss  0.690 in Step 1200\n",
      "Training loss  0.670 in Step 1300\n",
      "Training loss  0.707 in Step 1400\n",
      "Training loss  0.685 in Step 1500\n",
      "Training loss  0.703 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.691※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.686 in Step 100\n",
      "Valid loss  0.684 in Step 200\n",
      "Valid loss  0.705 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 133\n",
      "Training loss  0.660 in Step 0\n",
      "Training loss  0.669 in Step 100\n",
      "Training loss  0.682 in Step 200\n",
      "Training loss  0.664 in Step 300\n",
      "Training loss  0.683 in Step 400\n",
      "Training loss  0.660 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.673 in Step 700\n",
      "Training loss  0.671 in Step 800\n",
      "Training loss  0.709 in Step 900\n",
      "Training loss  0.709 in Step 1000\n",
      "Training loss  0.692 in Step 1100\n",
      "Training loss  0.683 in Step 1200\n",
      "Training loss  0.689 in Step 1300\n",
      "Training loss  0.670 in Step 1400\n",
      "Training loss  0.702 in Step 1500\n",
      "Training loss  0.699 in Step 1600\n",
      "Training loss  0.695 in Step 1700\n",
      "※※※Training loss  0.689※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.689 in Step 100\n",
      "Valid loss  0.688 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.718 in Step 400\n",
      "※※※Valid loss  0.691※※※\n",
      "Epoch 134\n",
      "Training loss  0.666 in Step 0\n",
      "Training loss  0.704 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.675 in Step 300\n",
      "Training loss  0.705 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.711 in Step 600\n",
      "Training loss  0.698 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.701 in Step 1100\n",
      "Training loss  0.695 in Step 1200\n",
      "Training loss  0.694 in Step 1300\n",
      "Training loss  0.697 in Step 1400\n",
      "Training loss  0.696 in Step 1500\n",
      "Training loss  0.691 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.671 in Step 0\n",
      "Valid loss  0.698 in Step 100\n",
      "Valid loss  0.682 in Step 200\n",
      "Valid loss  0.711 in Step 300\n",
      "Valid loss  0.706 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 135\n",
      "Training loss  0.679 in Step 0\n",
      "Training loss  0.699 in Step 100\n",
      "Training loss  0.696 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.733 in Step 500\n",
      "Training loss  0.704 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.685 in Step 800\n",
      "Training loss  0.744 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.688 in Step 1100\n",
      "Training loss  0.711 in Step 1200\n",
      "Training loss  0.714 in Step 1300\n",
      "Training loss  0.681 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.676 in Step 1600\n",
      "Training loss  0.683 in Step 1700\n",
      "※※※Training loss  0.692※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.682 in Step 100\n",
      "Valid loss  0.683 in Step 200\n",
      "Valid loss  0.711 in Step 300\n",
      "Valid loss  0.704 in Step 400\n",
      "※※※Valid loss  0.690※※※\n",
      "Epoch 136\n",
      "Training loss  0.691 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.664 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.688 in Step 400\n",
      "Training loss  0.699 in Step 500\n",
      "Training loss  0.697 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.672 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.678 in Step 1100\n",
      "Training loss  0.684 in Step 1200\n",
      "Training loss  0.709 in Step 1300\n",
      "Training loss  0.712 in Step 1400\n",
      "Training loss  0.717 in Step 1500\n",
      "Training loss  0.701 in Step 1600\n",
      "Training loss  0.687 in Step 1700\n",
      "※※※Training loss  0.689※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.706 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 137\n",
      "Training loss  0.679 in Step 0\n",
      "Training loss  0.721 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.715 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.683 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.696 in Step 800\n",
      "Training loss  0.678 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.695 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.698 in Step 1300\n",
      "Training loss  0.707 in Step 1400\n",
      "Training loss  0.704 in Step 1500\n",
      "Training loss  0.674 in Step 1600\n",
      "Training loss  0.729 in Step 1700\n",
      "※※※Training loss  0.688※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.674 in Step 0\n",
      "Valid loss  0.686 in Step 100\n",
      "Valid loss  0.687 in Step 200\n",
      "Valid loss  0.714 in Step 300\n",
      "Valid loss  0.711 in Step 400\n",
      "※※※Valid loss  0.695※※※\n",
      "Epoch 138\n",
      "Training loss  0.668 in Step 0\n",
      "Training loss  0.694 in Step 100\n",
      "Training loss  0.698 in Step 200\n",
      "Training loss  0.700 in Step 300\n",
      "Training loss  0.679 in Step 400\n",
      "Training loss  0.707 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.697 in Step 700\n",
      "Training loss  0.685 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.701 in Step 1000\n",
      "Training loss  0.698 in Step 1100\n",
      "Training loss  0.665 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.664 in Step 1400\n",
      "Training loss  0.667 in Step 1500\n",
      "Training loss  0.676 in Step 1600\n",
      "Training loss  0.696 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.680 in Step 200\n",
      "Valid loss  0.707 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 139\n",
      "Training loss  0.689 in Step 0\n",
      "Training loss  0.682 in Step 100\n",
      "Training loss  0.691 in Step 200\n",
      "Training loss  0.709 in Step 300\n",
      "Training loss  0.700 in Step 400\n",
      "Training loss  0.678 in Step 500\n",
      "Training loss  0.672 in Step 600\n",
      "Training loss  0.662 in Step 700\n",
      "Training loss  0.685 in Step 800\n",
      "Training loss  0.702 in Step 900\n",
      "Training loss  0.680 in Step 1000\n",
      "Training loss  0.675 in Step 1100\n",
      "Training loss  0.685 in Step 1200\n",
      "Training loss  0.683 in Step 1300\n",
      "Training loss  0.687 in Step 1400\n",
      "Training loss  0.665 in Step 1500\n",
      "Training loss  0.706 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.685※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.704 in Step 300\n",
      "Valid loss  0.699 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 140\n",
      "Training loss  0.678 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.672 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.696 in Step 500\n",
      "Training loss  0.666 in Step 600\n",
      "Training loss  0.678 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.698 in Step 900\n",
      "Training loss  0.710 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.695 in Step 1200\n",
      "Training loss  0.682 in Step 1300\n",
      "Training loss  0.690 in Step 1400\n",
      "Training loss  0.695 in Step 1500\n",
      "Training loss  0.689 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.674 in Step 200\n",
      "Valid loss  0.707 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 141\n",
      "Training loss  0.660 in Step 0\n",
      "Training loss  0.694 in Step 100\n",
      "Training loss  0.665 in Step 200\n",
      "Training loss  0.707 in Step 300\n",
      "Training loss  0.654 in Step 400\n",
      "Training loss  0.700 in Step 500\n",
      "Training loss  0.686 in Step 600\n",
      "Training loss  0.660 in Step 700\n",
      "Training loss  0.668 in Step 800\n",
      "Training loss  0.667 in Step 900\n",
      "Training loss  0.691 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.695 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.675 in Step 1400\n",
      "Training loss  0.669 in Step 1500\n",
      "Training loss  0.699 in Step 1600\n",
      "Training loss  0.679 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.711 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 142\n",
      "Training loss  0.690 in Step 0\n",
      "Training loss  0.695 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.670 in Step 300\n",
      "Training loss  0.687 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.661 in Step 600\n",
      "Training loss  0.691 in Step 700\n",
      "Training loss  0.684 in Step 800\n",
      "Training loss  0.681 in Step 900\n",
      "Training loss  0.673 in Step 1000\n",
      "Training loss  0.677 in Step 1100\n",
      "Training loss  0.708 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.700 in Step 1400\n",
      "Training loss  0.676 in Step 1500\n",
      "Training loss  0.682 in Step 1600\n",
      "Training loss  0.698 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.703 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 143\n",
      "Training loss  0.708 in Step 0\n",
      "Training loss  0.677 in Step 100\n",
      "Training loss  0.689 in Step 200\n",
      "Training loss  0.660 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.686 in Step 600\n",
      "Training loss  0.705 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.677 in Step 1000\n",
      "Training loss  0.686 in Step 1100\n",
      "Training loss  0.693 in Step 1200\n",
      "Training loss  0.683 in Step 1300\n",
      "Training loss  0.673 in Step 1400\n",
      "Training loss  0.684 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 144\n",
      "Training loss  0.685 in Step 0\n",
      "Training loss  0.663 in Step 100\n",
      "Training loss  0.664 in Step 200\n",
      "Training loss  0.652 in Step 300\n",
      "Training loss  0.686 in Step 400\n",
      "Training loss  0.659 in Step 500\n",
      "Training loss  0.692 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.675 in Step 800\n",
      "Training loss  0.672 in Step 900\n",
      "Training loss  0.669 in Step 1000\n",
      "Training loss  0.710 in Step 1100\n",
      "Training loss  0.708 in Step 1200\n",
      "Training loss  0.669 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.676 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.661 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 145\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.669 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.708 in Step 300\n",
      "Training loss  0.681 in Step 400\n",
      "Training loss  0.708 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.642 in Step 700\n",
      "Training loss  0.679 in Step 800\n",
      "Training loss  0.682 in Step 900\n",
      "Training loss  0.677 in Step 1000\n",
      "Training loss  0.693 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.661 in Step 1300\n",
      "Training loss  0.683 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.667 in Step 1600\n",
      "Training loss  0.658 in Step 1700\n",
      "※※※Training loss  0.685※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.674 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 146\n",
      "Training loss  0.687 in Step 0\n",
      "Training loss  0.669 in Step 100\n",
      "Training loss  0.686 in Step 200\n",
      "Training loss  0.672 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.679 in Step 500\n",
      "Training loss  0.698 in Step 600\n",
      "Training loss  0.713 in Step 700\n",
      "Training loss  0.654 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.703 in Step 1000\n",
      "Training loss  0.685 in Step 1100\n",
      "Training loss  0.689 in Step 1200\n",
      "Training loss  0.672 in Step 1300\n",
      "Training loss  0.689 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.664 in Step 1600\n",
      "Training loss  0.671 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.671 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.703 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 147\n",
      "Training loss  0.671 in Step 0\n",
      "Training loss  0.678 in Step 100\n",
      "Training loss  0.707 in Step 200\n",
      "Training loss  0.664 in Step 300\n",
      "Training loss  0.698 in Step 400\n",
      "Training loss  0.687 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.674 in Step 700\n",
      "Training loss  0.697 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.693 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.685 in Step 1200\n",
      "Training loss  0.686 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.724 in Step 1500\n",
      "Training loss  0.695 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 148\n",
      "Training loss  0.710 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.685 in Step 200\n",
      "Training loss  0.685 in Step 300\n",
      "Training loss  0.673 in Step 400\n",
      "Training loss  0.702 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.705 in Step 700\n",
      "Training loss  0.693 in Step 800\n",
      "Training loss  0.703 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.675 in Step 1100\n",
      "Training loss  0.689 in Step 1200\n",
      "Training loss  0.673 in Step 1300\n",
      "Training loss  0.654 in Step 1400\n",
      "Training loss  0.678 in Step 1500\n",
      "Training loss  0.703 in Step 1600\n",
      "Training loss  0.703 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.699 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 149\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.678 in Step 100\n",
      "Training loss  0.684 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.701 in Step 400\n",
      "Training loss  0.700 in Step 500\n",
      "Training loss  0.707 in Step 600\n",
      "Training loss  0.689 in Step 700\n",
      "Training loss  0.663 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.677 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.675 in Step 1200\n",
      "Training loss  0.682 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.697 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.663 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.683※※※\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "train_losses.save()\n",
    "\n",
    "valid_losses.save()\n",
    "\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbcf839add0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This model should converge to loss around 0.68. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
