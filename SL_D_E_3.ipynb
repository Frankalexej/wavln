{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-mljeGlqMqo"
   },
   "source": [
    "# Sequence Learning - Direct - English\n",
    "Version 1: In this version we make the model \"simple\": make the encoder RNN into normal RNN first and try to see the result.  \n",
    "Version 2: Learning is not very much. Following Dr Coupe's advice we try simpler model structure.   \n",
    "Version 3: A simple trial training with Mel spectrogram instead of MFCC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jN5DNuExjwet"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PhxLearner, SimplerPhxLearner\n",
    "from my_dataset import DS_Tools\n",
    "from dataset import SeqDataset, MelSpecTransform\n",
    "from paths import *\n",
    "from my_utils import *\n",
    "from recorder import *\n",
    "from loss import *\n",
    "from padding import generate_mask_from_lengths_mat, mask_it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iGouCDYD3h18"
   },
   "outputs": [],
   "source": [
    "model_save_dir = model_eng_save_dir\n",
    "# random_data:phone_seg_random_path\n",
    "# anno_data: phone_seg_anno_path\n",
    "\n",
    "# random_log_path = phone_seg_random_log_path + \"log.csv\"\n",
    "random_log_path = word_seg_anno_log_path\n",
    "random_path = word_seg_anno_path\n",
    "anno_log_path = phone_seg_anno_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIM = 64\n",
    "OUTPUT_DIM = 64\n",
    "\n",
    "INTER_DIM_0 = 32\n",
    "INTER_DIM_1 = 16\n",
    "INTER_DIM_2 = 3\n",
    "\n",
    "ENC_SIZE_LIST = [INPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "DEC_SIZE_LIST = [OUTPUT_DIM, INTER_DIM_0, INTER_DIM_1, INTER_DIM_2]\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "REC_SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 64\n",
    "\n",
    "LOADER_WORKER = 16\n",
    "# LOADER_WORKER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lUxoYBUg1jLq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "masked_recon_loss = MaskedLoss(recon_loss)\n",
    "model_loss = masked_recon_loss\n",
    "\n",
    "model = SimplerPhxLearner(enc_size_list=ENC_SIZE_LIST, dec_size_list=DEC_SIZE_LIST, num_layers=2)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZBCTRw3iXys",
    "outputId": "7947acdb-1a95-49a4-8b1d-93f442cf41d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplerPhxLearner(\n",
       "  (encoder): RLEncoder(\n",
       "    (rnn): LSTM(64, 16, num_layers=2, batch_first=True)\n",
       "    (lin_2): LinearPack(\n",
       "      (linear): Linear(in_features=16, out_features=3, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): RALDecoder(\n",
       "    (rnn): LSTM(64, 3, num_layers=2, batch_first=True)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (lin_3): LinearPack(\n",
       "      (linear): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (relu): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8691"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ofsEE6OaoyPh"
   },
   "outputs": [],
   "source": [
    "# Just for keeping records of training hists. \n",
    "ts = \"0918192113\"\n",
    "stop_epoch = \"149\"\n",
    "# ts = str(get_timestamp())\n",
    "save_txt_name = \"train_txt_{}.hst\".format(ts)\n",
    "save_trainhist_name = \"train_hist_{}.hst\".format(ts)\n",
    "\n",
    "save_valhist_name = \"val_hist_{}.hst\".format(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xUHYarigvT64"
   },
   "outputs": [],
   "source": [
    "train_losses = LossRecorder(model_save_dir + save_trainhist_name)\n",
    "\n",
    "valid_losses = LossRecorder(model_save_dir + save_valhist_name)\n",
    "\n",
    "text_hist = HistRecorder(model_save_dir + save_txt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-T4OYaoXsxe_"
   },
   "outputs": [],
   "source": [
    "# READ = False\n",
    "READ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nVvnpUk5sWxb"
   },
   "outputs": [],
   "source": [
    "if READ: \n",
    "    valid_losses.read()\n",
    "    train_losses.read()\n",
    "\n",
    "    model_raw_name = \"PT_{}_{}_full\".format(ts, stop_epoch)\n",
    "    model_name = model_raw_name + \".pt\"\n",
    "    model_path = os.path.join(model_save_dir, model_name)\n",
    "    state = torch.load(model_path)\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6OCx4nqP40fz"
   },
   "outputs": [],
   "source": [
    "mytrans = MelSpecTransform(sample_rate=REC_SAMPLE_RATE, n_fft=N_FFT, n_mels=N_MELS)\n",
    "ds = SeqDataset(random_path, os.path.join(random_log_path, \"log.csv\"), transform=mytrans)\n",
    "\n",
    "\n",
    "if READ: \n",
    "    valid_ds_indices = DS_Tools.read_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)))\n",
    "    all_indices = list(range(len(ds)))\n",
    "    train_ds_indices = list(set(all_indices).difference(set(valid_ds_indices)))\n",
    "\n",
    "    train_ds = torch.utils.data.Subset(ds, train_ds_indices)\n",
    "    valid_ds = torch.utils.data.Subset(ds, valid_ds_indices)\n",
    "else: \n",
    "    train_len = int(0.8 * len(ds))\n",
    "    valid_len = len(ds) - train_len\n",
    "\n",
    "    # Randomly split the dataset into train and validation sets\n",
    "    train_ds, valid_ds = random_split(ds, [train_len, valid_len])\n",
    "    DS_Tools.save_indices(os.path.join(model_save_dir, \"valid_ds_{}.pkl\".format(ts)), valid_ds.indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=LOADER_WORKER, collate_fn=SeqDataset.collate_fn)\n",
    "train_num = len(train_loader.dataset)\n",
    "\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=LOADER_WORKER, collate_fn=SeqDataset.collate_fn)\n",
    "valid_num = len(valid_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BASE = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2n7doAD1uRi",
    "outputId": "e9c5bcb7-72db-4238-e83f-36e4dbe35748"
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    for epoch in range(BASE, BASE + EPOCHS):\n",
    "        text_hist.print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_num = len(train_loader)    # train_loader\n",
    "        for idx, (x, x_lens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y = x \n",
    "            \n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            # 这个函数计算的是全局梯度范数\n",
    "            # torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "            # parameters: an iterable of Variables that will have gradients normalized\n",
    "            # max_norm: max norm of the gradients(阈值设定)\n",
    "            # norm_type: type of the used p-norm. Can be'inf'for infinity norm(定义范数类型)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Training loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        train_losses.append(train_loss / train_num)\n",
    "        text_hist.print(f\"※※※Training loss {train_loss / train_num: .3f}※※※\")\n",
    "\n",
    "        last_model_name = \"PT_{}_{}_full.pt\".format(ts, epoch)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, last_model_name))\n",
    "        text_hist.print(\"Training timepoint saved\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.\n",
    "        valid_num = len(valid_loader)\n",
    "        for idx, (x, x_lens) in enumerate(valid_loader):\n",
    "            y = x    # extract MFCC-only data\n",
    "            x_mask = generate_mask_from_lengths_mat(x_lens, device=device)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            recon_x, attn_weight = model(x, x_lens, x_mask)\n",
    "\n",
    "            loss = model_loss.get_loss(recon_x, y, x_mask)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                text_hist.print(f\"Valid loss {loss: .3f} in Step {idx}\")\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        text_hist.print(f\"※※※Valid loss {valid_loss / valid_num: .3f}※※※\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss  0.716 in Step 0\n",
      "Training loss  0.677 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.681 in Step 300\n",
      "Training loss  0.684 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.678 in Step 600\n",
      "Training loss  0.691 in Step 700\n",
      "Training loss  0.692 in Step 800\n",
      "Training loss  0.695 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.699 in Step 1100\n",
      "Training loss  0.696 in Step 1200\n",
      "Training loss  0.686 in Step 1300\n",
      "Training loss  0.689 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.683 in Step 1600\n",
      "Training loss  0.682 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 151\n",
      "Training loss  0.683 in Step 0\n",
      "Training loss  0.675 in Step 100\n",
      "Training loss  0.692 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.697 in Step 400\n",
      "Training loss  0.686 in Step 500\n",
      "Training loss  0.693 in Step 600\n",
      "Training loss  0.683 in Step 700\n",
      "Training loss  0.688 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.686 in Step 1100\n",
      "Training loss  0.666 in Step 1200\n",
      "Training loss  0.674 in Step 1300\n",
      "Training loss  0.696 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.696 in Step 1600\n",
      "Training loss  0.682 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 152\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.681 in Step 100\n",
      "Training loss  0.691 in Step 200\n",
      "Training loss  0.676 in Step 300\n",
      "Training loss  0.701 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.686 in Step 600\n",
      "Training loss  0.672 in Step 700\n",
      "Training loss  0.703 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.704 in Step 1000\n",
      "Training loss  0.673 in Step 1100\n",
      "Training loss  0.659 in Step 1200\n",
      "Training loss  0.699 in Step 1300\n",
      "Training loss  0.684 in Step 1400\n",
      "Training loss  0.670 in Step 1500\n",
      "Training loss  0.667 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 153\n",
      "Training loss  0.692 in Step 0\n",
      "Training loss  0.686 in Step 100\n",
      "Training loss  0.653 in Step 200\n",
      "Training loss  0.658 in Step 300\n",
      "Training loss  0.698 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.654 in Step 700\n",
      "Training loss  0.691 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.681 in Step 1000\n",
      "Training loss  0.683 in Step 1100\n",
      "Training loss  0.664 in Step 1200\n",
      "Training loss  0.686 in Step 1300\n",
      "Training loss  0.660 in Step 1400\n",
      "Training loss  0.681 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 154\n",
      "Training loss  0.694 in Step 0\n",
      "Training loss  0.673 in Step 100\n",
      "Training loss  0.678 in Step 200\n",
      "Training loss  0.706 in Step 300\n",
      "Training loss  0.692 in Step 400\n",
      "Training loss  0.682 in Step 500\n",
      "Training loss  0.709 in Step 600\n",
      "Training loss  0.697 in Step 700\n",
      "Training loss  0.696 in Step 800\n",
      "Training loss  0.712 in Step 900\n",
      "Training loss  0.702 in Step 1000\n",
      "Training loss  0.697 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.672 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.685 in Step 1500\n",
      "Training loss  0.690 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.688※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.700 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 155\n",
      "Training loss  0.689 in Step 0\n",
      "Training loss  0.680 in Step 100\n",
      "Training loss  0.671 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.668 in Step 400\n",
      "Training loss  0.669 in Step 500\n",
      "Training loss  0.702 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.689 in Step 800\n",
      "Training loss  0.700 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.677 in Step 1100\n",
      "Training loss  0.683 in Step 1200\n",
      "Training loss  0.663 in Step 1300\n",
      "Training loss  0.675 in Step 1400\n",
      "Training loss  0.665 in Step 1500\n",
      "Training loss  0.708 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 156\n",
      "Training loss  0.687 in Step 0\n",
      "Training loss  0.665 in Step 100\n",
      "Training loss  0.670 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.676 in Step 400\n",
      "Training loss  0.718 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.689 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.677 in Step 900\n",
      "Training loss  0.696 in Step 1000\n",
      "Training loss  0.700 in Step 1100\n",
      "Training loss  0.664 in Step 1200\n",
      "Training loss  0.697 in Step 1300\n",
      "Training loss  0.671 in Step 1400\n",
      "Training loss  0.702 in Step 1500\n",
      "Training loss  0.686 in Step 1600\n",
      "Training loss  0.691 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 157\n",
      "Training loss  0.663 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.704 in Step 200\n",
      "Training loss  0.667 in Step 300\n",
      "Training loss  0.687 in Step 400\n",
      "Training loss  0.656 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.670 in Step 800\n",
      "Training loss  0.692 in Step 900\n",
      "Training loss  0.656 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.685 in Step 1200\n",
      "Training loss  0.660 in Step 1300\n",
      "Training loss  0.669 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.656 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.698 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 158\n",
      "Training loss  0.681 in Step 0\n",
      "Training loss  0.676 in Step 100\n",
      "Training loss  0.662 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.683 in Step 400\n",
      "Training loss  0.697 in Step 500\n",
      "Training loss  0.670 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.713 in Step 800\n",
      "Training loss  0.667 in Step 900\n",
      "Training loss  0.677 in Step 1000\n",
      "Training loss  0.697 in Step 1100\n",
      "Training loss  0.666 in Step 1200\n",
      "Training loss  0.675 in Step 1300\n",
      "Training loss  0.657 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.653 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 159\n",
      "Training loss  0.694 in Step 0\n",
      "Training loss  0.664 in Step 100\n",
      "Training loss  0.685 in Step 200\n",
      "Training loss  0.720 in Step 300\n",
      "Training loss  0.695 in Step 400\n",
      "Training loss  0.668 in Step 500\n",
      "Training loss  0.688 in Step 600\n",
      "Training loss  0.738 in Step 700\n",
      "Training loss  0.696 in Step 800\n",
      "Training loss  0.667 in Step 900\n",
      "Training loss  0.659 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.712 in Step 1200\n",
      "Training loss  0.672 in Step 1300\n",
      "Training loss  0.683 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.671 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 160\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.714 in Step 200\n",
      "Training loss  0.709 in Step 300\n",
      "Training loss  0.672 in Step 400\n",
      "Training loss  0.677 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.660 in Step 700\n",
      "Training loss  0.669 in Step 800\n",
      "Training loss  0.643 in Step 900\n",
      "Training loss  0.702 in Step 1000\n",
      "Training loss  0.685 in Step 1100\n",
      "Training loss  0.680 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.685 in Step 1400\n",
      "Training loss  0.693 in Step 1500\n",
      "Training loss  0.681 in Step 1600\n",
      "Training loss  0.653 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 161\n",
      "Training loss  0.688 in Step 0\n",
      "Training loss  0.673 in Step 100\n",
      "Training loss  0.695 in Step 200\n",
      "Training loss  0.683 in Step 300\n",
      "Training loss  0.681 in Step 400\n",
      "Training loss  0.695 in Step 500\n",
      "Training loss  0.687 in Step 600\n",
      "Training loss  0.651 in Step 700\n",
      "Training loss  0.665 in Step 800\n",
      "Training loss  0.711 in Step 900\n",
      "Training loss  0.648 in Step 1000\n",
      "Training loss  0.702 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.691 in Step 1300\n",
      "Training loss  0.694 in Step 1400\n",
      "Training loss  0.700 in Step 1500\n",
      "Training loss  0.683 in Step 1600\n",
      "Training loss  0.702 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 162\n",
      "Training loss  0.696 in Step 0\n",
      "Training loss  0.694 in Step 100\n",
      "Training loss  0.669 in Step 200\n",
      "Training loss  0.672 in Step 300\n",
      "Training loss  0.667 in Step 400\n",
      "Training loss  0.683 in Step 500\n",
      "Training loss  0.656 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.668 in Step 800\n",
      "Training loss  0.695 in Step 900\n",
      "Training loss  0.625 in Step 1000\n",
      "Training loss  0.695 in Step 1100\n",
      "Training loss  0.666 in Step 1200\n",
      "Training loss  0.686 in Step 1300\n",
      "Training loss  0.682 in Step 1400\n",
      "Training loss  0.690 in Step 1500\n",
      "Training loss  0.686 in Step 1600\n",
      "Training loss  0.683 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 163\n",
      "Training loss  0.693 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.678 in Step 200\n",
      "Training loss  0.680 in Step 300\n",
      "Training loss  0.714 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.689 in Step 700\n",
      "Training loss  0.702 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.666 in Step 1100\n",
      "Training loss  0.686 in Step 1200\n",
      "Training loss  0.709 in Step 1300\n",
      "Training loss  0.694 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.675 in Step 1600\n",
      "Training loss  0.670 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.699 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 164\n",
      "Training loss  0.683 in Step 0\n",
      "Training loss  0.670 in Step 100\n",
      "Training loss  0.707 in Step 200\n",
      "Training loss  0.680 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.690 in Step 500\n",
      "Training loss  0.678 in Step 600\n",
      "Training loss  0.664 in Step 700\n",
      "Training loss  0.680 in Step 800\n",
      "Training loss  0.690 in Step 900\n",
      "Training loss  0.701 in Step 1000\n",
      "Training loss  0.716 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.692 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.660 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 165\n",
      "Training loss  0.673 in Step 0\n",
      "Training loss  0.706 in Step 100\n",
      "Training loss  0.676 in Step 200\n",
      "Training loss  0.699 in Step 300\n",
      "Training loss  0.713 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.699 in Step 700\n",
      "Training loss  0.666 in Step 800\n",
      "Training loss  0.678 in Step 900\n",
      "Training loss  0.670 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.659 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.687 in Step 1400\n",
      "Training loss  0.697 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.684※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 166\n",
      "Training loss  0.703 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.675 in Step 200\n",
      "Training loss  0.670 in Step 300\n",
      "Training loss  0.662 in Step 400\n",
      "Training loss  0.676 in Step 500\n",
      "Training loss  0.687 in Step 600\n",
      "Training loss  0.636 in Step 700\n",
      "Training loss  0.683 in Step 800\n",
      "Training loss  0.687 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.680 in Step 1200\n",
      "Training loss  0.687 in Step 1300\n",
      "Training loss  0.664 in Step 1400\n",
      "Training loss  0.696 in Step 1500\n",
      "Training loss  0.685 in Step 1600\n",
      "Training loss  0.662 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.670 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.704 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 167\n",
      "Training loss  0.678 in Step 0\n",
      "Training loss  0.653 in Step 100\n",
      "Training loss  0.667 in Step 200\n",
      "Training loss  0.650 in Step 300\n",
      "Training loss  0.671 in Step 400\n",
      "Training loss  0.664 in Step 500\n",
      "Training loss  0.695 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.685 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.687 in Step 1100\n",
      "Training loss  0.697 in Step 1200\n",
      "Training loss  0.685 in Step 1300\n",
      "Training loss  0.699 in Step 1400\n",
      "Training loss  0.667 in Step 1500\n",
      "Training loss  0.683 in Step 1600\n",
      "Training loss  0.676 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 168\n",
      "Training loss  0.665 in Step 0\n",
      "Training loss  0.676 in Step 100\n",
      "Training loss  0.696 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.672 in Step 400\n",
      "Training loss  0.682 in Step 500\n",
      "Training loss  0.677 in Step 600\n",
      "Training loss  0.690 in Step 700\n",
      "Training loss  0.659 in Step 800\n",
      "Training loss  0.687 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.670 in Step 1100\n",
      "Training loss  0.680 in Step 1200\n",
      "Training loss  0.709 in Step 1300\n",
      "Training loss  0.684 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.697 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.686※※※\n",
      "Epoch 169\n",
      "Training loss  0.687 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.717 in Step 200\n",
      "Training loss  0.661 in Step 300\n",
      "Training loss  0.686 in Step 400\n",
      "Training loss  0.693 in Step 500\n",
      "Training loss  0.665 in Step 600\n",
      "Training loss  0.684 in Step 700\n",
      "Training loss  0.677 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.707 in Step 1000\n",
      "Training loss  0.673 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.657 in Step 1300\n",
      "Training loss  0.701 in Step 1400\n",
      "Training loss  0.687 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.669 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.674 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 170\n",
      "Training loss  0.691 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.685 in Step 200\n",
      "Training loss  0.684 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.662 in Step 500\n",
      "Training loss  0.682 in Step 600\n",
      "Training loss  0.708 in Step 700\n",
      "Training loss  0.697 in Step 800\n",
      "Training loss  0.690 in Step 900\n",
      "Training loss  0.672 in Step 1000\n",
      "Training loss  0.678 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.662 in Step 1300\n",
      "Training loss  0.690 in Step 1400\n",
      "Training loss  0.692 in Step 1500\n",
      "Training loss  0.690 in Step 1600\n",
      "Training loss  0.667 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 171\n",
      "Training loss  0.712 in Step 0\n",
      "Training loss  0.682 in Step 100\n",
      "Training loss  0.686 in Step 200\n",
      "Training loss  0.660 in Step 300\n",
      "Training loss  0.683 in Step 400\n",
      "Training loss  0.654 in Step 500\n",
      "Training loss  0.677 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.668 in Step 800\n",
      "Training loss  0.659 in Step 900\n",
      "Training loss  0.682 in Step 1000\n",
      "Training loss  0.694 in Step 1100\n",
      "Training loss  0.687 in Step 1200\n",
      "Training loss  0.683 in Step 1300\n",
      "Training loss  0.675 in Step 1400\n",
      "Training loss  0.695 in Step 1500\n",
      "Training loss  0.671 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 172\n",
      "Training loss  0.723 in Step 0\n",
      "Training loss  0.649 in Step 100\n",
      "Training loss  0.679 in Step 200\n",
      "Training loss  0.670 in Step 300\n",
      "Training loss  0.682 in Step 400\n",
      "Training loss  0.687 in Step 500\n",
      "Training loss  0.692 in Step 600\n",
      "Training loss  0.685 in Step 700\n",
      "Training loss  0.663 in Step 800\n",
      "Training loss  0.658 in Step 900\n",
      "Training loss  0.657 in Step 1000\n",
      "Training loss  0.634 in Step 1100\n",
      "Training loss  0.687 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.684 in Step 1400\n",
      "Training loss  0.690 in Step 1500\n",
      "Training loss  0.659 in Step 1600\n",
      "Training loss  0.704 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.686 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.698 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 173\n",
      "Training loss  0.681 in Step 0\n",
      "Training loss  0.693 in Step 100\n",
      "Training loss  0.691 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.677 in Step 400\n",
      "Training loss  0.706 in Step 500\n",
      "Training loss  0.688 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.697 in Step 800\n",
      "Training loss  0.672 in Step 900\n",
      "Training loss  0.669 in Step 1000\n",
      "Training loss  0.652 in Step 1100\n",
      "Training loss  0.669 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.671 in Step 1500\n",
      "Training loss  0.684 in Step 1600\n",
      "Training loss  0.698 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.700 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 174\n",
      "Training loss  0.687 in Step 0\n",
      "Training loss  0.707 in Step 100\n",
      "Training loss  0.667 in Step 200\n",
      "Training loss  0.670 in Step 300\n",
      "Training loss  0.679 in Step 400\n",
      "Training loss  0.687 in Step 500\n",
      "Training loss  0.682 in Step 600\n",
      "Training loss  0.698 in Step 700\n",
      "Training loss  0.683 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.690 in Step 1000\n",
      "Training loss  0.672 in Step 1100\n",
      "Training loss  0.689 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.677 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.695 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 175\n",
      "Training loss  0.665 in Step 0\n",
      "Training loss  0.690 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.659 in Step 300\n",
      "Training loss  0.695 in Step 400\n",
      "Training loss  0.684 in Step 500\n",
      "Training loss  0.677 in Step 600\n",
      "Training loss  0.670 in Step 700\n",
      "Training loss  0.660 in Step 800\n",
      "Training loss  0.669 in Step 900\n",
      "Training loss  0.672 in Step 1000\n",
      "Training loss  0.675 in Step 1100\n",
      "Training loss  0.654 in Step 1200\n",
      "Training loss  0.695 in Step 1300\n",
      "Training loss  0.683 in Step 1400\n",
      "Training loss  0.691 in Step 1500\n",
      "Training loss  0.674 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.700 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 176\n",
      "Training loss  0.659 in Step 0\n",
      "Training loss  0.695 in Step 100\n",
      "Training loss  0.648 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.681 in Step 400\n",
      "Training loss  0.668 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.673 in Step 700\n",
      "Training loss  0.695 in Step 800\n",
      "Training loss  0.710 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.686 in Step 1100\n",
      "Training loss  0.690 in Step 1200\n",
      "Training loss  0.686 in Step 1300\n",
      "Training loss  0.683 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.687 in Step 1600\n",
      "Training loss  0.685 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 177\n",
      "Training loss  0.659 in Step 0\n",
      "Training loss  0.653 in Step 100\n",
      "Training loss  0.692 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.703 in Step 400\n",
      "Training loss  0.669 in Step 500\n",
      "Training loss  0.693 in Step 600\n",
      "Training loss  0.661 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.692 in Step 900\n",
      "Training loss  0.699 in Step 1000\n",
      "Training loss  0.699 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.665 in Step 1500\n",
      "Training loss  0.695 in Step 1600\n",
      "Training loss  0.677 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.704 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 178\n",
      "Training loss  0.669 in Step 0\n",
      "Training loss  0.661 in Step 100\n",
      "Training loss  0.672 in Step 200\n",
      "Training loss  0.685 in Step 300\n",
      "Training loss  0.661 in Step 400\n",
      "Training loss  0.681 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.683 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.684 in Step 1200\n",
      "Training loss  0.654 in Step 1300\n",
      "Training loss  0.689 in Step 1400\n",
      "Training loss  0.685 in Step 1500\n",
      "Training loss  0.659 in Step 1600\n",
      "Training loss  0.662 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 179\n",
      "Training loss  0.675 in Step 0\n",
      "Training loss  0.701 in Step 100\n",
      "Training loss  0.683 in Step 200\n",
      "Training loss  0.686 in Step 300\n",
      "Training loss  0.654 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.661 in Step 600\n",
      "Training loss  0.685 in Step 700\n",
      "Training loss  0.693 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.686 in Step 1000\n",
      "Training loss  0.693 in Step 1100\n",
      "Training loss  0.664 in Step 1200\n",
      "Training loss  0.675 in Step 1300\n",
      "Training loss  0.685 in Step 1400\n",
      "Training loss  0.664 in Step 1500\n",
      "Training loss  0.702 in Step 1600\n",
      "Training loss  0.679 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.680 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.705 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 180\n",
      "Training loss  0.708 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.690 in Step 300\n",
      "Training loss  0.699 in Step 400\n",
      "Training loss  0.673 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.680 in Step 700\n",
      "Training loss  0.665 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.667 in Step 1000\n",
      "Training loss  0.676 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.662 in Step 1300\n",
      "Training loss  0.697 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.688 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.699 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 181\n",
      "Training loss  0.691 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.693 in Step 200\n",
      "Training loss  0.693 in Step 300\n",
      "Training loss  0.682 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.664 in Step 600\n",
      "Training loss  0.694 in Step 700\n",
      "Training loss  0.674 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.667 in Step 1000\n",
      "Training loss  0.667 in Step 1100\n",
      "Training loss  0.693 in Step 1200\n",
      "Training loss  0.669 in Step 1300\n",
      "Training loss  0.675 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.654 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 182\n",
      "Training loss  0.676 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.689 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.666 in Step 500\n",
      "Training loss  0.675 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.684 in Step 800\n",
      "Training loss  0.675 in Step 900\n",
      "Training loss  0.704 in Step 1000\n",
      "Training loss  0.652 in Step 1100\n",
      "Training loss  0.661 in Step 1200\n",
      "Training loss  0.689 in Step 1300\n",
      "Training loss  0.697 in Step 1400\n",
      "Training loss  0.705 in Step 1500\n",
      "Training loss  0.686 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.681 in Step 100\n",
      "Valid loss  0.678 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 183\n",
      "Training loss  0.680 in Step 0\n",
      "Training loss  0.675 in Step 100\n",
      "Training loss  0.690 in Step 200\n",
      "Training loss  0.681 in Step 300\n",
      "Training loss  0.661 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.693 in Step 700\n",
      "Training loss  0.666 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.678 in Step 1000\n",
      "Training loss  0.668 in Step 1100\n",
      "Training loss  0.674 in Step 1200\n",
      "Training loss  0.667 in Step 1300\n",
      "Training loss  0.666 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.719 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 184\n",
      "Training loss  0.690 in Step 0\n",
      "Training loss  0.668 in Step 100\n",
      "Training loss  0.665 in Step 200\n",
      "Training loss  0.696 in Step 300\n",
      "Training loss  0.697 in Step 400\n",
      "Training loss  0.655 in Step 500\n",
      "Training loss  0.661 in Step 600\n",
      "Training loss  0.681 in Step 700\n",
      "Training loss  0.665 in Step 800\n",
      "Training loss  0.709 in Step 900\n",
      "Training loss  0.699 in Step 1000\n",
      "Training loss  0.682 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.685 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.655 in Step 1600\n",
      "Training loss  0.677 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.680 in Step 100\n",
      "Valid loss  0.677 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.702 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 185\n",
      "Training loss  0.695 in Step 0\n",
      "Training loss  0.672 in Step 100\n",
      "Training loss  0.675 in Step 200\n",
      "Training loss  0.697 in Step 300\n",
      "Training loss  0.670 in Step 400\n",
      "Training loss  0.696 in Step 500\n",
      "Training loss  0.710 in Step 600\n",
      "Training loss  0.661 in Step 700\n",
      "Training loss  0.658 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.687 in Step 1000\n",
      "Training loss  0.687 in Step 1100\n",
      "Training loss  0.684 in Step 1200\n",
      "Training loss  0.682 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.654 in Step 1500\n",
      "Training loss  0.691 in Step 1600\n",
      "Training loss  0.665 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.699 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 186\n",
      "Training loss  0.661 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.682 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.703 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.686 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.683 in Step 900\n",
      "Training loss  0.690 in Step 1000\n",
      "Training loss  0.648 in Step 1100\n",
      "Training loss  0.686 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.691 in Step 1400\n",
      "Training loss  0.692 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.674 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 187\n",
      "Training loss  0.690 in Step 0\n",
      "Training loss  0.674 in Step 100\n",
      "Training loss  0.667 in Step 200\n",
      "Training loss  0.663 in Step 300\n",
      "Training loss  0.662 in Step 400\n",
      "Training loss  0.662 in Step 500\n",
      "Training loss  0.677 in Step 600\n",
      "Training loss  0.690 in Step 700\n",
      "Training loss  0.691 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.682 in Step 1000\n",
      "Training loss  0.669 in Step 1100\n",
      "Training loss  0.689 in Step 1200\n",
      "Training loss  0.699 in Step 1300\n",
      "Training loss  0.669 in Step 1400\n",
      "Training loss  0.685 in Step 1500\n",
      "Training loss  0.694 in Step 1600\n",
      "Training loss  0.702 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 188\n",
      "Training loss  0.690 in Step 0\n",
      "Training loss  0.678 in Step 100\n",
      "Training loss  0.685 in Step 200\n",
      "Training loss  0.683 in Step 300\n",
      "Training loss  0.697 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.670 in Step 600\n",
      "Training loss  0.702 in Step 700\n",
      "Training loss  0.671 in Step 800\n",
      "Training loss  0.664 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.699 in Step 1100\n",
      "Training loss  0.688 in Step 1200\n",
      "Training loss  0.699 in Step 1300\n",
      "Training loss  0.701 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.682 in Step 1600\n",
      "Training loss  0.663 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 189\n",
      "Training loss  0.662 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.678 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.654 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.693 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.667 in Step 800\n",
      "Training loss  0.678 in Step 900\n",
      "Training loss  0.682 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.691 in Step 1200\n",
      "Training loss  0.680 in Step 1300\n",
      "Training loss  0.694 in Step 1400\n",
      "Training loss  0.663 in Step 1500\n",
      "Training loss  0.668 in Step 1600\n",
      "Training loss  0.669 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.688 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 190\n",
      "Training loss  0.675 in Step 0\n",
      "Training loss  0.699 in Step 100\n",
      "Training loss  0.669 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.686 in Step 400\n",
      "Training loss  0.694 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.670 in Step 700\n",
      "Training loss  0.676 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.678 in Step 1000\n",
      "Training loss  0.727 in Step 1100\n",
      "Training loss  0.683 in Step 1200\n",
      "Training loss  0.695 in Step 1300\n",
      "Training loss  0.682 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.691 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.673 in Step 0\n",
      "Valid loss  0.708 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.697※※※\n",
      "Epoch 191\n",
      "Training loss  0.716 in Step 0\n",
      "Training loss  0.694 in Step 100\n",
      "Training loss  0.712 in Step 200\n",
      "Training loss  0.696 in Step 300\n",
      "Training loss  0.694 in Step 400\n",
      "Training loss  0.700 in Step 500\n",
      "Training loss  0.704 in Step 600\n",
      "Training loss  0.714 in Step 700\n",
      "Training loss  0.693 in Step 800\n",
      "Training loss  0.703 in Step 900\n",
      "Training loss  0.668 in Step 1000\n",
      "Training loss  0.704 in Step 1100\n",
      "Training loss  0.695 in Step 1200\n",
      "Training loss  0.663 in Step 1300\n",
      "Training loss  0.716 in Step 1400\n",
      "Training loss  0.709 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.707 in Step 1700\n",
      "※※※Training loss  0.696※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.686 in Step 0\n",
      "Valid loss  0.707 in Step 100\n",
      "Valid loss  0.685 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.708 in Step 400\n",
      "※※※Valid loss  0.701※※※\n",
      "Epoch 192\n",
      "Training loss  0.703 in Step 0\n",
      "Training loss  0.690 in Step 100\n",
      "Training loss  0.658 in Step 200\n",
      "Training loss  0.680 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.694 in Step 500\n",
      "Training loss  0.693 in Step 600\n",
      "Training loss  0.682 in Step 700\n",
      "Training loss  0.662 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.671 in Step 1000\n",
      "Training loss  0.664 in Step 1100\n",
      "Training loss  0.718 in Step 1200\n",
      "Training loss  0.700 in Step 1300\n",
      "Training loss  0.693 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.694 in Step 1600\n",
      "Training loss  0.708 in Step 1700\n",
      "※※※Training loss  0.690※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.680 in Step 0\n",
      "Valid loss  0.697 in Step 100\n",
      "Valid loss  0.693 in Step 200\n",
      "Valid loss  0.702 in Step 300\n",
      "Valid loss  0.710 in Step 400\n",
      "※※※Valid loss  0.702※※※\n",
      "Epoch 193\n",
      "Training loss  0.696 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.693 in Step 200\n",
      "Training loss  0.682 in Step 300\n",
      "Training loss  0.699 in Step 400\n",
      "Training loss  0.692 in Step 500\n",
      "Training loss  0.666 in Step 600\n",
      "Training loss  0.713 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.682 in Step 900\n",
      "Training loss  0.673 in Step 1000\n",
      "Training loss  0.653 in Step 1100\n",
      "Training loss  0.681 in Step 1200\n",
      "Training loss  0.679 in Step 1300\n",
      "Training loss  0.705 in Step 1400\n",
      "Training loss  0.674 in Step 1500\n",
      "Training loss  0.665 in Step 1600\n",
      "Training loss  0.671 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 194\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.693 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.675 in Step 500\n",
      "Training loss  0.668 in Step 600\n",
      "Training loss  0.685 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.691 in Step 1100\n",
      "Training loss  0.677 in Step 1200\n",
      "Training loss  0.709 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.694 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 195\n",
      "Training loss  0.696 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.668 in Step 200\n",
      "Training loss  0.676 in Step 300\n",
      "Training loss  0.695 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.659 in Step 600\n",
      "Training loss  0.696 in Step 700\n",
      "Training loss  0.689 in Step 800\n",
      "Training loss  0.697 in Step 900\n",
      "Training loss  0.680 in Step 1000\n",
      "Training loss  0.702 in Step 1100\n",
      "Training loss  0.711 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.659 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.686 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 196\n",
      "Training loss  0.668 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.666 in Step 400\n",
      "Training loss  0.675 in Step 500\n",
      "Training loss  0.671 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.693 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.669 in Step 1200\n",
      "Training loss  0.709 in Step 1300\n",
      "Training loss  0.688 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.668 in Step 1600\n",
      "Training loss  0.702 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.688 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 197\n",
      "Training loss  0.660 in Step 0\n",
      "Training loss  0.660 in Step 100\n",
      "Training loss  0.660 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.687 in Step 400\n",
      "Training loss  0.677 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.699 in Step 700\n",
      "Training loss  0.710 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.673 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.662 in Step 1300\n",
      "Training loss  0.662 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.676 in Step 1600\n",
      "Training loss  0.704 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 198\n",
      "Training loss  0.697 in Step 0\n",
      "Training loss  0.668 in Step 100\n",
      "Training loss  0.682 in Step 200\n",
      "Training loss  0.671 in Step 300\n",
      "Training loss  0.670 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.681 in Step 700\n",
      "Training loss  0.666 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.695 in Step 1000\n",
      "Training loss  0.685 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.695 in Step 1300\n",
      "Training loss  0.688 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.697 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 199\n",
      "Training loss  0.674 in Step 0\n",
      "Training loss  0.657 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.677 in Step 300\n",
      "Training loss  0.702 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.694 in Step 600\n",
      "Training loss  0.712 in Step 700\n",
      "Training loss  0.696 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.683 in Step 1200\n",
      "Training loss  0.670 in Step 1300\n",
      "Training loss  0.683 in Step 1400\n",
      "Training loss  0.684 in Step 1500\n",
      "Training loss  0.651 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 200\n",
      "Training loss  0.667 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.663 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.684 in Step 400\n",
      "Training loss  0.687 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.701 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.665 in Step 1000\n",
      "Training loss  0.675 in Step 1100\n",
      "Training loss  0.670 in Step 1200\n",
      "Training loss  0.700 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.670 in Step 1600\n",
      "Training loss  0.676 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 201\n",
      "Training loss  0.657 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.691 in Step 200\n",
      "Training loss  0.667 in Step 300\n",
      "Training loss  0.666 in Step 400\n",
      "Training loss  0.681 in Step 500\n",
      "Training loss  0.683 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.689 in Step 800\n",
      "Training loss  0.678 in Step 900\n",
      "Training loss  0.665 in Step 1000\n",
      "Training loss  0.676 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.699 in Step 1300\n",
      "Training loss  0.662 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.681 in Step 1600\n",
      "Training loss  0.661 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.685 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 202\n",
      "Training loss  0.691 in Step 0\n",
      "Training loss  0.675 in Step 100\n",
      "Training loss  0.664 in Step 200\n",
      "Training loss  0.655 in Step 300\n",
      "Training loss  0.703 in Step 400\n",
      "Training loss  0.706 in Step 500\n",
      "Training loss  0.688 in Step 600\n",
      "Training loss  0.657 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.697 in Step 900\n",
      "Training loss  0.687 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.705 in Step 1200\n",
      "Training loss  0.700 in Step 1300\n",
      "Training loss  0.667 in Step 1400\n",
      "Training loss  0.696 in Step 1500\n",
      "Training loss  0.699 in Step 1600\n",
      "Training loss  0.670 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.688 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 203\n",
      "Training loss  0.675 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.696 in Step 200\n",
      "Training loss  0.696 in Step 300\n",
      "Training loss  0.675 in Step 400\n",
      "Training loss  0.690 in Step 500\n",
      "Training loss  0.669 in Step 600\n",
      "Training loss  0.680 in Step 700\n",
      "Training loss  0.660 in Step 800\n",
      "Training loss  0.707 in Step 900\n",
      "Training loss  0.677 in Step 1000\n",
      "Training loss  0.677 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.699 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.676 in Step 1500\n",
      "Training loss  0.670 in Step 1600\n",
      "Training loss  0.655 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 204\n",
      "Training loss  0.671 in Step 0\n",
      "Training loss  0.686 in Step 100\n",
      "Training loss  0.692 in Step 200\n",
      "Training loss  0.685 in Step 300\n",
      "Training loss  0.662 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.684 in Step 700\n",
      "Training loss  0.659 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.673 in Step 1100\n",
      "Training loss  0.663 in Step 1200\n",
      "Training loss  0.719 in Step 1300\n",
      "Training loss  0.664 in Step 1400\n",
      "Training loss  0.662 in Step 1500\n",
      "Training loss  0.700 in Step 1600\n",
      "Training loss  0.702 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 205\n",
      "Training loss  0.687 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.698 in Step 300\n",
      "Training loss  0.677 in Step 400\n",
      "Training loss  0.677 in Step 500\n",
      "Training loss  0.668 in Step 600\n",
      "Training loss  0.680 in Step 700\n",
      "Training loss  0.704 in Step 800\n",
      "Training loss  0.688 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.682 in Step 1100\n",
      "Training loss  0.710 in Step 1200\n",
      "Training loss  0.679 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.698 in Step 1500\n",
      "Training loss  0.691 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.670 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 206\n",
      "Training loss  0.682 in Step 0\n",
      "Training loss  0.705 in Step 100\n",
      "Training loss  0.686 in Step 200\n",
      "Training loss  0.680 in Step 300\n",
      "Training loss  0.695 in Step 400\n",
      "Training loss  0.695 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.702 in Step 700\n",
      "Training loss  0.656 in Step 800\n",
      "Training loss  0.700 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.681 in Step 1200\n",
      "Training loss  0.723 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.703 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.667 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 207\n",
      "Training loss  0.700 in Step 0\n",
      "Training loss  0.686 in Step 100\n",
      "Training loss  0.700 in Step 200\n",
      "Training loss  0.668 in Step 300\n",
      "Training loss  0.685 in Step 400\n",
      "Training loss  0.692 in Step 500\n",
      "Training loss  0.668 in Step 600\n",
      "Training loss  0.680 in Step 700\n",
      "Training loss  0.657 in Step 800\n",
      "Training loss  0.671 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.658 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.685 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.706 in Step 1600\n",
      "Training loss  0.674 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 208\n",
      "Training loss  0.693 in Step 0\n",
      "Training loss  0.676 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.691 in Step 300\n",
      "Training loss  0.684 in Step 400\n",
      "Training loss  0.679 in Step 500\n",
      "Training loss  0.687 in Step 600\n",
      "Training loss  0.713 in Step 700\n",
      "Training loss  0.705 in Step 800\n",
      "Training loss  0.664 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.678 in Step 1300\n",
      "Training loss  0.665 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.691 in Step 1600\n",
      "Training loss  0.688 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.674 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 209\n",
      "Training loss  0.669 in Step 0\n",
      "Training loss  0.695 in Step 100\n",
      "Training loss  0.703 in Step 200\n",
      "Training loss  0.645 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.691 in Step 600\n",
      "Training loss  0.693 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.659 in Step 1000\n",
      "Training loss  0.641 in Step 1100\n",
      "Training loss  0.681 in Step 1200\n",
      "Training loss  0.664 in Step 1300\n",
      "Training loss  0.682 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.681 in Step 1600\n",
      "Training loss  0.684 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 210\n",
      "Training loss  0.683 in Step 0\n",
      "Training loss  0.659 in Step 100\n",
      "Training loss  0.675 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.686 in Step 400\n",
      "Training loss  0.676 in Step 500\n",
      "Training loss  0.677 in Step 600\n",
      "Training loss  0.681 in Step 700\n",
      "Training loss  0.692 in Step 800\n",
      "Training loss  0.671 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.665 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.692 in Step 1300\n",
      "Training loss  0.682 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.664 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.699 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 211\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.670 in Step 100\n",
      "Training loss  0.682 in Step 200\n",
      "Training loss  0.672 in Step 300\n",
      "Training loss  0.666 in Step 400\n",
      "Training loss  0.708 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.660 in Step 800\n",
      "Training loss  0.669 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.660 in Step 1200\n",
      "Training loss  0.676 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.704 in Step 1500\n",
      "Training loss  0.671 in Step 1600\n",
      "Training loss  0.695 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.687 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 212\n",
      "Training loss  0.685 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.663 in Step 200\n",
      "Training loss  0.698 in Step 300\n",
      "Training loss  0.695 in Step 400\n",
      "Training loss  0.651 in Step 500\n",
      "Training loss  0.657 in Step 600\n",
      "Training loss  0.686 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.678 in Step 900\n",
      "Training loss  0.677 in Step 1000\n",
      "Training loss  0.668 in Step 1100\n",
      "Training loss  0.668 in Step 1200\n",
      "Training loss  0.680 in Step 1300\n",
      "Training loss  0.688 in Step 1400\n",
      "Training loss  0.665 in Step 1500\n",
      "Training loss  0.666 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 213\n",
      "Training loss  0.664 in Step 0\n",
      "Training loss  0.696 in Step 100\n",
      "Training loss  0.672 in Step 200\n",
      "Training loss  0.705 in Step 300\n",
      "Training loss  0.679 in Step 400\n",
      "Training loss  0.686 in Step 500\n",
      "Training loss  0.709 in Step 600\n",
      "Training loss  0.671 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.667 in Step 900\n",
      "Training loss  0.668 in Step 1000\n",
      "Training loss  0.668 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.683 in Step 1300\n",
      "Training loss  0.677 in Step 1400\n",
      "Training loss  0.690 in Step 1500\n",
      "Training loss  0.692 in Step 1600\n",
      "Training loss  0.679 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 214\n",
      "Training loss  0.696 in Step 0\n",
      "Training loss  0.657 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.667 in Step 300\n",
      "Training loss  0.692 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.662 in Step 600\n",
      "Training loss  0.691 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.653 in Step 900\n",
      "Training loss  0.672 in Step 1000\n",
      "Training loss  0.655 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.657 in Step 1300\n",
      "Training loss  0.678 in Step 1400\n",
      "Training loss  0.672 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.674 in Step 200\n",
      "Valid loss  0.707 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.687※※※\n",
      "Epoch 215\n",
      "Training loss  0.706 in Step 0\n",
      "Training loss  0.652 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.682 in Step 300\n",
      "Training loss  0.687 in Step 400\n",
      "Training loss  0.663 in Step 500\n",
      "Training loss  0.696 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.671 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.672 in Step 1000\n",
      "Training loss  0.669 in Step 1100\n",
      "Training loss  0.691 in Step 1200\n",
      "Training loss  0.654 in Step 1300\n",
      "Training loss  0.687 in Step 1400\n",
      "Training loss  0.696 in Step 1500\n",
      "Training loss  0.675 in Step 1600\n",
      "Training loss  0.666 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.688 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 216\n",
      "Training loss  0.669 in Step 0\n",
      "Training loss  0.673 in Step 100\n",
      "Training loss  0.676 in Step 200\n",
      "Training loss  0.683 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.686 in Step 700\n",
      "Training loss  0.663 in Step 800\n",
      "Training loss  0.669 in Step 900\n",
      "Training loss  0.694 in Step 1000\n",
      "Training loss  0.667 in Step 1100\n",
      "Training loss  0.677 in Step 1200\n",
      "Training loss  0.687 in Step 1300\n",
      "Training loss  0.665 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.682 in Step 1600\n",
      "Training loss  0.652 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 217\n",
      "Training loss  0.682 in Step 0\n",
      "Training loss  0.678 in Step 100\n",
      "Training loss  0.691 in Step 200\n",
      "Training loss  0.685 in Step 300\n",
      "Training loss  0.682 in Step 400\n",
      "Training loss  0.663 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.667 in Step 700\n",
      "Training loss  0.670 in Step 800\n",
      "Training loss  0.667 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.666 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.672 in Step 1300\n",
      "Training loss  0.678 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.696 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 218\n",
      "Training loss  0.680 in Step 0\n",
      "Training loss  0.683 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.673 in Step 400\n",
      "Training loss  0.679 in Step 500\n",
      "Training loss  0.671 in Step 600\n",
      "Training loss  0.691 in Step 700\n",
      "Training loss  0.665 in Step 800\n",
      "Training loss  0.660 in Step 900\n",
      "Training loss  0.679 in Step 1000\n",
      "Training loss  0.692 in Step 1100\n",
      "Training loss  0.664 in Step 1200\n",
      "Training loss  0.672 in Step 1300\n",
      "Training loss  0.697 in Step 1400\n",
      "Training loss  0.657 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.685 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 219\n",
      "Training loss  0.696 in Step 0\n",
      "Training loss  0.695 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.678 in Step 400\n",
      "Training loss  0.665 in Step 500\n",
      "Training loss  0.659 in Step 600\n",
      "Training loss  0.690 in Step 700\n",
      "Training loss  0.671 in Step 800\n",
      "Training loss  0.697 in Step 900\n",
      "Training loss  0.680 in Step 1000\n",
      "Training loss  0.662 in Step 1100\n",
      "Training loss  0.687 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.684 in Step 1400\n",
      "Training loss  0.684 in Step 1500\n",
      "Training loss  0.666 in Step 1600\n",
      "Training loss  0.634 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.687 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 220\n",
      "Training loss  0.671 in Step 0\n",
      "Training loss  0.681 in Step 100\n",
      "Training loss  0.678 in Step 200\n",
      "Training loss  0.668 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.654 in Step 600\n",
      "Training loss  0.695 in Step 700\n",
      "Training loss  0.695 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.693 in Step 1000\n",
      "Training loss  0.679 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.664 in Step 1300\n",
      "Training loss  0.669 in Step 1400\n",
      "Training loss  0.684 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 221\n",
      "Training loss  0.672 in Step 0\n",
      "Training loss  0.672 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.676 in Step 300\n",
      "Training loss  0.653 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.690 in Step 900\n",
      "Training loss  0.682 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.682 in Step 1300\n",
      "Training loss  0.693 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.659 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.688 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 222\n",
      "Training loss  0.662 in Step 0\n",
      "Training loss  0.671 in Step 100\n",
      "Training loss  0.701 in Step 200\n",
      "Training loss  0.686 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.677 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.683 in Step 700\n",
      "Training loss  0.699 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.704 in Step 1000\n",
      "Training loss  0.660 in Step 1100\n",
      "Training loss  0.669 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.668 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.684 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.704 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 223\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.674 in Step 100\n",
      "Training loss  0.692 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.682 in Step 400\n",
      "Training loss  0.690 in Step 500\n",
      "Training loss  0.665 in Step 600\n",
      "Training loss  0.695 in Step 700\n",
      "Training loss  0.677 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.680 in Step 1000\n",
      "Training loss  0.677 in Step 1100\n",
      "Training loss  0.691 in Step 1200\n",
      "Training loss  0.660 in Step 1300\n",
      "Training loss  0.682 in Step 1400\n",
      "Training loss  0.693 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.674 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 224\n",
      "Training loss  0.649 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.658 in Step 200\n",
      "Training loss  0.686 in Step 300\n",
      "Training loss  0.683 in Step 400\n",
      "Training loss  0.684 in Step 500\n",
      "Training loss  0.683 in Step 600\n",
      "Training loss  0.683 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.675 in Step 900\n",
      "Training loss  0.686 in Step 1000\n",
      "Training loss  0.693 in Step 1100\n",
      "Training loss  0.674 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.660 in Step 1400\n",
      "Training loss  0.699 in Step 1500\n",
      "Training loss  0.699 in Step 1600\n",
      "Training loss  0.704 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 225\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.671 in Step 100\n",
      "Training loss  0.649 in Step 200\n",
      "Training loss  0.679 in Step 300\n",
      "Training loss  0.660 in Step 400\n",
      "Training loss  0.664 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.646 in Step 700\n",
      "Training loss  0.674 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.671 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.653 in Step 1200\n",
      "Training loss  0.696 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.652 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.655 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 226\n",
      "Training loss  0.686 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.690 in Step 200\n",
      "Training loss  0.668 in Step 300\n",
      "Training loss  0.693 in Step 400\n",
      "Training loss  0.675 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.671 in Step 700\n",
      "Training loss  0.662 in Step 800\n",
      "Training loss  0.687 in Step 900\n",
      "Training loss  0.705 in Step 1000\n",
      "Training loss  0.695 in Step 1100\n",
      "Training loss  0.640 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.667 in Step 1400\n",
      "Training loss  0.685 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.695 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.663 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 227\n",
      "Training loss  0.688 in Step 0\n",
      "Training loss  0.695 in Step 100\n",
      "Training loss  0.669 in Step 200\n",
      "Training loss  0.672 in Step 300\n",
      "Training loss  0.664 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.690 in Step 600\n",
      "Training loss  0.685 in Step 700\n",
      "Training loss  0.679 in Step 800\n",
      "Training loss  0.689 in Step 900\n",
      "Training loss  0.707 in Step 1000\n",
      "Training loss  0.672 in Step 1100\n",
      "Training loss  0.659 in Step 1200\n",
      "Training loss  0.667 in Step 1300\n",
      "Training loss  0.688 in Step 1400\n",
      "Training loss  0.675 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.696 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.663 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 228\n",
      "Training loss  0.681 in Step 0\n",
      "Training loss  0.671 in Step 100\n",
      "Training loss  0.671 in Step 200\n",
      "Training loss  0.671 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.690 in Step 600\n",
      "Training loss  0.703 in Step 700\n",
      "Training loss  0.677 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.693 in Step 1100\n",
      "Training loss  0.671 in Step 1200\n",
      "Training loss  0.687 in Step 1300\n",
      "Training loss  0.665 in Step 1400\n",
      "Training loss  0.658 in Step 1500\n",
      "Training loss  0.691 in Step 1600\n",
      "Training loss  0.682 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.679 in Step 100\n",
      "Valid loss  0.674 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.707 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 229\n",
      "Training loss  0.681 in Step 0\n",
      "Training loss  0.674 in Step 100\n",
      "Training loss  0.686 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.659 in Step 500\n",
      "Training loss  0.683 in Step 600\n",
      "Training loss  0.684 in Step 700\n",
      "Training loss  0.663 in Step 800\n",
      "Training loss  0.707 in Step 900\n",
      "Training loss  0.666 in Step 1000\n",
      "Training loss  0.687 in Step 1100\n",
      "Training loss  0.665 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.702 in Step 1400\n",
      "Training loss  0.662 in Step 1500\n",
      "Training loss  0.690 in Step 1600\n",
      "Training loss  0.671 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.663 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 230\n",
      "Training loss  0.678 in Step 0\n",
      "Training loss  0.691 in Step 100\n",
      "Training loss  0.682 in Step 200\n",
      "Training loss  0.691 in Step 300\n",
      "Training loss  0.694 in Step 400\n",
      "Training loss  0.675 in Step 500\n",
      "Training loss  0.667 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.672 in Step 800\n",
      "Training loss  0.697 in Step 900\n",
      "Training loss  0.661 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.668 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.708 in Step 1400\n",
      "Training loss  0.720 in Step 1500\n",
      "Training loss  0.670 in Step 1600\n",
      "Training loss  0.687 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 231\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.690 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.664 in Step 400\n",
      "Training loss  0.658 in Step 500\n",
      "Training loss  0.667 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.665 in Step 800\n",
      "Training loss  0.663 in Step 900\n",
      "Training loss  0.671 in Step 1000\n",
      "Training loss  0.672 in Step 1100\n",
      "Training loss  0.675 in Step 1200\n",
      "Training loss  0.653 in Step 1300\n",
      "Training loss  0.672 in Step 1400\n",
      "Training loss  0.698 in Step 1500\n",
      "Training loss  0.667 in Step 1600\n",
      "Training loss  0.650 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.675 in Step 100\n",
      "Valid loss  0.673 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 232\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.674 in Step 100\n",
      "Training loss  0.695 in Step 200\n",
      "Training loss  0.660 in Step 300\n",
      "Training loss  0.647 in Step 400\n",
      "Training loss  0.688 in Step 500\n",
      "Training loss  0.667 in Step 600\n",
      "Training loss  0.686 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.691 in Step 1100\n",
      "Training loss  0.692 in Step 1200\n",
      "Training loss  0.667 in Step 1300\n",
      "Training loss  0.665 in Step 1400\n",
      "Training loss  0.699 in Step 1500\n",
      "Training loss  0.696 in Step 1600\n",
      "Training loss  0.648 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 233\n",
      "Training loss  0.666 in Step 0\n",
      "Training loss  0.661 in Step 100\n",
      "Training loss  0.674 in Step 200\n",
      "Training loss  0.667 in Step 300\n",
      "Training loss  0.701 in Step 400\n",
      "Training loss  0.683 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.685 in Step 700\n",
      "Training loss  0.651 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.708 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.674 in Step 1300\n",
      "Training loss  0.691 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.665 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.657 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 234\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.675 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.662 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.667 in Step 500\n",
      "Training loss  0.659 in Step 600\n",
      "Training loss  0.671 in Step 700\n",
      "Training loss  0.682 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.680 in Step 1000\n",
      "Training loss  0.689 in Step 1100\n",
      "Training loss  0.680 in Step 1200\n",
      "Training loss  0.671 in Step 1300\n",
      "Training loss  0.670 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.690 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.697 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 235\n",
      "Training loss  0.672 in Step 0\n",
      "Training loss  0.682 in Step 100\n",
      "Training loss  0.667 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.680 in Step 400\n",
      "Training loss  0.702 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.686 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.667 in Step 1000\n",
      "Training loss  0.672 in Step 1100\n",
      "Training loss  0.695 in Step 1200\n",
      "Training loss  0.675 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.699 in Step 1500\n",
      "Training loss  0.675 in Step 1600\n",
      "Training loss  0.664 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.657 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 236\n",
      "Training loss  0.657 in Step 0\n",
      "Training loss  0.666 in Step 100\n",
      "Training loss  0.704 in Step 200\n",
      "Training loss  0.647 in Step 300\n",
      "Training loss  0.702 in Step 400\n",
      "Training loss  0.687 in Step 500\n",
      "Training loss  0.675 in Step 600\n",
      "Training loss  0.674 in Step 700\n",
      "Training loss  0.691 in Step 800\n",
      "Training loss  0.687 in Step 900\n",
      "Training loss  0.664 in Step 1000\n",
      "Training loss  0.693 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.680 in Step 1300\n",
      "Training loss  0.685 in Step 1400\n",
      "Training loss  0.698 in Step 1500\n",
      "Training loss  0.667 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.664 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 237\n",
      "Training loss  0.674 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.674 in Step 200\n",
      "Training loss  0.686 in Step 300\n",
      "Training loss  0.687 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.685 in Step 700\n",
      "Training loss  0.706 in Step 800\n",
      "Training loss  0.660 in Step 900\n",
      "Training loss  0.672 in Step 1000\n",
      "Training loss  0.691 in Step 1100\n",
      "Training loss  0.704 in Step 1200\n",
      "Training loss  0.666 in Step 1300\n",
      "Training loss  0.663 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.724 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 238\n",
      "Training loss  0.673 in Step 0\n",
      "Training loss  0.674 in Step 100\n",
      "Training loss  0.666 in Step 200\n",
      "Training loss  0.680 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.684 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.691 in Step 700\n",
      "Training loss  0.674 in Step 800\n",
      "Training loss  0.673 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.662 in Step 1100\n",
      "Training loss  0.690 in Step 1200\n",
      "Training loss  0.674 in Step 1300\n",
      "Training loss  0.661 in Step 1400\n",
      "Training loss  0.663 in Step 1500\n",
      "Training loss  0.675 in Step 1600\n",
      "Training loss  0.674 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 239\n",
      "Training loss  0.670 in Step 0\n",
      "Training loss  0.691 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.656 in Step 300\n",
      "Training loss  0.682 in Step 400\n",
      "Training loss  0.676 in Step 500\n",
      "Training loss  0.665 in Step 600\n",
      "Training loss  0.667 in Step 700\n",
      "Training loss  0.689 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.707 in Step 1000\n",
      "Training loss  0.687 in Step 1100\n",
      "Training loss  0.654 in Step 1200\n",
      "Training loss  0.683 in Step 1300\n",
      "Training loss  0.684 in Step 1400\n",
      "Training loss  0.676 in Step 1500\n",
      "Training loss  0.648 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 240\n",
      "Training loss  0.673 in Step 0\n",
      "Training loss  0.690 in Step 100\n",
      "Training loss  0.672 in Step 200\n",
      "Training loss  0.684 in Step 300\n",
      "Training loss  0.678 in Step 400\n",
      "Training loss  0.673 in Step 500\n",
      "Training loss  0.678 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.702 in Step 800\n",
      "Training loss  0.702 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.675 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.662 in Step 1300\n",
      "Training loss  0.685 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.656 in Step 1600\n",
      "Training loss  0.676 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.687 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 241\n",
      "Training loss  0.691 in Step 0\n",
      "Training loss  0.688 in Step 100\n",
      "Training loss  0.659 in Step 200\n",
      "Training loss  0.683 in Step 300\n",
      "Training loss  0.685 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.694 in Step 600\n",
      "Training loss  0.683 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.662 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.681 in Step 1200\n",
      "Training loss  0.654 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.668 in Step 1500\n",
      "Training loss  0.676 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 242\n",
      "Training loss  0.651 in Step 0\n",
      "Training loss  0.669 in Step 100\n",
      "Training loss  0.674 in Step 200\n",
      "Training loss  0.666 in Step 300\n",
      "Training loss  0.685 in Step 400\n",
      "Training loss  0.665 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.675 in Step 800\n",
      "Training loss  0.695 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.678 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.668 in Step 1300\n",
      "Training loss  0.700 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.692 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.663 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 243\n",
      "Training loss  0.669 in Step 0\n",
      "Training loss  0.665 in Step 100\n",
      "Training loss  0.660 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.673 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.670 in Step 700\n",
      "Training loss  0.693 in Step 800\n",
      "Training loss  0.666 in Step 900\n",
      "Training loss  0.659 in Step 1000\n",
      "Training loss  0.673 in Step 1100\n",
      "Training loss  0.687 in Step 1200\n",
      "Training loss  0.698 in Step 1300\n",
      "Training loss  0.671 in Step 1400\n",
      "Training loss  0.650 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.677 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.662 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 244\n",
      "Training loss  0.667 in Step 0\n",
      "Training loss  0.677 in Step 100\n",
      "Training loss  0.677 in Step 200\n",
      "Training loss  0.671 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.694 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.647 in Step 700\n",
      "Training loss  0.687 in Step 800\n",
      "Training loss  0.667 in Step 900\n",
      "Training loss  0.679 in Step 1000\n",
      "Training loss  0.685 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.669 in Step 1300\n",
      "Training loss  0.687 in Step 1400\n",
      "Training loss  0.686 in Step 1500\n",
      "Training loss  0.681 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.664 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 245\n",
      "Training loss  0.682 in Step 0\n",
      "Training loss  0.672 in Step 100\n",
      "Training loss  0.671 in Step 200\n",
      "Training loss  0.668 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.688 in Step 800\n",
      "Training loss  0.666 in Step 900\n",
      "Training loss  0.690 in Step 1000\n",
      "Training loss  0.688 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.677 in Step 1300\n",
      "Training loss  0.665 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.669 in Step 1600\n",
      "Training loss  0.669 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 246\n",
      "Training loss  0.676 in Step 0\n",
      "Training loss  0.682 in Step 100\n",
      "Training loss  0.669 in Step 200\n",
      "Training loss  0.700 in Step 300\n",
      "Training loss  0.682 in Step 400\n",
      "Training loss  0.682 in Step 500\n",
      "Training loss  0.662 in Step 600\n",
      "Training loss  0.683 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.677 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.664 in Step 1100\n",
      "Training loss  0.701 in Step 1200\n",
      "Training loss  0.673 in Step 1300\n",
      "Training loss  0.706 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.691 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 247\n",
      "Training loss  0.697 in Step 0\n",
      "Training loss  0.678 in Step 100\n",
      "Training loss  0.666 in Step 200\n",
      "Training loss  0.681 in Step 300\n",
      "Training loss  0.663 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.698 in Step 700\n",
      "Training loss  0.678 in Step 800\n",
      "Training loss  0.690 in Step 900\n",
      "Training loss  0.667 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.672 in Step 1300\n",
      "Training loss  0.673 in Step 1400\n",
      "Training loss  0.660 in Step 1500\n",
      "Training loss  0.678 in Step 1600\n",
      "Training loss  0.693 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 248\n",
      "Training loss  0.679 in Step 0\n",
      "Training loss  0.664 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.686 in Step 300\n",
      "Training loss  0.675 in Step 400\n",
      "Training loss  0.655 in Step 500\n",
      "Training loss  0.709 in Step 600\n",
      "Training loss  0.691 in Step 700\n",
      "Training loss  0.678 in Step 800\n",
      "Training loss  0.677 in Step 900\n",
      "Training loss  0.691 in Step 1000\n",
      "Training loss  0.691 in Step 1100\n",
      "Training loss  0.687 in Step 1200\n",
      "Training loss  0.693 in Step 1300\n",
      "Training loss  0.676 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.699 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 249\n",
      "Training loss  0.666 in Step 0\n",
      "Training loss  0.681 in Step 100\n",
      "Training loss  0.658 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.660 in Step 500\n",
      "Training loss  0.693 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.650 in Step 800\n",
      "Training loss  0.696 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.686 in Step 1100\n",
      "Training loss  0.642 in Step 1200\n",
      "Training loss  0.677 in Step 1300\n",
      "Training loss  0.645 in Step 1400\n",
      "Training loss  0.663 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.678 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 250\n",
      "Training loss  0.676 in Step 0\n",
      "Training loss  0.678 in Step 100\n",
      "Training loss  0.658 in Step 200\n",
      "Training loss  0.681 in Step 300\n",
      "Training loss  0.676 in Step 400\n",
      "Training loss  0.660 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.669 in Step 700\n",
      "Training loss  0.678 in Step 800\n",
      "Training loss  0.687 in Step 900\n",
      "Training loss  0.669 in Step 1000\n",
      "Training loss  0.655 in Step 1100\n",
      "Training loss  0.664 in Step 1200\n",
      "Training loss  0.660 in Step 1300\n",
      "Training loss  0.693 in Step 1400\n",
      "Training loss  0.682 in Step 1500\n",
      "Training loss  0.663 in Step 1600\n",
      "Training loss  0.666 in Step 1700\n",
      "※※※Training loss  0.676※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 251\n",
      "Training loss  0.673 in Step 0\n",
      "Training loss  0.683 in Step 100\n",
      "Training loss  0.693 in Step 200\n",
      "Training loss  0.683 in Step 300\n",
      "Training loss  0.699 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.670 in Step 600\n",
      "Training loss  0.667 in Step 700\n",
      "Training loss  0.707 in Step 800\n",
      "Training loss  0.696 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.669 in Step 1100\n",
      "Training loss  0.700 in Step 1200\n",
      "Training loss  0.666 in Step 1300\n",
      "Training loss  0.691 in Step 1400\n",
      "Training loss  0.660 in Step 1500\n",
      "Training loss  0.672 in Step 1600\n",
      "Training loss  0.665 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 252\n",
      "Training loss  0.663 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.697 in Step 200\n",
      "Training loss  0.674 in Step 300\n",
      "Training loss  0.659 in Step 400\n",
      "Training loss  0.657 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.678 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.667 in Step 1100\n",
      "Training loss  0.670 in Step 1200\n",
      "Training loss  0.681 in Step 1300\n",
      "Training loss  0.651 in Step 1400\n",
      "Training loss  0.693 in Step 1500\n",
      "Training loss  0.682 in Step 1600\n",
      "Training loss  0.676 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.679※※※\n",
      "Epoch 253\n",
      "Training loss  0.691 in Step 0\n",
      "Training loss  0.680 in Step 100\n",
      "Training loss  0.673 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.683 in Step 400\n",
      "Training loss  0.684 in Step 500\n",
      "Training loss  0.671 in Step 600\n",
      "Training loss  0.686 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.677 in Step 900\n",
      "Training loss  0.668 in Step 1000\n",
      "Training loss  0.666 in Step 1100\n",
      "Training loss  0.662 in Step 1200\n",
      "Training loss  0.689 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.660 in Step 1500\n",
      "Training loss  0.686 in Step 1600\n",
      "Training loss  0.658 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.677 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 254\n",
      "Training loss  0.688 in Step 0\n",
      "Training loss  0.683 in Step 100\n",
      "Training loss  0.662 in Step 200\n",
      "Training loss  0.691 in Step 300\n",
      "Training loss  0.680 in Step 400\n",
      "Training loss  0.683 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.689 in Step 700\n",
      "Training loss  0.661 in Step 800\n",
      "Training loss  0.693 in Step 900\n",
      "Training loss  0.680 in Step 1000\n",
      "Training loss  0.666 in Step 1100\n",
      "Training loss  0.664 in Step 1200\n",
      "Training loss  0.700 in Step 1300\n",
      "Training loss  0.670 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.688 in Step 1600\n",
      "Training loss  0.685 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 255\n",
      "Training loss  0.644 in Step 0\n",
      "Training loss  0.667 in Step 100\n",
      "Training loss  0.715 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.673 in Step 400\n",
      "Training loss  0.671 in Step 500\n",
      "Training loss  0.688 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.682 in Step 800\n",
      "Training loss  0.660 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.656 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.683 in Step 1300\n",
      "Training loss  0.676 in Step 1400\n",
      "Training loss  0.656 in Step 1500\n",
      "Training loss  0.666 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 256\n",
      "Training loss  0.668 in Step 0\n",
      "Training loss  0.674 in Step 100\n",
      "Training loss  0.686 in Step 200\n",
      "Training loss  0.685 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.686 in Step 500\n",
      "Training loss  0.681 in Step 600\n",
      "Training loss  0.676 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.686 in Step 1000\n",
      "Training loss  0.685 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.665 in Step 1300\n",
      "Training loss  0.690 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.656 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 257\n",
      "Training loss  0.685 in Step 0\n",
      "Training loss  0.687 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.703 in Step 300\n",
      "Training loss  0.676 in Step 400\n",
      "Training loss  0.672 in Step 500\n",
      "Training loss  0.651 in Step 600\n",
      "Training loss  0.694 in Step 700\n",
      "Training loss  0.658 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.674 in Step 1000\n",
      "Training loss  0.689 in Step 1100\n",
      "Training loss  0.673 in Step 1200\n",
      "Training loss  0.680 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.672 in Step 1500\n",
      "Training loss  0.696 in Step 1600\n",
      "Training loss  0.701 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 258\n",
      "Training loss  0.701 in Step 0\n",
      "Training loss  0.679 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.678 in Step 500\n",
      "Training loss  0.653 in Step 600\n",
      "Training loss  0.677 in Step 700\n",
      "Training loss  0.664 in Step 800\n",
      "Training loss  0.698 in Step 900\n",
      "Training loss  0.670 in Step 1000\n",
      "Training loss  0.698 in Step 1100\n",
      "Training loss  0.686 in Step 1200\n",
      "Training loss  0.666 in Step 1300\n",
      "Training loss  0.655 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.681 in Step 1600\n",
      "Training loss  0.699 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 259\n",
      "Training loss  0.689 in Step 0\n",
      "Training loss  0.707 in Step 100\n",
      "Training loss  0.656 in Step 200\n",
      "Training loss  0.691 in Step 300\n",
      "Training loss  0.671 in Step 400\n",
      "Training loss  0.679 in Step 500\n",
      "Training loss  0.698 in Step 600\n",
      "Training loss  0.671 in Step 700\n",
      "Training loss  0.674 in Step 800\n",
      "Training loss  0.672 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.676 in Step 1100\n",
      "Training loss  0.689 in Step 1200\n",
      "Training loss  0.677 in Step 1300\n",
      "Training loss  0.684 in Step 1400\n",
      "Training loss  0.690 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.666 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.664 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.675※※※\n",
      "Epoch 260\n",
      "Training loss  0.669 in Step 0\n",
      "Training loss  0.675 in Step 100\n",
      "Training loss  0.678 in Step 200\n",
      "Training loss  0.661 in Step 300\n",
      "Training loss  0.657 in Step 400\n",
      "Training loss  0.682 in Step 500\n",
      "Training loss  0.660 in Step 600\n",
      "Training loss  0.675 in Step 700\n",
      "Training loss  0.663 in Step 800\n",
      "Training loss  0.678 in Step 900\n",
      "Training loss  0.678 in Step 1000\n",
      "Training loss  0.679 in Step 1100\n",
      "Training loss  0.708 in Step 1200\n",
      "Training loss  0.659 in Step 1300\n",
      "Training loss  0.670 in Step 1400\n",
      "Training loss  0.683 in Step 1500\n",
      "Training loss  0.683 in Step 1600\n",
      "Training loss  0.654 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.655 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 261\n",
      "Training loss  0.672 in Step 0\n",
      "Training loss  0.661 in Step 100\n",
      "Training loss  0.667 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.681 in Step 400\n",
      "Training loss  0.679 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.688 in Step 700\n",
      "Training loss  0.669 in Step 800\n",
      "Training loss  0.679 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.686 in Step 1100\n",
      "Training loss  0.666 in Step 1200\n",
      "Training loss  0.665 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.667 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.702 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 262\n",
      "Training loss  0.675 in Step 0\n",
      "Training loss  0.699 in Step 100\n",
      "Training loss  0.676 in Step 200\n",
      "Training loss  0.676 in Step 300\n",
      "Training loss  0.669 in Step 400\n",
      "Training loss  0.680 in Step 500\n",
      "Training loss  0.661 in Step 600\n",
      "Training loss  0.677 in Step 700\n",
      "Training loss  0.704 in Step 800\n",
      "Training loss  0.671 in Step 900\n",
      "Training loss  0.672 in Step 1000\n",
      "Training loss  0.687 in Step 1100\n",
      "Training loss  0.671 in Step 1200\n",
      "Training loss  0.673 in Step 1300\n",
      "Training loss  0.661 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.689 in Step 1600\n",
      "Training loss  0.685 in Step 1700\n",
      "※※※Training loss  0.683※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.678 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 263\n",
      "Training loss  0.695 in Step 0\n",
      "Training loss  0.683 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.682 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.687 in Step 500\n",
      "Training loss  0.678 in Step 600\n",
      "Training loss  0.681 in Step 700\n",
      "Training loss  0.694 in Step 800\n",
      "Training loss  0.680 in Step 900\n",
      "Training loss  0.664 in Step 1000\n",
      "Training loss  0.690 in Step 1100\n",
      "Training loss  0.663 in Step 1200\n",
      "Training loss  0.676 in Step 1300\n",
      "Training loss  0.667 in Step 1400\n",
      "Training loss  0.674 in Step 1500\n",
      "Training loss  0.696 in Step 1600\n",
      "Training loss  0.664 in Step 1700\n",
      "※※※Training loss  0.680※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 264\n",
      "Training loss  0.670 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.681 in Step 200\n",
      "Training loss  0.667 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.693 in Step 500\n",
      "Training loss  0.657 in Step 600\n",
      "Training loss  0.681 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.681 in Step 900\n",
      "Training loss  0.668 in Step 1000\n",
      "Training loss  0.690 in Step 1100\n",
      "Training loss  0.675 in Step 1200\n",
      "Training loss  0.661 in Step 1300\n",
      "Training loss  0.680 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.668 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 265\n",
      "Training loss  0.671 in Step 0\n",
      "Training loss  0.683 in Step 100\n",
      "Training loss  0.670 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.684 in Step 400\n",
      "Training loss  0.676 in Step 500\n",
      "Training loss  0.676 in Step 600\n",
      "Training loss  0.652 in Step 700\n",
      "Training loss  0.664 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.678 in Step 1100\n",
      "Training loss  0.677 in Step 1200\n",
      "Training loss  0.667 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.710 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.664 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 266\n",
      "Training loss  0.662 in Step 0\n",
      "Training loss  0.667 in Step 100\n",
      "Training loss  0.670 in Step 200\n",
      "Training loss  0.672 in Step 300\n",
      "Training loss  0.680 in Step 400\n",
      "Training loss  0.666 in Step 500\n",
      "Training loss  0.675 in Step 600\n",
      "Training loss  0.690 in Step 700\n",
      "Training loss  0.726 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.707 in Step 1000\n",
      "Training loss  0.656 in Step 1100\n",
      "Training loss  0.666 in Step 1200\n",
      "Training loss  0.683 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.663 in Step 1500\n",
      "Training loss  0.684 in Step 1600\n",
      "Training loss  0.666 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.700 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 267\n",
      "Training loss  0.688 in Step 0\n",
      "Training loss  0.675 in Step 100\n",
      "Training loss  0.690 in Step 200\n",
      "Training loss  0.650 in Step 300\n",
      "Training loss  0.675 in Step 400\n",
      "Training loss  0.700 in Step 500\n",
      "Training loss  0.659 in Step 600\n",
      "Training loss  0.674 in Step 700\n",
      "Training loss  0.670 in Step 800\n",
      "Training loss  0.695 in Step 900\n",
      "Training loss  0.678 in Step 1000\n",
      "Training loss  0.677 in Step 1100\n",
      "Training loss  0.695 in Step 1200\n",
      "Training loss  0.687 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.681 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.686 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.688 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 268\n",
      "Training loss  0.679 in Step 0\n",
      "Training loss  0.671 in Step 100\n",
      "Training loss  0.676 in Step 200\n",
      "Training loss  0.676 in Step 300\n",
      "Training loss  0.688 in Step 400\n",
      "Training loss  0.714 in Step 500\n",
      "Training loss  0.646 in Step 600\n",
      "Training loss  0.693 in Step 700\n",
      "Training loss  0.699 in Step 800\n",
      "Training loss  0.683 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.661 in Step 1100\n",
      "Training loss  0.701 in Step 1200\n",
      "Training loss  0.707 in Step 1300\n",
      "Training loss  0.671 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.691 in Step 1600\n",
      "Training loss  0.695 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.666 in Step 0\n",
      "Valid loss  0.680 in Step 100\n",
      "Valid loss  0.676 in Step 200\n",
      "Valid loss  0.701 in Step 300\n",
      "Valid loss  0.688 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 269\n",
      "Training loss  0.706 in Step 0\n",
      "Training loss  0.690 in Step 100\n",
      "Training loss  0.701 in Step 200\n",
      "Training loss  0.691 in Step 300\n",
      "Training loss  0.688 in Step 400\n",
      "Training loss  0.666 in Step 500\n",
      "Training loss  0.686 in Step 600\n",
      "Training loss  0.678 in Step 700\n",
      "Training loss  0.702 in Step 800\n",
      "Training loss  0.692 in Step 900\n",
      "Training loss  0.682 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.669 in Step 1200\n",
      "Training loss  0.698 in Step 1300\n",
      "Training loss  0.664 in Step 1400\n",
      "Training loss  0.699 in Step 1500\n",
      "Training loss  0.693 in Step 1600\n",
      "Training loss  0.670 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.680 in Step 100\n",
      "Valid loss  0.691 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 270\n",
      "Training loss  0.664 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.679 in Step 200\n",
      "Training loss  0.684 in Step 300\n",
      "Training loss  0.684 in Step 400\n",
      "Training loss  0.685 in Step 500\n",
      "Training loss  0.718 in Step 600\n",
      "Training loss  0.694 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.664 in Step 900\n",
      "Training loss  0.683 in Step 1000\n",
      "Training loss  0.694 in Step 1100\n",
      "Training loss  0.671 in Step 1200\n",
      "Training loss  0.684 in Step 1300\n",
      "Training loss  0.698 in Step 1400\n",
      "Training loss  0.694 in Step 1500\n",
      "Training loss  0.698 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.685※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.669 in Step 0\n",
      "Valid loss  0.700 in Step 100\n",
      "Valid loss  0.681 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.701 in Step 400\n",
      "※※※Valid loss  0.689※※※\n",
      "Epoch 271\n",
      "Training loss  0.663 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.697 in Step 200\n",
      "Training loss  0.689 in Step 300\n",
      "Training loss  0.676 in Step 400\n",
      "Training loss  0.709 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.680 in Step 700\n",
      "Training loss  0.692 in Step 800\n",
      "Training loss  0.682 in Step 900\n",
      "Training loss  0.686 in Step 1000\n",
      "Training loss  0.657 in Step 1100\n",
      "Training loss  0.700 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.677 in Step 1500\n",
      "Training loss  0.685 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.683 in Step 100\n",
      "Valid loss  0.672 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.682※※※\n",
      "Epoch 272\n",
      "Training loss  0.690 in Step 0\n",
      "Training loss  0.679 in Step 100\n",
      "Training loss  0.693 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.660 in Step 400\n",
      "Training loss  0.668 in Step 500\n",
      "Training loss  0.703 in Step 600\n",
      "Training loss  0.701 in Step 700\n",
      "Training loss  0.680 in Step 800\n",
      "Training loss  0.683 in Step 900\n",
      "Training loss  0.693 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.677 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.700 in Step 1400\n",
      "Training loss  0.660 in Step 1500\n",
      "Training loss  0.702 in Step 1600\n",
      "Training loss  0.687 in Step 1700\n",
      "※※※Training loss  0.685※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.682 in Step 100\n",
      "Valid loss  0.678 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.695 in Step 400\n",
      "※※※Valid loss  0.684※※※\n",
      "Epoch 273\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.680 in Step 100\n",
      "Training loss  0.697 in Step 200\n",
      "Training loss  0.672 in Step 300\n",
      "Training loss  0.676 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.687 in Step 700\n",
      "Training loss  0.696 in Step 800\n",
      "Training loss  0.692 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.676 in Step 1100\n",
      "Training loss  0.687 in Step 1200\n",
      "Training loss  0.694 in Step 1300\n",
      "Training loss  0.700 in Step 1400\n",
      "Training loss  0.696 in Step 1500\n",
      "Training loss  0.682 in Step 1600\n",
      "Training loss  0.701 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.688 in Step 100\n",
      "Valid loss  0.678 in Step 200\n",
      "Valid loss  0.703 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.685※※※\n",
      "Epoch 274\n",
      "Training loss  0.683 in Step 0\n",
      "Training loss  0.665 in Step 100\n",
      "Training loss  0.682 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.670 in Step 400\n",
      "Training loss  0.684 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.669 in Step 700\n",
      "Training loss  0.692 in Step 800\n",
      "Training loss  0.701 in Step 900\n",
      "Training loss  0.666 in Step 1000\n",
      "Training loss  0.668 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.661 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.702 in Step 1500\n",
      "Training loss  0.698 in Step 1600\n",
      "Training loss  0.690 in Step 1700\n",
      "※※※Training loss  0.682※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.685 in Step 100\n",
      "Valid loss  0.675 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.683※※※\n",
      "Epoch 275\n",
      "Training loss  0.701 in Step 0\n",
      "Training loss  0.671 in Step 100\n",
      "Training loss  0.696 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.678 in Step 500\n",
      "Training loss  0.701 in Step 600\n",
      "Training loss  0.698 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.689 in Step 900\n",
      "Training loss  0.675 in Step 1000\n",
      "Training loss  0.730 in Step 1100\n",
      "Training loss  0.698 in Step 1200\n",
      "Training loss  0.698 in Step 1300\n",
      "Training loss  0.676 in Step 1400\n",
      "Training loss  0.668 in Step 1500\n",
      "Training loss  0.697 in Step 1600\n",
      "Training loss  0.683 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.668 in Step 0\n",
      "Valid loss  0.688 in Step 100\n",
      "Valid loss  0.679 in Step 200\n",
      "Valid loss  0.698 in Step 300\n",
      "Valid loss  0.698 in Step 400\n",
      "※※※Valid loss  0.688※※※\n",
      "Epoch 276\n",
      "Training loss  0.671 in Step 0\n",
      "Training loss  0.686 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.669 in Step 300\n",
      "Training loss  0.689 in Step 400\n",
      "Training loss  0.681 in Step 500\n",
      "Training loss  0.697 in Step 600\n",
      "Training loss  0.648 in Step 700\n",
      "Training loss  0.715 in Step 800\n",
      "Training loss  0.692 in Step 900\n",
      "Training loss  0.687 in Step 1000\n",
      "Training loss  0.678 in Step 1100\n",
      "Training loss  0.704 in Step 1200\n",
      "Training loss  0.687 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.671 in Step 1600\n",
      "Training loss  0.683 in Step 1700\n",
      "※※※Training loss  0.686※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.674 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 277\n",
      "Training loss  0.683 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.675 in Step 200\n",
      "Training loss  0.677 in Step 300\n",
      "Training loss  0.708 in Step 400\n",
      "Training loss  0.676 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.673 in Step 800\n",
      "Training loss  0.661 in Step 900\n",
      "Training loss  0.669 in Step 1000\n",
      "Training loss  0.697 in Step 1100\n",
      "Training loss  0.680 in Step 1200\n",
      "Training loss  0.677 in Step 1300\n",
      "Training loss  0.664 in Step 1400\n",
      "Training loss  0.650 in Step 1500\n",
      "Training loss  0.667 in Step 1600\n",
      "Training loss  0.678 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 278\n",
      "Training loss  0.672 in Step 0\n",
      "Training loss  0.692 in Step 100\n",
      "Training loss  0.663 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.666 in Step 400\n",
      "Training loss  0.678 in Step 500\n",
      "Training loss  0.670 in Step 600\n",
      "Training loss  0.674 in Step 700\n",
      "Training loss  0.675 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.699 in Step 1000\n",
      "Training loss  0.664 in Step 1100\n",
      "Training loss  0.684 in Step 1200\n",
      "Training loss  0.713 in Step 1300\n",
      "Training loss  0.686 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.674 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.681※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.671 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 279\n",
      "Training loss  0.669 in Step 0\n",
      "Training loss  0.659 in Step 100\n",
      "Training loss  0.664 in Step 200\n",
      "Training loss  0.666 in Step 300\n",
      "Training loss  0.650 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.677 in Step 700\n",
      "Training loss  0.688 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.684 in Step 1000\n",
      "Training loss  0.665 in Step 1100\n",
      "Training loss  0.674 in Step 1200\n",
      "Training loss  0.666 in Step 1300\n",
      "Training loss  0.682 in Step 1400\n",
      "Training loss  0.691 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.692 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 280\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.709 in Step 200\n",
      "Training loss  0.711 in Step 300\n",
      "Training loss  0.700 in Step 400\n",
      "Training loss  0.681 in Step 500\n",
      "Training loss  0.699 in Step 600\n",
      "Training loss  0.674 in Step 700\n",
      "Training loss  0.693 in Step 800\n",
      "Training loss  0.678 in Step 900\n",
      "Training loss  0.692 in Step 1000\n",
      "Training loss  0.683 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.688 in Step 1300\n",
      "Training loss  0.690 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.673 in Step 1600\n",
      "Training loss  0.681 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.691 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 281\n",
      "Training loss  0.676 in Step 0\n",
      "Training loss  0.683 in Step 100\n",
      "Training loss  0.674 in Step 200\n",
      "Training loss  0.676 in Step 300\n",
      "Training loss  0.683 in Step 400\n",
      "Training loss  0.691 in Step 500\n",
      "Training loss  0.707 in Step 600\n",
      "Training loss  0.672 in Step 700\n",
      "Training loss  0.670 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.673 in Step 1000\n",
      "Training loss  0.669 in Step 1100\n",
      "Training loss  0.663 in Step 1200\n",
      "Training loss  0.679 in Step 1300\n",
      "Training loss  0.695 in Step 1400\n",
      "Training loss  0.663 in Step 1500\n",
      "Training loss  0.692 in Step 1600\n",
      "Training loss  0.665 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.673 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.704 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 282\n",
      "Training loss  0.697 in Step 0\n",
      "Training loss  0.689 in Step 100\n",
      "Training loss  0.688 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.681 in Step 400\n",
      "Training loss  0.656 in Step 500\n",
      "Training loss  0.691 in Step 600\n",
      "Training loss  0.683 in Step 700\n",
      "Training loss  0.685 in Step 800\n",
      "Training loss  0.676 in Step 900\n",
      "Training loss  0.691 in Step 1000\n",
      "Training loss  0.666 in Step 1100\n",
      "Training loss  0.682 in Step 1200\n",
      "Training loss  0.671 in Step 1300\n",
      "Training loss  0.664 in Step 1400\n",
      "Training loss  0.656 in Step 1500\n",
      "Training loss  0.711 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.657 in Step 0\n",
      "Valid loss  0.667 in Step 100\n",
      "Valid loss  0.664 in Step 200\n",
      "Valid loss  0.696 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 283\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.697 in Step 100\n",
      "Training loss  0.695 in Step 200\n",
      "Training loss  0.673 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.663 in Step 600\n",
      "Training loss  0.681 in Step 700\n",
      "Training loss  0.690 in Step 800\n",
      "Training loss  0.681 in Step 900\n",
      "Training loss  0.677 in Step 1000\n",
      "Training loss  0.679 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.690 in Step 1300\n",
      "Training loss  0.679 in Step 1400\n",
      "Training loss  0.690 in Step 1500\n",
      "Training loss  0.686 in Step 1600\n",
      "Training loss  0.680 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.695 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 284\n",
      "Training loss  0.671 in Step 0\n",
      "Training loss  0.667 in Step 100\n",
      "Training loss  0.654 in Step 200\n",
      "Training loss  0.681 in Step 300\n",
      "Training loss  0.690 in Step 400\n",
      "Training loss  0.696 in Step 500\n",
      "Training loss  0.693 in Step 600\n",
      "Training loss  0.682 in Step 700\n",
      "Training loss  0.667 in Step 800\n",
      "Training loss  0.668 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.683 in Step 1100\n",
      "Training loss  0.659 in Step 1200\n",
      "Training loss  0.666 in Step 1300\n",
      "Training loss  0.714 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.687 in Step 1600\n",
      "Training loss  0.654 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.665 in Step 0\n",
      "Valid loss  0.676 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.692 in Step 400\n",
      "※※※Valid loss  0.681※※※\n",
      "Epoch 285\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.693 in Step 100\n",
      "Training loss  0.671 in Step 200\n",
      "Training loss  0.692 in Step 300\n",
      "Training loss  0.685 in Step 400\n",
      "Training loss  0.692 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.682 in Step 700\n",
      "Training loss  0.675 in Step 800\n",
      "Training loss  0.688 in Step 900\n",
      "Training loss  0.666 in Step 1000\n",
      "Training loss  0.699 in Step 1100\n",
      "Training loss  0.678 in Step 1200\n",
      "Training loss  0.685 in Step 1300\n",
      "Training loss  0.663 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.683 in Step 1600\n",
      "Training loss  0.685 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.687 in Step 300\n",
      "Valid loss  0.689 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 286\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.671 in Step 100\n",
      "Training loss  0.679 in Step 200\n",
      "Training loss  0.671 in Step 300\n",
      "Training loss  0.684 in Step 400\n",
      "Training loss  0.706 in Step 500\n",
      "Training loss  0.680 in Step 600\n",
      "Training loss  0.649 in Step 700\n",
      "Training loss  0.686 in Step 800\n",
      "Training loss  0.660 in Step 900\n",
      "Training loss  0.687 in Step 1000\n",
      "Training loss  0.684 in Step 1100\n",
      "Training loss  0.665 in Step 1200\n",
      "Training loss  0.681 in Step 1300\n",
      "Training loss  0.674 in Step 1400\n",
      "Training loss  0.653 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.675 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.672 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.687 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 287\n",
      "Training loss  0.687 in Step 0\n",
      "Training loss  0.664 in Step 100\n",
      "Training loss  0.666 in Step 200\n",
      "Training loss  0.669 in Step 300\n",
      "Training loss  0.671 in Step 400\n",
      "Training loss  0.690 in Step 500\n",
      "Training loss  0.679 in Step 600\n",
      "Training loss  0.654 in Step 700\n",
      "Training loss  0.663 in Step 800\n",
      "Training loss  0.670 in Step 900\n",
      "Training loss  0.671 in Step 1000\n",
      "Training loss  0.661 in Step 1100\n",
      "Training loss  0.710 in Step 1200\n",
      "Training loss  0.671 in Step 1300\n",
      "Training loss  0.671 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.679 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.692 in Step 300\n",
      "Valid loss  0.687 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 288\n",
      "Training loss  0.692 in Step 0\n",
      "Training loss  0.684 in Step 100\n",
      "Training loss  0.694 in Step 200\n",
      "Training loss  0.688 in Step 300\n",
      "Training loss  0.688 in Step 400\n",
      "Training loss  0.676 in Step 500\n",
      "Training loss  0.684 in Step 600\n",
      "Training loss  0.654 in Step 700\n",
      "Training loss  0.655 in Step 800\n",
      "Training loss  0.675 in Step 900\n",
      "Training loss  0.670 in Step 1000\n",
      "Training loss  0.671 in Step 1100\n",
      "Training loss  0.669 in Step 1200\n",
      "Training loss  0.697 in Step 1300\n",
      "Training loss  0.667 in Step 1400\n",
      "Training loss  0.680 in Step 1500\n",
      "Training loss  0.676 in Step 1600\n",
      "Training loss  0.702 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.685 in Step 400\n",
      "※※※Valid loss  0.676※※※\n",
      "Epoch 289\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.669 in Step 100\n",
      "Training loss  0.677 in Step 200\n",
      "Training loss  0.684 in Step 300\n",
      "Training loss  0.680 in Step 400\n",
      "Training loss  0.676 in Step 500\n",
      "Training loss  0.698 in Step 600\n",
      "Training loss  0.661 in Step 700\n",
      "Training loss  0.669 in Step 800\n",
      "Training loss  0.681 in Step 900\n",
      "Training loss  0.685 in Step 1000\n",
      "Training loss  0.711 in Step 1100\n",
      "Training loss  0.669 in Step 1200\n",
      "Training loss  0.677 in Step 1300\n",
      "Training loss  0.689 in Step 1400\n",
      "Training loss  0.681 in Step 1500\n",
      "Training loss  0.676 in Step 1600\n",
      "Training loss  0.655 in Step 1700\n",
      "※※※Training loss  0.679※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.663 in Step 0\n",
      "Valid loss  0.688 in Step 100\n",
      "Valid loss  0.669 in Step 200\n",
      "Valid loss  0.697 in Step 300\n",
      "Valid loss  0.688 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 290\n",
      "Training loss  0.653 in Step 0\n",
      "Training loss  0.662 in Step 100\n",
      "Training loss  0.669 in Step 200\n",
      "Training loss  0.687 in Step 300\n",
      "Training loss  0.665 in Step 400\n",
      "Training loss  0.680 in Step 500\n",
      "Training loss  0.668 in Step 600\n",
      "Training loss  0.666 in Step 700\n",
      "Training loss  0.674 in Step 800\n",
      "Training loss  0.664 in Step 900\n",
      "Training loss  0.687 in Step 1000\n",
      "Training loss  0.679 in Step 1100\n",
      "Training loss  0.716 in Step 1200\n",
      "Training loss  0.658 in Step 1300\n",
      "Training loss  0.681 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.669 in Step 1700\n",
      "※※※Training loss  0.678※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.662 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.686 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 291\n",
      "Training loss  0.674 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.683 in Step 200\n",
      "Training loss  0.661 in Step 300\n",
      "Training loss  0.674 in Step 400\n",
      "Training loss  0.663 in Step 500\n",
      "Training loss  0.692 in Step 600\n",
      "Training loss  0.659 in Step 700\n",
      "Training loss  0.681 in Step 800\n",
      "Training loss  0.685 in Step 900\n",
      "Training loss  0.702 in Step 1000\n",
      "Training loss  0.672 in Step 1100\n",
      "Training loss  0.676 in Step 1200\n",
      "Training loss  0.686 in Step 1300\n",
      "Training loss  0.698 in Step 1400\n",
      "Training loss  0.672 in Step 1500\n",
      "Training loss  0.674 in Step 1600\n",
      "Training loss  0.689 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.656 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.664 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.685 in Step 400\n",
      "※※※Valid loss  0.675※※※\n",
      "Epoch 292\n",
      "Training loss  0.656 in Step 0\n",
      "Training loss  0.666 in Step 100\n",
      "Training loss  0.675 in Step 200\n",
      "Training loss  0.682 in Step 300\n",
      "Training loss  0.681 in Step 400\n",
      "Training loss  0.696 in Step 500\n",
      "Training loss  0.682 in Step 600\n",
      "Training loss  0.692 in Step 700\n",
      "Training loss  0.674 in Step 800\n",
      "Training loss  0.677 in Step 900\n",
      "Training loss  0.681 in Step 1000\n",
      "Training loss  0.680 in Step 1100\n",
      "Training loss  0.686 in Step 1200\n",
      "Training loss  0.693 in Step 1300\n",
      "Training loss  0.689 in Step 1400\n",
      "Training loss  0.654 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.694 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.665 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.685 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 293\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.661 in Step 200\n",
      "Training loss  0.660 in Step 300\n",
      "Training loss  0.675 in Step 400\n",
      "Training loss  0.658 in Step 500\n",
      "Training loss  0.673 in Step 600\n",
      "Training loss  0.665 in Step 700\n",
      "Training loss  0.670 in Step 800\n",
      "Training loss  0.674 in Step 900\n",
      "Training loss  0.689 in Step 1000\n",
      "Training loss  0.669 in Step 1100\n",
      "Training loss  0.677 in Step 1200\n",
      "Training loss  0.670 in Step 1300\n",
      "Training loss  0.687 in Step 1400\n",
      "Training loss  0.673 in Step 1500\n",
      "Training loss  0.671 in Step 1600\n",
      "Training loss  0.672 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.657 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.663 in Step 200\n",
      "Valid loss  0.690 in Step 300\n",
      "Valid loss  0.685 in Step 400\n",
      "※※※Valid loss  0.675※※※\n",
      "Epoch 294\n",
      "Training loss  0.673 in Step 0\n",
      "Training loss  0.667 in Step 100\n",
      "Training loss  0.687 in Step 200\n",
      "Training loss  0.665 in Step 300\n",
      "Training loss  0.681 in Step 400\n",
      "Training loss  0.674 in Step 500\n",
      "Training loss  0.678 in Step 600\n",
      "Training loss  0.674 in Step 700\n",
      "Training loss  0.685 in Step 800\n",
      "Training loss  0.661 in Step 900\n",
      "Training loss  0.695 in Step 1000\n",
      "Training loss  0.676 in Step 1100\n",
      "Training loss  0.675 in Step 1200\n",
      "Training loss  0.685 in Step 1300\n",
      "Training loss  0.677 in Step 1400\n",
      "Training loss  0.688 in Step 1500\n",
      "Training loss  0.680 in Step 1600\n",
      "Training loss  0.697 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.664 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.694 in Step 300\n",
      "Valid loss  0.687 in Step 400\n",
      "※※※Valid loss  0.680※※※\n",
      "Epoch 295\n",
      "Training loss  0.684 in Step 0\n",
      "Training loss  0.653 in Step 100\n",
      "Training loss  0.671 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.692 in Step 400\n",
      "Training loss  0.681 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.671 in Step 700\n",
      "Training loss  0.669 in Step 800\n",
      "Training loss  0.696 in Step 900\n",
      "Training loss  0.697 in Step 1000\n",
      "Training loss  0.688 in Step 1100\n",
      "Training loss  0.675 in Step 1200\n",
      "Training loss  0.655 in Step 1300\n",
      "Training loss  0.671 in Step 1400\n",
      "Training loss  0.689 in Step 1500\n",
      "Training loss  0.704 in Step 1600\n",
      "Training loss  0.683 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.660 in Step 0\n",
      "Valid loss  0.671 in Step 100\n",
      "Valid loss  0.668 in Step 200\n",
      "Valid loss  0.693 in Step 300\n",
      "Valid loss  0.693 in Step 400\n",
      "※※※Valid loss  0.678※※※\n",
      "Epoch 296\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.685 in Step 100\n",
      "Training loss  0.671 in Step 200\n",
      "Training loss  0.657 in Step 300\n",
      "Training loss  0.666 in Step 400\n",
      "Training loss  0.707 in Step 500\n",
      "Training loss  0.689 in Step 600\n",
      "Training loss  0.657 in Step 700\n",
      "Training loss  0.691 in Step 800\n",
      "Training loss  0.684 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.662 in Step 1100\n",
      "Training loss  0.658 in Step 1200\n",
      "Training loss  0.667 in Step 1300\n",
      "Training loss  0.660 in Step 1400\n",
      "Training loss  0.684 in Step 1500\n",
      "Training loss  0.659 in Step 1600\n",
      "Training loss  0.662 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.661 in Step 0\n",
      "Valid loss  0.668 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.686 in Step 400\n",
      "※※※Valid loss  0.677※※※\n",
      "Epoch 297\n",
      "Training loss  0.664 in Step 0\n",
      "Training loss  0.678 in Step 100\n",
      "Training loss  0.668 in Step 200\n",
      "Training loss  0.701 in Step 300\n",
      "Training loss  0.664 in Step 400\n",
      "Training loss  0.673 in Step 500\n",
      "Training loss  0.685 in Step 600\n",
      "Training loss  0.671 in Step 700\n",
      "Training loss  0.668 in Step 800\n",
      "Training loss  0.662 in Step 900\n",
      "Training loss  0.676 in Step 1000\n",
      "Training loss  0.666 in Step 1100\n",
      "Training loss  0.671 in Step 1200\n",
      "Training loss  0.647 in Step 1300\n",
      "Training loss  0.693 in Step 1400\n",
      "Training loss  0.663 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.679 in Step 1700\n",
      "※※※Training loss  0.677※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.659 in Step 0\n",
      "Valid loss  0.667 in Step 100\n",
      "Valid loss  0.666 in Step 200\n",
      "Valid loss  0.689 in Step 300\n",
      "Valid loss  0.690 in Step 400\n",
      "※※※Valid loss  0.675※※※\n",
      "Epoch 298\n",
      "Training loss  0.687 in Step 0\n",
      "Training loss  0.670 in Step 100\n",
      "Training loss  0.667 in Step 200\n",
      "Training loss  0.678 in Step 300\n",
      "Training loss  0.691 in Step 400\n",
      "Training loss  0.689 in Step 500\n",
      "Training loss  0.678 in Step 600\n",
      "Training loss  0.679 in Step 700\n",
      "Training loss  0.656 in Step 800\n",
      "Training loss  0.655 in Step 900\n",
      "Training loss  0.666 in Step 1000\n",
      "Training loss  0.676 in Step 1100\n",
      "Training loss  0.679 in Step 1200\n",
      "Training loss  0.667 in Step 1300\n",
      "Training loss  0.665 in Step 1400\n",
      "Training loss  0.670 in Step 1500\n",
      "Training loss  0.685 in Step 1600\n",
      "Training loss  0.700 in Step 1700\n",
      "※※※Training loss  0.676※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.657 in Step 0\n",
      "Valid loss  0.670 in Step 100\n",
      "Valid loss  0.664 in Step 200\n",
      "Valid loss  0.688 in Step 300\n",
      "Valid loss  0.696 in Step 400\n",
      "※※※Valid loss  0.675※※※\n",
      "Epoch 299\n",
      "Training loss  0.677 in Step 0\n",
      "Training loss  0.700 in Step 100\n",
      "Training loss  0.680 in Step 200\n",
      "Training loss  0.675 in Step 300\n",
      "Training loss  0.678 in Step 400\n",
      "Training loss  0.679 in Step 500\n",
      "Training loss  0.658 in Step 600\n",
      "Training loss  0.681 in Step 700\n",
      "Training loss  0.701 in Step 800\n",
      "Training loss  0.657 in Step 900\n",
      "Training loss  0.686 in Step 1000\n",
      "Training loss  0.681 in Step 1100\n",
      "Training loss  0.666 in Step 1200\n",
      "Training loss  0.674 in Step 1300\n",
      "Training loss  0.677 in Step 1400\n",
      "Training loss  0.679 in Step 1500\n",
      "Training loss  0.677 in Step 1600\n",
      "Training loss  0.676 in Step 1700\n",
      "※※※Training loss  0.676※※※\n",
      "Training timepoint saved\n",
      "Valid loss  0.658 in Step 0\n",
      "Valid loss  0.669 in Step 100\n",
      "Valid loss  0.667 in Step 200\n",
      "Valid loss  0.691 in Step 300\n",
      "Valid loss  0.694 in Step 400\n",
      "※※※Valid loss  0.677※※※\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KSTTwi31xAvh"
   },
   "outputs": [],
   "source": [
    "### Save\n",
    "train_losses.save()\n",
    "\n",
    "valid_losses.save()\n",
    "\n",
    "text_hist.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3yaMyIzH12RD",
    "outputId": "1426c24a-c60c-48c2-8690-f3a07bb9ba7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc65c533690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.get(), label='Train')\n",
    "plt.plot(valid_losses.get(), label='Valid')\n",
    "plt.title(\"Learning Curve Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This model should converge to loss around 0.677. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
